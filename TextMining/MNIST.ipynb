{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the runs deterministic\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"MNIST Dataset\"\"\"\n",
    "    def __init__(self, input_dir: Path):\n",
    "        super().__init__()\n",
    "        self.input_dir = input_dir\n",
    "        self.num_samples = list(filter(lambda x: \"image\" in x.name, self.input_dir.iterdir()))\n",
    "        self.sample_ids = list(map(lambda x:int(x.name.split('.')[0]),self.num_samples))\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset\"\"\"\n",
    "        return len(self.num_samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Returns a data point (image and label) given an index\"\"\"\n",
    "        idx = self.sample_ids[idx]\n",
    "        image_txt = self.input_dir.joinpath(\"{}.image\".format(idx)).read_text()\n",
    "        image = np.fromstring(image_txt, sep=\" \").reshape((28, 28))\n",
    "        image = torch.from_numpy(image).float() # network inputs need to be float\n",
    "\n",
    "        label = int(self.input_dir.joinpath(\"{}.label\".format(idx)).read_text())\n",
    "        label = torch.tensor(label).long() # label is not a continuous value but class indices\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    \"\"\"Simple Feedforward Neural Network for MNIST\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = 28 * 28\n",
    "        self.hidden_1_size = 2048\n",
    "        self.hidden_2_size = 256\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # nn.Linear is a feedforward layer, i.e. that it captures weights and bias values\n",
    "        self.fc1 = nn.Linear(self.image_size, self.hidden_1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_1_size, self.hidden_2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(self.hidden_2_size, self.num_classes)\n",
    "        \n",
    "        # weight initialisation\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To use a fully connected network, we need a single vector, not a matrix\n",
    "        x = x.reshape((-1, self.image_size))\n",
    "        \n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x) # => logits\n",
    "        \n",
    "        # softmax is not used here as the predefined loss function automatically assigns it\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModelDropout(nn.Module):\n",
    "    \"\"\"Simple Feedforward Neural Network for MNIST\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = 28 * 28\n",
    "        self.hidden_1_size = 2048\n",
    "        self.hidden_2_size = 256\n",
    "        self.num_classes = 10\n",
    "\n",
    "        # nn.Linear is a feedforward layer, i.e. that it captures weights and bias values\n",
    "        self.fc1 = nn.Linear(self.image_size, self.hidden_1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_1_size, self.hidden_2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(self.hidden_2_size, self.num_classes)\n",
    "        \n",
    "        # Dropout with 5% chance of dropping a neuron\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "        \n",
    "        # weight initialisation\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To use a fully connected network, we need a single vector, not a matrix\n",
    "        x = x.reshape((-1, self.image_size))\n",
    "        \n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x) # => logits\n",
    "        \n",
    "        # softmax is not used here as the predefined loss function automatically assigns it\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModelLowLevel(nn.Module):\n",
    "    \"\"\"Simple Feedforward Neural Network for MNIST\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = 28 * 28\n",
    "        self.hidden_1_size = 2048\n",
    "        self.hidden_2_size = 256\n",
    "        self.num_classes = 10\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.randn(self.image_size, self.hidden_1_size, requires_grad=True))\n",
    "        self.b1 = nn.Parameter(torch.randn(1, self.hidden_1_size, requires_grad=True))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.W2 = nn.Parameter(torch.randn(self.hidden_1_size, self.hidden_2_size, requires_grad=True))\n",
    "        self.b2 = nn.Parameter(torch.randn(1, self.hidden_2_size, requires_grad=True))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.W3 = nn.Parameter(torch.randn(self.hidden_2_size, self.num_classes, requires_grad=True))\n",
    "        self.b3 = nn.Parameter(torch.randn(1, self.num_classes, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # To use a fully connected network, we need a single vector, not a matrix\n",
    "        x = x.reshape((-1, self.image_size))\n",
    "\n",
    "        # first hidden layer\n",
    "        x = x @ self.W1 + self.b1\n",
    "        x = self.relu1(x)\n",
    "        # second hidden layer\n",
    "        x = x @ self.W2 + self.b2\n",
    "        x = self.relu2(x)\n",
    "        # output layer\n",
    "        x = x @ self.W3 + self.b3 # => logits\n",
    "                \n",
    "        # softmax is not used here as the predefined loss function automatically assigns it\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data\")\n",
    "\n",
    "train_path = path.joinpath(\"train/train\")\n",
    "train_dataset = MNISTDataset(train_path)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_path = path.joinpath(\"test/test\")\n",
    "test_dataset = MNISTDataset(test_path)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MNISTModel()\n",
    "#model = MNISTModelDropout()\n",
    "model = MNISTModelLowLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the loss function and optimisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimiser\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimiser = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to check the performance\n",
    "\n",
    "We want to check how the model performs on the train and test datasets while and after training.\n",
    "Therefore we build a little helper that calculates the accuracy of the network's predictions.\n",
    "\n",
    "We need to handle the batches that are used while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"\"\"A class to keep track of the accuracy while training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the internal state\"\"\"\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        \n",
    "    def update(self, output, labels):\n",
    "        \"\"\"\n",
    "        Updates the internal state to later compute the overall accuracy\n",
    "        \n",
    "        output: the output of the network for a batch\n",
    "        labels: the target labels\n",
    "        \"\"\"\n",
    "        _, predicted = torch.max(output.data, 1) # predicted now contains the predicted class index/label\n",
    "        \n",
    "        self.total += labels.size(0)\n",
    "        self.correct += (predicted == labels).sum().item() # .item() gets the number, not the tensor\n",
    "\n",
    "    def compute(self):\n",
    "        return self.correct/self.total\n",
    "    \n",
    "\n",
    "accuracy = Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "We loop over the training dataset multiple times (every full iteration is called an *epoch*).\n",
    "For every batch in the dataset, we calculate the loss of the network output, calculate the gradients by using Autograd's automatic gradient calculation, and update the network parameters using the Adam optimiser we initialised before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.92579: 100%|███████████████████████████████████████████████████| 58/58 [00:49<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.41025: 100%|███████████████████████████████████████████████████| 58/58 [00:50<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.26162: 100%|███████████████████████████████████████████████████| 58/58 [00:50<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.18726: 100%|███████████████████████████████████████████████████| 58/58 [00:49<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.13968: 100%|███████████████████████████████████████████████████| 58/58 [00:50<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.10731: 100%|███████████████████████████████████████████████████| 58/58 [00:50<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.08212: 100%|███████████████████████████████████████████████████| 58/58 [00:49<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.06302: 100%|███████████████████████████████████████████████████| 58/58 [00:49<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.04873: 100%|███████████████████████████████████████████████████| 58/58 [00:50<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.03806: 100%|███████████████████████████████████████████████████| 58/58 [00:50<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    print(\"Starting epoch {}\".format(epoch+1))\n",
    "    \n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # to make a beautiful progress bar\n",
    "    loader = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, data in loader:\n",
    "        # get the data points\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients (else, they are accumulated)\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward the data through the network\n",
    "        outputs = model(inputs)\n",
    "        # calculate the loss given the output of the network and the target labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        # calculate the gradients of the network w.r.t. its parameters\n",
    "        loss.backward()\n",
    "        # Let the optimiser take an optimization step using the calculated gradients\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss\n",
    "        total += outputs.size(0)\n",
    "\n",
    "        loader.set_description(\"loss: {:.5f}\".format(running_loss/total))\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "We can now use the test set to run inference of our model.\n",
    "We can output resulting predictions or use them for testing how well our model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTModelLowLevel(\n",
       "  (relu1): ReLU()\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 58/58 [00:41<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.65%\n"
     ]
    }
   ],
   "source": [
    "accuracy.reset()\n",
    "\n",
    "# Gradients are calculated on the forward pass for every iteration.\n",
    "# As we do not need gradients now, we can disable the calculation.\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(train_loader):\n",
    "        # get the data points\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward the data through the network\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        accuracy.update(outputs, labels)\n",
    "        \n",
    "print(\"Accuracy: {:.2f}%\".format(100 * accuracy.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.64%\n"
     ]
    }
   ],
   "source": [
    "accuracy.reset()\n",
    "\n",
    "# Gradients are calculated on the forward pass for every iteration.\n",
    "# As we do not need gradients now, we can disable the calculation.\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader): # now the test_loader\n",
    "        # get the data points\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward the data through the network\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        accuracy.update(outputs, labels)\n",
    "        \n",
    "print(\"Accuracy: {:.2f}%\".format(100 * accuracy.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
