{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 2\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. Submissions of single students will not be accepted! Please use the Forum in case of doubt in order to find team mates!\n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Tuesday, 7th of January 23:55h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples.\n",
    "\n",
    "Do not forget to specify the (First_name, Last_name, student_id (matrikelnummer)) of all contributing students in the jupyter notebook here:\n",
    "\n",
    "Student 1: `First_name, Last_name, student_id`\n",
    "\n",
    "Student 2: `First_name, Last_name, student_id`\n",
    "\n",
    "Student 3: `First_name, Last_name, student_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing GloVe\n",
    "In this task you will implement the glove algorithm using PyTorch. (One advantage is that you need not calculate gradient by hand, but you can take advantage of the autograd module). The task will require implementation of certain functions, which we look into step-by-step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for word to index mapping. Since the model won't be able to take strings as input we will convert them into indices. The function will generate a mapping  w2i which uses words as keys and corresponding indices as values e.g., `w2i['walk'] = 42`. As Preprocessing, remove all punctuations and convert all words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'one': 0,\n",
       "  'advantage': 1,\n",
       "  'is': 2,\n",
       "  'that': 3,\n",
       "  'you': 4,\n",
       "  'need': 5,\n",
       "  'not': 6,\n",
       "  'calculate': 7,\n",
       "  'gradient': 8,\n",
       "  'by': 9,\n",
       "  'hand': 10,\n",
       "  'but': 11,\n",
       "  'can': 12,\n",
       "  'take': 13,\n",
       "  'of': 14,\n",
       "  'the': 15,\n",
       "  'autograd': 16,\n",
       "  'module': 17},\n",
       " ['one',\n",
       "  'advantage',\n",
       "  'is',\n",
       "  'that',\n",
       "  'you',\n",
       "  'need',\n",
       "  'not',\n",
       "  'calculate',\n",
       "  'gradient',\n",
       "  'by',\n",
       "  'hand',\n",
       "  'but',\n",
       "  'you',\n",
       "  'can',\n",
       "  'take',\n",
       "  'advantage',\n",
       "  'of',\n",
       "  'the',\n",
       "  'autograd',\n",
       "  'module'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "def word2indexMapping(textfile):\n",
    "    w2i = {}\n",
    "    text = [] # sequence of words as they appear in the text after removing punctuations\n",
    "    # write your code snippet here....\n",
    "    words_list = word_tokenize(textfile)\n",
    "    puncts = string.punctuation \n",
    "    #delete punctuations from text file\n",
    "    for word in words_list:\n",
    "        if word not in puncts:\n",
    "            text.append(word.lower())\n",
    "            \n",
    "    vocab_list = list(dict.fromkeys(text)) # store unique elements\n",
    "    #store word-index in the dictionary\n",
    "    for index,word in enumerate(vocab_list):\n",
    "        w2i[word] = index\n",
    "        \n",
    "    return w2i, text\n",
    "\n",
    "#test case\n",
    "word2indexMapping('(One advantage is that you need not calculate gradient by hand, but you can take advantage of the autograd module).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for calculating a two dimensional matrix $X_{ij}$ which is the number of times word $j$ occurred in the context of word $i$. The size of the context window $k$ (as a number of words, $k=2$ describes that the context contains the two words before and the two words after the central word) is also an argument of the function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 1., 2., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 2., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def co_occurrenceFreq(text, w2i, k): # text is a sequence of words ordered as they appear in the text\n",
    "    # note that there is no notion of sentence here...\n",
    "    \n",
    "    X_ij =  np.zeros((len(w2i), len(w2i)))\n",
    "    # write your code snippet here...\n",
    "    \n",
    "    # key:center word w_i, value: context of this word\n",
    "    word_contexts = defaultdict(list)\n",
    "\n",
    "    for w_i in w2i.keys():\n",
    "        for i,value in enumerate(text):\n",
    "            if w_i == value:\n",
    "                if i-k < 0 :\n",
    "                    context = text[0:i] + text[i+1:i+k+1]\n",
    "                else:\n",
    "                    context = text[i-k:i] + text[i+1:i+k+1]\n",
    "                word_contexts[w_i].append(context)\n",
    "#     print(word_contexts)          \n",
    "    #calculate times of word_j in context of center word_i\n",
    "    for i,w_i in enumerate(word_contexts.keys()):\n",
    "        for j,w_j in enumerate(w2i.keys()):\n",
    "            times = 0\n",
    "            for context in word_contexts[w_i]:\n",
    "                if w_j in context:\n",
    "                    times = times + context.count(w_j)\n",
    "            X_ij[i][j] = times\n",
    "            \n",
    "    return X_ij\n",
    "\n",
    "#test cases:\n",
    "textfile =\"In this in task you will implement the you can calculate gradient by hand).\"\n",
    "w2i,text = word2indexMapping(textfile)\n",
    "X_ij = co_occurrenceFreq(text,w2i,2)\n",
    "X_ij\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a GloVe model class with parameters $w$, $\\hat w$, $b$ and $\\hat b$. For a particular pair of words $i$, $j$, your forward function should implement $w_{i}^{T}\\hat w_{j} + b_{i} + \\hat b_{j}$. Assume a dimension of embedding to be $d$ which you will specify when creating an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.9181, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001E2B6B29BC8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Glove(nn.Module):\n",
    "\n",
    "    # write your model class here....\n",
    "    def __init__(self,X_ij,dimension):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = X_ij.shape[0]\n",
    "        self.d = dimension\n",
    "        #initialize parameters ùë§, ùë§ÃÇ , ùëè and ùëè^\n",
    "        self.W_1 = nn.Parameter(torch.randn(self.vocab_size, self.d, requires_grad=True))\n",
    "        self.W_2 = nn.Parameter(torch.randn(self.vocab_size, self.d, requires_grad=True))\n",
    "        \n",
    "        self.b_1 = nn.Parameter(torch.randn(self.vocab_size, requires_grad=True))\n",
    "        self.b_2 = nn.Parameter(torch.randn(self.vocab_size, requires_grad=True))\n",
    "        \n",
    "    def forward(self,i,j):\n",
    "        return torch.dot(self.W_1[i],self.W_2[j])+self.b_1[i]+self.b_2[j]\n",
    "\n",
    "#test case for model\n",
    "model = Glove(X_ij,2)\n",
    "print(model.forward(1,2))\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that implements the weighting function $f(X_{ij})$\n",
    "\n",
    "$f(x) = (\\frac{x}{100})^{\\frac{3}{4}}$ if x<100 \n",
    "\n",
    "$f(x) = 1 $ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053182958969449884"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weightFunction(X_ij, i, j):\n",
    "    # write your code snippet here\n",
    "    f = 0\n",
    "    if X_ij[i][j] < 100:\n",
    "        f = (X_ij[i][j]/100)**0.75\n",
    "    else:\n",
    "        f = 1\n",
    "    return f\n",
    "\n",
    "#test case\n",
    "weightFunction(X_ij,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train the model using stochastic gradient descent. Your each training instance would be a word-context pair ($i$, $j$) and the corresponding loss function would be \n",
    "\n",
    "$f(X_{ij})(w_{i}^{T}\\hat w_{j} + b_{i} + \\hat b_{j} - log(1 + X_{ij}))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [00:00<00:00, 6007.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [00:00<00:00, 4979.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [00:00<00:00, 4376.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [00:00<00:00, 4512.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [00:00<00:00, 5344.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finish training'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, X_ij, learning_rate=0.001, epochs=5):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)# use Adam as your optimizer\n",
    "    data = torch.from_numpy(X_ij).float()\n",
    "    vocab_size = X_ij.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        loss = torch.tensor(0) # gloabal loss\n",
    "        print(\"Starting epoch {}\".format(_+1))\n",
    "        # train across each word-conext pair\n",
    "        with tqdm(total=int(vocab_size**2)) as pbar:\n",
    "            for i in range(vocab_size):\n",
    "                for j in range(vocab_size):\n",
    "                    #loss function\n",
    "                    f = weightFunction(data,i,j)\n",
    "                    forward = model.forward(i,j)\n",
    "                    log_data = torch.log(1+data[i][i])\n",
    "                    # calculate inner_loss for word_pair(i,j)\n",
    "                    inner_loss = f*((forward-log_data)**2)\n",
    "                    loss = loss + inner_loss\n",
    "                    pbar.update(1)\n",
    "        #backpropagate for every training example\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(type(loss))\n",
    "        \n",
    "    return 'finish training'\n",
    "        \n",
    "#test case:\n",
    "train(model,X_ij)\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor,\"\\t\",model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to generate embeddings given a word. The embedding of a word with index $i$ would $w_i + \\hat w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0571, -0.6929])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getEmbedding(model, word, w2i):\n",
    "#     print(model.state_dict())\n",
    "    w1 = model.state_dict()['W_1']\n",
    "    w2 = model.state_dict()['W_2']\n",
    "    if word in w2i.keys():\n",
    "        i = w2i[word]\n",
    "        W = w1[i] + w2[i]\n",
    "        return W\n",
    "    \n",
    "#test case:\n",
    "getEmbedding(model,'you',w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the individual components together to train a Glove model from the given text file  'shakespeare-caesar.txt' with dimension 200 and a context window of 5.\n",
    "Manually inspect nearest neigbors for some self-picked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary:  3078\n"
     ]
    }
   ],
   "source": [
    "#open and read text\n",
    "with open('data_home_assignment_2/shakespeare-caesar.txt',\"r\") as f: \n",
    "    textfile = f.read()\n",
    "#generate word_to_index dictionary and text(list)\n",
    "w2i,text = word2indexMapping(textfile)\n",
    "#generate concurrence matrix\n",
    "X_ij = co_occurrenceFreq(text,w2i,5) # context_window size is 5\n",
    "print('size of vocabulary: ',X_ij.shape[0]) # vocabulary size\n",
    "# create the Glove model\n",
    "model = Glove(X_ij,200) # dimension 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 1781042/9474084 [05:17<21:21, 6002.05it/s]"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# model.cuda()\n",
    "train(model,X_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6732, -2.5370, -1.9711, -0.4504, -0.2566,  0.4666, -0.5044, -0.1568,\n",
      "         0.2660,  1.4180, -0.0242,  1.1922,  0.3318,  1.2960, -0.8224, -2.5935,\n",
      "        -2.3526,  0.0785,  0.7910,  1.3586, -2.2307, -1.1118, -0.9433,  0.3977,\n",
      "         0.9235,  0.7179, -0.3251, -0.0766,  0.2745, -0.6619,  1.5064, -0.9988,\n",
      "        -0.8563, -1.2591, -1.2737,  0.9747,  0.3047,  1.5791, -0.2592,  0.1422,\n",
      "        -1.3564, -0.9677,  0.4947, -2.8789, -1.5509, -3.4275, -2.2028, -1.2509,\n",
      "         0.3015,  1.1640, -1.6006, -3.7332, -0.1256, -0.1344,  1.5355, -0.7254,\n",
      "         0.3561, -0.3442, -0.2429, -2.4707, -0.4334, -1.7885, -1.5088,  0.6854,\n",
      "         0.6379,  2.6064,  1.0632,  1.7455, -1.6135, -1.4981,  0.7585, -0.1770,\n",
      "         0.6943, -0.2840, -1.2859,  0.3955,  2.6695, -0.1497,  1.1916,  0.0809,\n",
      "         0.6905, -0.7678,  0.2430, -2.1956,  2.4384, -2.4290, -0.4943,  3.0978,\n",
      "        -1.4817,  0.1274,  2.2315,  1.8575,  0.6328,  0.9677, -2.0755,  2.3361,\n",
      "        -0.0133,  0.0449,  2.5212, -1.3200,  1.3619,  0.7802, -0.7149,  1.2562,\n",
      "        -0.7700, -0.7616, -2.1799, -1.1417, -0.6531,  0.1208,  0.6105,  3.6321,\n",
      "         0.0779, -0.9828, -1.8504, -1.2850, -1.8838, -1.3128,  1.4104,  1.2087,\n",
      "         2.1905,  0.2986,  0.7067,  1.2053, -1.0341,  0.4387,  2.2118,  1.0031,\n",
      "        -0.1656, -0.7574,  0.3449, -1.6957,  0.4160, -0.8055, -0.2883, -0.4730,\n",
      "        -0.7179,  1.9186,  0.7691,  1.5022,  1.2508, -0.8357, -0.9992, -2.2657,\n",
      "         0.8589, -0.2415, -0.9400,  0.2469, -1.0964, -0.6101,  1.3003, -0.2052,\n",
      "         0.1101, -1.4925,  1.8339, -1.3532,  0.5763, -0.5352, -0.4709, -1.1634,\n",
      "         1.8830, -0.5412,  0.0598,  1.4408, -1.0359, -0.8088, -2.1709,  0.6376,\n",
      "         2.0585,  3.3390, -0.2544, -0.9198,  0.0461, -1.2769, -0.0663,  0.5210,\n",
      "         2.2654,  0.2912, -1.3190,  1.7119, -0.9073,  0.3195, -2.5855, -4.6389,\n",
      "         0.8285, -0.1520,  2.0782, -0.3545,  2.4830,  1.7344,  0.5921,  1.1311,\n",
      "        -1.6693,  2.2186,  1.5254, -0.5253,  0.9492,  2.0172, -1.1980, -0.6551])\n"
     ]
    }
   ],
   "source": [
    "print(getEmbedding(model,'like',w2i))\n",
    "# print(getEmbedding(model,'you',w2i))\n",
    "# print(getEmbedding(model,'i',w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a package which allows you to train word embeddings as well. The task is to take a text file (like 'shakespeare-caesar.txt') and generate embeddings of the vocabulary. You can consult the documentation here - \n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.LineSentence at 0x14616fa4c48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "\n",
    "path  = 'data_home_assignment_2/shakespeare-caesar.txt'\n",
    "sentences = word2vec.LineSentence(open(path,'r',encoding='utf-8',errors='ignore'))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.40851697  0.35675195 -0.15736736  0.21828593  0.8534455   0.48097837\n",
      "  0.15590157 -0.68053573  0.23185153 -0.25329414 -0.23145275 -0.07475159\n",
      " -0.06945339 -0.35280842  0.20342875  0.37044716 -0.5006525   0.4971659\n",
      " -0.04195439  0.52474755 -0.2756531   0.40530664  0.20769687 -0.02010888\n",
      " -0.4735823   0.090795   -0.14604501 -0.23641546  0.35008523 -0.14071035\n",
      "  0.31632656  0.03735245 -1.1408406   0.09548622 -0.40269765 -0.21565506\n",
      "  0.12023731  0.14155413 -0.79862416 -0.34802863  0.24203448  0.37179404\n",
      "  0.16187528 -0.5705363   0.35784858 -0.50281656  0.65102595  0.01796834\n",
      "  0.42673266 -0.03318949]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangr\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.9998339414596558),\n",
       " ('I', 0.9998040199279785),\n",
       " ('not', 0.9997918009757996),\n",
       " ('of', 0.9997735023498535),\n",
       " ('the', 0.9997362494468689),\n",
       " ('a', 0.9997299909591675),\n",
       " ('and', 0.9997232556343079),\n",
       " ('they', 0.9996997714042664),\n",
       " ('my', 0.9996960163116455),\n",
       " ('to', 0.9996954202651978)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train word_embedding model with gensim\n",
    "model = Word2Vec(sentences, \n",
    "                 size=50,      # the size of word vetor\n",
    "                 min_count=3,  # remove low frequency word\n",
    "                 window=5).wv  # set context window\n",
    "\n",
    "#trained result\n",
    "print(model.get_vector('you')) # word vector of 'you'\n",
    "# similar words of 'me'\n",
    "model.wv.most_similar('me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word similarity with pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of understanding how good the obtained embeddings are is the word similarity task. The file 'sim353.csv' contains a set of word pairs as well as their similarity as judged by humans (e.g., tiger,tiger,10). \n",
    "Also we provide a set of pre-trained embeddings in 'embeddings.pickle' in the form of a dictionary with words as keys and embeddings as values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Consider each word pair in the given file ('sim353.csv') and calculate the cosine similarity between them and then rank the word pairs based on the similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangr\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "dataset = pd.read_csv(\"data_home_assignment_2/sim353.csv\")\n",
    "#add attribute 'cosine similarity'\n",
    "dataset['cosine_similarity'] = float(0)\n",
    "\n",
    "with open('data_home_assignment_2/embeddings.pickle','rb') as f:\n",
    "    word_vector = pickle.load(f)\n",
    "# print(type(word_vector))\n",
    "\n",
    "# calaulate the cosine value beteween two vectors\n",
    "def cos_sim(a, b):\n",
    "    dot_product = torch.dot(a, b)\n",
    "    norm_a = torch.norm(a)\n",
    "    norm_b = torch.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "#update the value of 'cosine_similarity'\n",
    "for i in range(len(dataset)):\n",
    "    if dataset['Word 1'][i] in word_vector.keys() and dataset['Word 2'][i] in word_vector.keys():\n",
    "        dataset['cosine_similarity'][i] = cos_sim(torch.tensor(word_vector[dataset['Word 1'][i]]).cuda(), torch.tensor(word_vector[dataset['Word 2'][i]]).cuda())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tiger</td>\n",
       "      <td>organism</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.036870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.038883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>cup</td>\n",
       "      <td>artifact</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.045669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.088859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>cup</td>\n",
       "      <td>entity</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.095603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.122765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>precedent</td>\n",
       "      <td>group</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.122940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>precedent</td>\n",
       "      <td>cognition</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.124228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>monk</td>\n",
       "      <td>oracle</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.138562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>glass</td>\n",
       "      <td>magician</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.158012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>precedent</td>\n",
       "      <td>collection</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.159243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>monk</td>\n",
       "      <td>slave</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.160540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>delay</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.171557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>impartiality</td>\n",
       "      <td>interest</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.172755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>crane</td>\n",
       "      <td>implement</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.179740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>decoration</td>\n",
       "      <td>valor</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.181139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.181856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.182654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>production</td>\n",
       "      <td>hike</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.183246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>territory</td>\n",
       "      <td>kilometer</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.183836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>cup</td>\n",
       "      <td>substance</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.190121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>size</td>\n",
       "      <td>prominence</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.191225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>cup</td>\n",
       "      <td>object</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.197506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>asylum</td>\n",
       "      <td>madhouse</td>\n",
       "      <td>8.87</td>\n",
       "      <td>0.199657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>cup</td>\n",
       "      <td>tableware</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.206225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sign</td>\n",
       "      <td>recess</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.206303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>lad</td>\n",
       "      <td>wizard</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.207744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>reason</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.210041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tiger</td>\n",
       "      <td>fauna</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.226339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>mile</td>\n",
       "      <td>kilometer</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.646130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>psychology</td>\n",
       "      <td>science</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.650004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>psychology</td>\n",
       "      <td>psychiatry</td>\n",
       "      <td>8.08</td>\n",
       "      <td>0.650045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.655002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>drink</td>\n",
       "      <td>eat</td>\n",
       "      <td>6.87</td>\n",
       "      <td>0.664106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>midday</td>\n",
       "      <td>noon</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.665748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bread</td>\n",
       "      <td>butter</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.667959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>vodka</td>\n",
       "      <td>gin</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.668546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>calculation</td>\n",
       "      <td>computation</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.669189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>fuck</td>\n",
       "      <td>sex</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.670561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>day</td>\n",
       "      <td>summer</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.670856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>computer</td>\n",
       "      <td>software</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.672375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>seafood</td>\n",
       "      <td>lobster</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.674077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.674091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>coast</td>\n",
       "      <td>shore</td>\n",
       "      <td>9.10</td>\n",
       "      <td>0.677802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>type</td>\n",
       "      <td>kind</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.679077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.684565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>credit</td>\n",
       "      <td>card</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.689643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>street</td>\n",
       "      <td>avenue</td>\n",
       "      <td>8.88</td>\n",
       "      <td>0.708214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "      <td>0.716078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>physics</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.729284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>car</td>\n",
       "      <td>automobile</td>\n",
       "      <td>8.94</td>\n",
       "      <td>0.733075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>8.58</td>\n",
       "      <td>0.759618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>football</td>\n",
       "      <td>basketball</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.799937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.804799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>championship</td>\n",
       "      <td>tournament</td>\n",
       "      <td>8.36</td>\n",
       "      <td>0.807413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>football</td>\n",
       "      <td>soccer</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.814274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0.815317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.15</td>\n",
       "      <td>0.815317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1        Word 2  Human (mean)  cosine_similarity\n",
       "0            love           sex          6.77           0.000000\n",
       "105         tiger      organism          4.77           0.036870\n",
       "88        rooster        voyage          0.62           0.038883\n",
       "136           cup      artifact          2.92           0.045669\n",
       "31      professor      cucumber          0.31           0.088859\n",
       "138           cup        entity          2.15           0.095603\n",
       "87           noon        string          0.54           0.122765\n",
       "131     precedent         group          1.77           0.122940\n",
       "128     precedent     cognition          2.81           0.124228\n",
       "76           monk        oracle          5.00           0.138562\n",
       "86          glass      magician          2.08           0.158012\n",
       "130     precedent    collection          2.50           0.159243\n",
       "82           monk         slave          0.92           0.160540\n",
       "171         delay        racism          1.19           0.171557\n",
       "226  impartiality      interest          5.16           0.172755\n",
       "73          crane     implement          2.69           0.179740\n",
       "168    decoration         valor          5.63           0.181139\n",
       "85          chord         smile          0.54           0.181856\n",
       "32           king       cabbage          0.23           0.182654\n",
       "235    production          hike          1.75           0.183246\n",
       "206     territory     kilometer          5.28           0.183836\n",
       "141           cup     substance          1.92           0.190121\n",
       "312          size    prominence          5.31           0.191225\n",
       "137           cup        object          3.69           0.197506\n",
       "64         asylum      madhouse          8.87           0.199657\n",
       "134           cup     tableware          6.85           0.206225\n",
       "156          sign        recess          2.38           0.206303\n",
       "84            lad        wizard          0.92           0.207744\n",
       "192        reason  hypertension          2.31           0.210041\n",
       "106         tiger         fauna          5.62           0.226339\n",
       "..            ...           ...           ...                ...\n",
       "157          mile     kilometer          8.66           0.646130\n",
       "116    psychology       science          6.71           0.650004\n",
       "108    psychology    psychiatry          8.08           0.650045\n",
       "42            law        lawyer          8.38           0.655002\n",
       "56          drink           eat          6.87           0.664106\n",
       "66         midday          noon          9.29           0.665748\n",
       "12          bread        butter          6.19           0.667959\n",
       "51          vodka           gin          8.46           0.668546\n",
       "241   calculation   computation          8.44           0.669189\n",
       "37           fuck           sex          9.44           0.670561\n",
       "282           day        summer          3.94           0.670856\n",
       "252      computer      software          8.50           0.672375\n",
       "270       seafood       lobster          8.70           0.674077\n",
       "9      television         radio          6.77           0.674091\n",
       "63          coast         shore          9.10           0.677802\n",
       "220          type          kind          8.97           0.679077\n",
       "319      aluminum         metal          7.83           0.684565\n",
       "213        credit          card          8.06           0.689643\n",
       "229        street        avenue          8.88           0.708214\n",
       "331       weather      forecast          8.34           0.716078\n",
       "48        physics     chemistry          7.35           0.729284\n",
       "59            car    automobile          8.94           0.733075\n",
       "33           king         queen          8.58           0.759618\n",
       "39       football    basketball          6.81           0.799937\n",
       "289           man         woman          8.30           0.804799\n",
       "279  championship    tournament          8.36           0.807413\n",
       "38       football        soccer          9.03           0.814274\n",
       "90          money          cash          9.08           0.815317\n",
       "30          money          cash          9.15           0.815317\n",
       "2           tiger         tiger         10.00           1.000000\n",
       "\n",
       "[335 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort the word pairs based on the similarity value\n",
    "sorted_dataset_1 = dataset.sort_values(by = 'cosine_similarity')\n",
    "sorted_dataset_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Rank the word pairs based on the similarity values as determined by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.182654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.088859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.122765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.181856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.038883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>sugar</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.249256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stock</td>\n",
       "      <td>jaguar</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.276747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stock</td>\n",
       "      <td>life</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.411336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>monk</td>\n",
       "      <td>slave</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.160540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>lad</td>\n",
       "      <td>wizard</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.207744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>delay</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.171557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>drink</td>\n",
       "      <td>ear</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.270334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stock</td>\n",
       "      <td>phone</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.306102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>holy</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.266039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>production</td>\n",
       "      <td>hike</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.183246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>precedent</td>\n",
       "      <td>group</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.122940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>month</td>\n",
       "      <td>hotel</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.333641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>energy</td>\n",
       "      <td>secretary</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.340388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stock</td>\n",
       "      <td>egg</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.339092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>forest</td>\n",
       "      <td>graveyard</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.298963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>cup</td>\n",
       "      <td>substance</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.190121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>possibility</td>\n",
       "      <td>girl</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.310412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cemetery</td>\n",
       "      <td>woodland</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.379716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>glass</td>\n",
       "      <td>magician</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.158012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>cup</td>\n",
       "      <td>entity</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.095603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>direction</td>\n",
       "      <td>combination</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.400209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>reason</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.210041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sign</td>\n",
       "      <td>recess</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.206303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>problem</td>\n",
       "      <td>airport</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.282557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>cup</td>\n",
       "      <td>article</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.258458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.655002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>money</td>\n",
       "      <td>dollar</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0.614731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>calculation</td>\n",
       "      <td>computation</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.669189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>planet</td>\n",
       "      <td>star</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.519998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>vodka</td>\n",
       "      <td>gin</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.668546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>money</td>\n",
       "      <td>bank</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.602752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>computer</td>\n",
       "      <td>software</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.672375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>murder</td>\n",
       "      <td>manslaughter</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.569898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>8.58</td>\n",
       "      <td>0.759618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>mile</td>\n",
       "      <td>kilometer</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.646130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>seafood</td>\n",
       "      <td>lobster</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.674077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>furnace</td>\n",
       "      <td>stove</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0.585582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>environment</td>\n",
       "      <td>ecology</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0.535517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>boy</td>\n",
       "      <td>lad</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.531423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>asylum</td>\n",
       "      <td>madhouse</td>\n",
       "      <td>8.87</td>\n",
       "      <td>0.199657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>street</td>\n",
       "      <td>avenue</td>\n",
       "      <td>8.88</td>\n",
       "      <td>0.708214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>car</td>\n",
       "      <td>automobile</td>\n",
       "      <td>8.94</td>\n",
       "      <td>0.733075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>gem</td>\n",
       "      <td>jewel</td>\n",
       "      <td>8.96</td>\n",
       "      <td>0.578007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>type</td>\n",
       "      <td>kind</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.679077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>magician</td>\n",
       "      <td>wizard</td>\n",
       "      <td>9.02</td>\n",
       "      <td>0.513534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>football</td>\n",
       "      <td>soccer</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.814274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>money</td>\n",
       "      <td>currency</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0.522259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0.815317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>coast</td>\n",
       "      <td>shore</td>\n",
       "      <td>9.10</td>\n",
       "      <td>0.677802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.15</td>\n",
       "      <td>0.815317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>dollar</td>\n",
       "      <td>buck</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0.426276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>journey</td>\n",
       "      <td>voyage</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>midday</td>\n",
       "      <td>noon</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.665748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>fuck</td>\n",
       "      <td>sex</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.670561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 1        Word 2  Human (mean)  cosine_similarity\n",
       "32          king       cabbage          0.23           0.182654\n",
       "31     professor      cucumber          0.31           0.088859\n",
       "87          noon        string          0.54           0.122765\n",
       "85         chord         smile          0.54           0.181856\n",
       "88       rooster        voyage          0.62           0.038883\n",
       "308        sugar      approach          0.88           0.249256\n",
       "22         stock        jaguar          0.92           0.276747\n",
       "26         stock          life          0.92           0.411336\n",
       "82          monk         slave          0.92           0.160540\n",
       "84           lad        wizard          0.92           0.207744\n",
       "171        delay        racism          1.19           0.171557\n",
       "54         drink           ear          1.31           0.270334\n",
       "21         stock         phone          1.62           0.306102\n",
       "36          holy           sex          1.62           0.266039\n",
       "235   production          hike          1.75           0.183246\n",
       "131    precedent         group          1.77           0.122940\n",
       "219        month         hotel          1.81           0.333641\n",
       "145       energy     secretary          1.81           0.340388\n",
       "23         stock           egg          1.81           0.339092\n",
       "80        forest     graveyard          1.85           0.298963\n",
       "141          cup     substance          1.92           0.190121\n",
       "300  possibility          girl          1.94           0.310412\n",
       "77      cemetery      woodland          2.08           0.379716\n",
       "86         glass      magician          2.08           0.158012\n",
       "138          cup        entity          2.15           0.095603\n",
       "227    direction   combination          2.25           0.400209\n",
       "192       reason  hypertension          2.31           0.210041\n",
       "156         sign        recess          2.38           0.206303\n",
       "211      problem       airport          2.38           0.282557\n",
       "135          cup       article          2.40           0.258458\n",
       "..           ...           ...           ...                ...\n",
       "42           law        lawyer          8.38           0.655002\n",
       "89         money        dollar          8.42           0.614731\n",
       "241  calculation   computation          8.44           0.669189\n",
       "119       planet          star          8.45           0.519998\n",
       "51         vodka           gin          8.46           0.668546\n",
       "95         money          bank          8.50           0.602752\n",
       "252     computer      software          8.50           0.672375\n",
       "291       murder  manslaughter          8.53           0.569898\n",
       "33          king         queen          8.58           0.759618\n",
       "157         mile     kilometer          8.66           0.646130\n",
       "270      seafood       lobster          8.70           0.674077\n",
       "67       furnace         stove          8.79           0.585582\n",
       "287  environment       ecology          8.81           0.535517\n",
       "62           boy           lad          8.83           0.531423\n",
       "64        asylum      madhouse          8.87           0.199657\n",
       "229       street        avenue          8.88           0.708214\n",
       "59           car    automobile          8.94           0.733075\n",
       "60           gem         jewel          8.96           0.578007\n",
       "220         type          kind          8.97           0.679077\n",
       "65      magician        wizard          9.02           0.513534\n",
       "38      football        soccer          9.03           0.814274\n",
       "91         money      currency          9.04           0.522259\n",
       "90         money          cash          9.08           0.815317\n",
       "63         coast         shore          9.10           0.677802\n",
       "30         money          cash          9.15           0.815317\n",
       "249       dollar          buck          9.22           0.426276\n",
       "61       journey        voyage          9.29           0.643200\n",
       "66        midday          noon          9.29           0.665748\n",
       "37          fuck           sex          9.44           0.670561\n",
       "2          tiger         tiger         10.00           1.000000\n",
       "\n",
       "[335 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort the word pairs based on the similarity value\n",
    "sorted_dataset_2 = dataset.sort_values(by = 'Human (mean)')\n",
    "sorted_dataset_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Calculate the spearman rank correlation between the two ranked list of word-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman rank correlation: 0.12702782069124008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_1</th>\n",
       "      <th>index_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.118961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_2</th>\n",
       "      <td>0.118961</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index_1   index_2\n",
       "index_1  1.000000  0.118961\n",
       "index_2  0.118961  1.000000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s1 = sorted_dataset_1.drop(index = 0) # delete 'love' word pair\n",
    "s2 = sorted_dataset_2.drop(index = 0)\n",
    "# print(s2)\n",
    "d = np.array(s1.index) - np.array(s2.index)\n",
    "# d = np.array(sorted_dataset_1.index) - np.array(sorted_dataset_2.index)\n",
    "temp = sum(d**2)*6\n",
    "n = len(sorted_dataset_1)\n",
    "r = 1 - temp/(n*(n**2 - 1))\n",
    "print('spearman rank correlation:', r)\n",
    "\n",
    "#using pandas to calculate spearman rank correaltion\n",
    "values = {\"index_1\":sorted_dataset_1.index,\n",
    "\n",
    "          \"index_2\":sorted_dataset_2.index} # include 'love' word-pair\n",
    "\n",
    "df = pd.DataFrame(data=values)\n",
    "corrrelation = df.corr(method=\"spearman\");\n",
    "corrrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a set of news articles which are to be labelled 1 or 0. The data is split into 3 groups (train/test/validation). Each group is further divided into 2 files which consists of the text(ending with \\_X.p) and the label (ending with \\_y.p) respectively. Each datapoint is a list of words and they are all accumulated in a list forming a list of lists. The label file is a list of labels (0/1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will design a character-level CNN. So the first task would be to obtain a one-hot encoding for the characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocabulary of caracters is provided for your reference.. \n",
    "vocabulary = list(\"\"\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'/\\|_@#$%ÀÜ&*Àú‚Äò+-=<>()[]{}\"\"\")\n",
    "# vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def characterEncoding(vocabulary):\n",
    "    c2v = {} # dictionary with key as a character and one-hot encoding as value\n",
    "    # write your code snippet here..\n",
    "    for i,v in enumerate(vocabulary):\n",
    "        encoding = torch.zeros(len(vocabulary))\n",
    "        encoding[i] = 1\n",
    "        if v not in c2v.keys():\n",
    "            c2v[v] = encoding\n",
    "    return c2v\n",
    "#test case:\n",
    "c2v = characterEncoding(vocabulary)\n",
    "print(len(c2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode a sentence as a 2D matrix with each row representing the one-hot encoding of a character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def sentence2tensor(sentence,c2v):\n",
    "    # write your code snippet here\n",
    "    # initialize 2D matrix\n",
    "    chars = ''.join(sentence)\n",
    "    s2v  = torch.zeros(len(chars), len(c2v['a']))\n",
    "    # update the row value of matrix\n",
    "    for i,char in enumerate(chars):\n",
    "        if char in c2v.keys():\n",
    "            s2v[i] = c2v[char]\n",
    "    return s2v\n",
    "\n",
    "#test case:\n",
    "c2v = characterEncoding(vocabulary)\n",
    "s2v = sentence2tensor(['i', 'love', 'you'],c2v)\n",
    "print(s2v[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataset class for feeding data to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934\n",
      "encoding vector: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.]]])\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    # write your code snippet here...\n",
    "    def __init__(self,_X,_y,sentence2tensor,c2v):\n",
    "        super(NewsDataset,self).__init__()\n",
    "        self._X = _X #datapoints\n",
    "        self._y = _y #labels\n",
    "        #transform sequences of words into onehot encoding matix\n",
    "        self.sentence2tensor = sentence2tensor\n",
    "        self.c2v = c2v\n",
    "        \n",
    "    def __len__(self):\n",
    "         # Returns the size of the dataset\n",
    "        return len(self._y)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #we choose the first 150 character and its vectors as input of the model\n",
    "        encoding = self.sentence2tensor(self._X[index],self.c2v)[:100]\n",
    "        s2v = Variable(torch.unsqueeze(encoding, dim=0).float(), requires_grad=False)\n",
    "        return s2v, self._y[index]\n",
    "\n",
    "#test case   \n",
    "with open('./data_home_assignment_2/Data/news_computer_train_X.p', 'rb') as f:\n",
    "    train_datapoits = pickle.load(f)\n",
    "with open('./data_home_assignment_2/Data/news_computer_train_y.p', 'rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "\n",
    "train_dataset = NewsDataset(train_datapoits,train_labels,sentence2tensor,c2v)\n",
    "print(len(train_dataset))\n",
    "s2v,label = train_dataset[0]\n",
    "print('encoding vector:', s2v)\n",
    "print('label:',label)\n",
    "\n",
    "# with open('./data_home_assignment_2/Data/news_computer_test_X.p', 'rb') as f:\n",
    "#     test_datapoits = pickle.load(f)\n",
    "# with open('./data_home_assignment_2/Data/news_computer_test_y.p', 'rb') as f:\n",
    "#     test_labels = pickle.load(f)\n",
    "\n",
    "# test_dataset = NewsDataset(test_datapoits,test_labels,sentence2tensor,c2v)\n",
    "\n",
    "# print(len(test_dataset))\n",
    "# s2v,label = train_dataset[5]\n",
    "# print(label)\n",
    "# test_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "# for i,v in test_dataloader:\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model class with 2 layers of convolutions each followed by a ReLU unit. The model should have linear layer which maps to the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (conv_1): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_2): Sequential(\n",
      "    (0): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out): Linear(in_features=27600, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Classifier(nn.Module):\n",
    "    # write your code snippet here\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # set convolutions layer\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=4, \n",
    "                      kernel_size=5, \n",
    "                      stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(4,4,5,1,2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # linear layer,which aims to map 2 classes\n",
    "        self.out = torch.nn.Linear(27600, 2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "#test cases\n",
    "model = Classifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a train function which trains on the train dataset. You can use binary cross entropy as your loss function and Adam as your optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:18,  1.63it/s]                        \n",
      "30it [00:19,  1.53it/s]                        \n",
      "30it [00:19,  1.56it/s]                        \n",
      "30it [00:19,  1.52it/s]                        \n",
      "30it [00:19,  1.56it/s]                        \n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "def train(model, train_dataset, epochs=5, batchsize=32, learning_rate=0.0001):\n",
    "    # write your code snippet here\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batchsize,shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    for _ in range(epochs):\n",
    "         with tqdm(total=int(len(train_dataset)/batchsize)) as pbar:\n",
    "                for inp,labels in train_dataloader:\n",
    "                    inp,labels = Variable(inp),Variable(labels)\n",
    "                    out = model(inp)\n",
    "                    loss = criterion(out, labels)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "#test case\n",
    "train(model,train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a test function which takes the trained model and test dataset and outputs the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def test(model, test_datset, batchsize=32):\n",
    "    # write your code snippet here\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batchsize,shuffle=True)\n",
    "    accuracy_sum = []\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs,labels = Variable(inputs),Variable(labels)\n",
    "        out = model(inputs)\n",
    "        accuracy = torch.max(out,1)[1].numpy() == labels.numpy()\n",
    "        accuracy_sum.append(accuracy.mean())\n",
    "    print('accuracy:\\t',sum(accuracy_sum)/len(accuracy_sum))\n",
    "    return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:18,  1.65it/s]                        \n",
      "30it [00:18,  1.63it/s]                        \n",
      "30it [00:19,  1.57it/s]                        \n",
      "30it [00:18,  1.58it/s]                        \n",
      "30it [00:18,  1.59it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\t 0.62625\n"
     ]
    }
   ],
   "source": [
    "# putting it altogether\n",
    "\n",
    "#train_dataset\n",
    "with open('./data_home_assignment_2/Data/news_computer_train_X.p', 'rb') as f:\n",
    "    train_datapoits = pickle.load(f)\n",
    "with open('./data_home_assignment_2/Data/news_computer_train_y.p', 'rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "\n",
    "train_dataset = NewsDataset(train_datapoits,train_labels,sentence2tensor,c2v)\n",
    "\n",
    "#test_dataset\n",
    "with open('./data_home_assignment_2/Data/news_computer_test_X.p', 'rb') as f:\n",
    "    test_datapoits = pickle.load(f)\n",
    "with open('./data_home_assignment_2/Data/news_computer_test_y.p', 'rb') as f:\n",
    "    test_labels = pickle.load(f)\n",
    "\n",
    "test_dataset = NewsDataset(test_datapoits,test_labels,sentence2tensor,c2v)\n",
    "\n",
    "model = Classifier()\n",
    "\n",
    "train(model, train_dataset)\n",
    "test(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
