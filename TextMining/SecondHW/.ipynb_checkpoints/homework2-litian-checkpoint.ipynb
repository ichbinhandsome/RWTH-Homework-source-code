{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 2\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. Submissions of single students will not be accepted! Please use the Forum in case of doubt in order to find team mates!\n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Tuesday, 7th of January 23:55h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples.\n",
    "\n",
    "Do not forget to specify the (First_name, Last_name, student_id (matrikelnummer)) of all contributing students in the jupyter notebook here:\n",
    "\n",
    "Student 1: `First_name, Last_name, student_id`\n",
    "\n",
    "Student 2: `First_name, Last_name, student_id`\n",
    "\n",
    "Student 3: `First_name, Last_name, student_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing GloVe\n",
    "In this task you will implement the glove algorithm using PyTorch. (One advantage is that you need not calculate gradient by hand, but you can take advantage of the autograd module). The task will require implementation of certain functions, which we look into step-by-step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for word to index mapping. Since the model won't be able to take strings as input we will convert them into indices. The function will generate a mapping  w2i which uses words as keys and corresponding indices as values e.g., `w2i['walk'] = 42`. As Preprocessing, remove all punctuations and convert all words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2indexMapping(textfile):\n",
    "    w2i = {}\n",
    "    text = [] # sequence of words as they appear in the text after removing punctuations\n",
    "    # write your code snippet here....\n",
    "    \n",
    "    \n",
    "    return w2i, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for calculating a two dimensional matrix $X_{ij}$ which is the number of times word $j$ occurred in the context of word $i$. The size of the context window $k$ (as a number of words, $k=2$ describes that the context contains the two words before and the two words after the central word) is also an argument of the function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrenceFreq(text, w2i, k): # text is a sequence of words ordered as they appear in the text\n",
    "    # note that there is no notion of sentence here...\n",
    "    \n",
    "    X_ij =  np.zeros((len(w2i), len(w2i)))\n",
    "    # write your code snippet here...\n",
    "    \n",
    "    \n",
    "    return X_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a GloVe model class with parameters $w$, $\\hat w$, $b$ and $\\hat b$. For a particular pair of words $i$, $j$, your forward function should implement $w_{i}^{T}\\hat w_{j} + b_{i} + \\hat b_{j}$. Assume a dimension of embedding to be $d$ which you will specify when creating an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    # write your model class here....\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that implements the weighting function $f(X_{ij})$\n",
    "\n",
    "$f(x) = (\\frac{x}{100})^{\\frac{3}{4}}$ if x<100 \n",
    "\n",
    "$f(x) = 1 $ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightFunction(X_ij, i, j):\n",
    "    f = 0\n",
    "    \n",
    "    # write your code snippet here\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train the model using stochastic gradient descent. Your each training instance would be a word-context pair ($i$, $j$) and the corresponding loss function would be \n",
    "\n",
    "$f(X_{ij})(w_{i}^{T}\\hat w_{j} + b_{i} + \\hat b_{j} - log(1 + X_{ij}))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_ij, learning_rate=0.001, epochs=5):\n",
    "    optimizer = # use Adam as your optimizer\n",
    "    for _ in epochs:\n",
    "        # train across each word-conext pair\n",
    "        # calculate loss\n",
    "        # backpropagate for every training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to generate embeddings given a word. The embedding of a word with index $i$ would $w_i + \\hat w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(model, word):\n",
    "    # write your code snippet here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the individual components together to train a Glove model from the given text file  'shakespeare-caesar.txt' with dimension 200 and a context window of 5.\n",
    "Manually inspect nearest neigbors for some self-picked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a package which allows you to train word embeddings as well. The task is to take a text file (like 'shakespeare-caesar.txt') and generate embeddings of the vocabulary. You can consult the documentation here - \n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word similarity with pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of understanding how good the obtained embeddings are is the word similarity task. The file 'sim353.csv' contains a set of word pairs as well as their similarity as judged by humans (e.g., tiger,tiger,10). \n",
    "Also we provide a set of pre-trained embeddings in 'embeddings.pickle' in the form of a dictionary with words as keys and embeddings as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('./embeddings.pickle', 'rb') as handle:\n",
    "    pickle_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Consider each word pair in the given file ('sim353.csv') and calculate the cosine similarity between them and then rank the word pairs based on the similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiger,cat': 111.0,\n",
       " 'tiger,tiger': 1.0,\n",
       " 'book,paper': 75.0,\n",
       " 'computer,keyboard': 67.0,\n",
       " 'computer,internet': 41.0,\n",
       " 'plane,car': 148.0,\n",
       " 'train,car': 107.0,\n",
       " 'telephone,communication': 63.0,\n",
       " 'television,radio': 16.0,\n",
       " 'media,radio': 114.0,\n",
       " 'drug,abuse': 40.0,\n",
       " 'bread,butter': 23.0,\n",
       " 'cucumber,potato': 91.0,\n",
       " 'doctor,nurse': 37.0,\n",
       " 'professor,doctor': 136.0,\n",
       " 'student,professor': 55.0,\n",
       " 'smart,student': 208.0,\n",
       " 'smart,stupid': 92.0,\n",
       " 'company,stock': 99.0,\n",
       " 'stock,market': 51.0,\n",
       " 'stock,phone': 272.0,\n",
       " 'stock,jaguar': 282.0,\n",
       " 'stock,egg': 252.0,\n",
       " 'fertility,egg': 242.0,\n",
       " 'stock,live': 264.0,\n",
       " 'stock,life': 182.0,\n",
       " 'book,library': 76.0,\n",
       " 'bank,money': 44.5,\n",
       " 'wood,forest': 119.0,\n",
       " 'money,cash': 2.0,\n",
       " 'professor,cucumber': 330.0,\n",
       " 'king,cabbage': 316.0,\n",
       " 'king,queen': 7.0,\n",
       " 'king,rook': 273.0,\n",
       " 'bishop,rabbi': 255.0,\n",
       " 'holy,sex': 285.0,\n",
       " 'fuck,sex': 20.0,\n",
       " 'football,soccer': 3.0,\n",
       " 'football,basketball': 6.0,\n",
       " 'football,tennis': 53.0,\n",
       " 'tennis,racket': 123.0,\n",
       " 'law,lawyer': 26.0,\n",
       " 'movie,star': 65.0,\n",
       " 'movie,popcorn': 209.0,\n",
       " 'movie,critic': 229.0,\n",
       " 'movie,theater': 94.0,\n",
       " 'physics,proton': 270.0,\n",
       " 'physics,chemistry': 9.0,\n",
       " 'space,chemistry': 281.0,\n",
       " 'alcohol,chemistry': 253.0,\n",
       " 'vodka,gin': 22.0,\n",
       " 'vodka,brandy': 59.0,\n",
       " 'drink,car': 259.0,\n",
       " 'drink,ear': 284.0,\n",
       " 'drink,mouth': 110.0,\n",
       " 'drink,eat': 25.0,\n",
       " 'baby,mother': 48.0,\n",
       " 'drink,mother': 217.0,\n",
       " 'car,automobile': 8.0,\n",
       " 'gem,jewel': 56.0,\n",
       " 'journey,voyage': 30.0,\n",
       " 'boy,lad': 85.0,\n",
       " 'coast,shore': 15.0,\n",
       " 'asylum,madhouse': 310.0,\n",
       " 'magician,wizard': 100.0,\n",
       " 'midday,noon': 24.0,\n",
       " 'furnace,stove': 52.0,\n",
       " 'food,fruit': 73.0,\n",
       " 'bird,cock': 297.0,\n",
       " 'bird,crane': 117.0,\n",
       " 'tool,implement': 130.0,\n",
       " 'brother,monk': 226.0,\n",
       " 'crane,implement': 319.0,\n",
       " 'lad,brother': 200.0,\n",
       " 'journey,car': 220.0,\n",
       " 'monk,oracle': 325.0,\n",
       " 'cemetery,woodland': 213.0,\n",
       " 'food,rooster': 302.0,\n",
       " 'coast,hill': 177.0,\n",
       " 'forest,graveyard': 274.0,\n",
       " 'shore,woodland': 233.0,\n",
       " 'monk,slave': 322.0,\n",
       " 'coast,forest': 179.0,\n",
       " 'lad,wizard': 307.0,\n",
       " 'chord,smile': 317.0,\n",
       " 'glass,magician': 324.0,\n",
       " 'noon,string': 328.0,\n",
       " 'rooster,voyage': 332.0,\n",
       " 'money,dollar': 39.0,\n",
       " 'money,currency': 95.0,\n",
       " 'money,wealth': 31.0,\n",
       " 'money,property': 134.0,\n",
       " 'money,possession': 234.0,\n",
       " 'money,bank': 44.5,\n",
       " 'money,deposit': 90.0,\n",
       " 'money,withdrawal': 227.0,\n",
       " 'money,laundering': 210.0,\n",
       " 'money,operation': 203.0,\n",
       " 'tiger,jaguar': 188.0,\n",
       " 'tiger,feline': 293.0,\n",
       " 'tiger,carnivore': 298.0,\n",
       " 'tiger,mammal': 289.0,\n",
       " 'tiger,animal': 180.0,\n",
       " 'tiger,organism': 333.0,\n",
       " 'tiger,fauna': 305.0,\n",
       " 'tiger,zoo': 173.0,\n",
       " 'psychology,psychiatry': 27.0,\n",
       " 'psychology,anxiety': 144.0,\n",
       " 'psychology,fear': 243.0,\n",
       " 'psychology,depression': 172.0,\n",
       " 'psychology,clinic': 239.0,\n",
       " 'psychology,doctor': 240.0,\n",
       " 'psychology,mind': 128.0,\n",
       " 'psychology,health': 142.0,\n",
       " 'psychology,science': 28.0,\n",
       " 'psychology,discipline': 106.0,\n",
       " 'psychology,cognition': 57.0,\n",
       " 'planet,star': 97.0,\n",
       " 'planet,constellation': 246.0,\n",
       " 'planet,moon': 50.0,\n",
       " 'planet,sun': 160.0,\n",
       " 'planet,galaxy': 112.0,\n",
       " 'planet,space': 66.0,\n",
       " 'planet,astronomer': 301.0,\n",
       " 'precedent,example': 237.0,\n",
       " 'precedent,information': 303.0,\n",
       " 'precedent,cognition': 326.0,\n",
       " 'precedent,law': 138.0,\n",
       " 'precedent,collection': 323.0,\n",
       " 'precedent,group': 327.0,\n",
       " 'precedent,antecedent': 257.0,\n",
       " 'cup,coffee': 104.0,\n",
       " 'cup,tableware': 309.0,\n",
       " 'cup,article': 292.0,\n",
       " 'cup,artifact': 331.0,\n",
       " 'cup,object': 311.0,\n",
       " 'cup,entity': 329.0,\n",
       " 'cup,drink': 131.0,\n",
       " 'cup,food': 244.0,\n",
       " 'cup,substance': 313.0,\n",
       " 'cup,liquid': 215.0,\n",
       " 'jaguar,cat': 267.0,\n",
       " 'jaguar,car': 147.0,\n",
       " 'energy,secretary': 250.0,\n",
       " 'secretary,senate': 102.0,\n",
       " 'energy,laboratory': 199.0,\n",
       " 'computer,laboratory': 145.0,\n",
       " 'weapon,secret': 190.0,\n",
       " 'investigation,effort': 126.0,\n",
       " 'news,report': 38.0,\n",
       " 'canyon,landscape': 232.0,\n",
       " 'image,surface': 167.0,\n",
       " 'discovery,space': 156.0,\n",
       " 'water,seepage': 230.0,\n",
       " 'sign,recess': 308.0,\n",
       " 'mile,kilometer': 29.0,\n",
       " 'computer,news': 192.0,\n",
       " 'territory,surface': 279.0,\n",
       " 'atmosphere,landscape': 219.0,\n",
       " 'president,medal': 249.0,\n",
       " 'war,troops': 42.0,\n",
       " 'record,number': 72.0,\n",
       " 'skin,eye': 49.0,\n",
       " 'theater,history': 176.0,\n",
       " 'volunteer,motto': 296.0,\n",
       " 'prejudice,recognition': 291.0,\n",
       " 'decoration,valor': 318.0,\n",
       " 'century,year': 135.0,\n",
       " 'century,nation': 150.0,\n",
       " 'delay,racism': 321.0,\n",
       " 'delay,news': 265.0,\n",
       " 'minister,party': 187.0,\n",
       " 'peace,plan': 183.0,\n",
       " 'minority,peace': 268.0,\n",
       " 'attempt,peace': 216.0,\n",
       " 'government,crisis': 64.0,\n",
       " 'deployment,departure': 294.0,\n",
       " 'deployment,withdrawal': 241.0,\n",
       " 'energy,crisis': 197.0,\n",
       " 'announcement,news': 84.0,\n",
       " 'announcement,effort': 196.0,\n",
       " 'stroke,hospital': 198.0,\n",
       " 'disability,death': 193.0,\n",
       " 'victim,emergency': 211.0,\n",
       " 'treatment,recovery': 103.0,\n",
       " 'journal,association': 101.0,\n",
       " 'doctor,personnel': 275.0,\n",
       " 'doctor,liability': 299.0,\n",
       " 'liability,insurance': 33.0,\n",
       " 'school,center': 96.0,\n",
       " 'reason,hypertension': 306.0,\n",
       " 'reason,criterion': 221.0,\n",
       " 'hundred,percent': 163.0,\n",
       " 'hospital,infrastructure': 277.0,\n",
       " 'death,row': 262.0,\n",
       " 'death,inmate': 201.0,\n",
       " 'lawyer,evidence': 228.0,\n",
       " 'life,death': 34.0,\n",
       " 'life,term': 78.0,\n",
       " 'word,similarity': 263.0,\n",
       " 'board,recommendation': 121.0,\n",
       " 'governor,interview': 266.0,\n",
       " 'peace,atmosphere': 245.0,\n",
       " 'peace,insurance': 290.0,\n",
       " 'territory,kilometer': 314.0,\n",
       " 'travel,activity': 194.0,\n",
       " 'competition,price': 175.0,\n",
       " 'consumer,confidence': 151.0,\n",
       " 'consumer,energy': 132.0,\n",
       " 'problem,airport': 280.0,\n",
       " 'car,flight': 191.0,\n",
       " 'credit,card': 12.0,\n",
       " 'credit,information': 87.0,\n",
       " 'hotel,reservation': 82.0,\n",
       " 'grocery,money': 184.0,\n",
       " 'registration,arrangement': 260.0,\n",
       " 'arrangement,accommodation': 231.0,\n",
       " 'month,hotel': 256.0,\n",
       " 'type,kind': 14.0,\n",
       " 'arrival,hotel': 168.0,\n",
       " 'bed,closet': 155.0,\n",
       " 'closet,clothes': 74.0,\n",
       " 'situation,conclusion': 83.0,\n",
       " 'situation,isolation': 189.0,\n",
       " 'impartiality,interest': 320.0,\n",
       " 'direction,combination': 195.0,\n",
       " 'street,place': 68.0,\n",
       " 'street,avenue': 11.0,\n",
       " 'street,block': 109.0,\n",
       " 'street,children': 225.0,\n",
       " 'listing,proximity': 283.0,\n",
       " 'listing,category': 113.0,\n",
       " 'cell,phone': 32.0,\n",
       " 'production,hike': 315.0,\n",
       " 'benchmark,index': 93.0,\n",
       " 'media,trading': 251.0,\n",
       " 'media,gain': 206.0,\n",
       " 'dividend,payment': 185.0,\n",
       " 'dividend,calculation': 247.0,\n",
       " 'calculation,computation': 21.0,\n",
       " 'currency,market': 88.0,\n",
       " 'oil,stock': 86.0,\n",
       " 'announcement,production': 235.0,\n",
       " 'announcement,warning': 162.0,\n",
       " 'profit,warning': 288.0,\n",
       " 'profit,loss': 89.0,\n",
       " 'dollar,yen': 47.0,\n",
       " 'dollar,buck': 170.0,\n",
       " 'dollar,profit': 71.0,\n",
       " 'dollar,loss': 223.0,\n",
       " 'computer,software': 18.0,\n",
       " 'network,hardware': 139.0,\n",
       " 'phone,equipment': 224.0,\n",
       " 'equipment,maker': 212.0,\n",
       " 'luxury,car': 143.0,\n",
       " 'five,month': 36.0,\n",
       " 'report,gain': 238.0,\n",
       " 'investor,earning': 258.0,\n",
       " 'liquid,water': 46.0,\n",
       " 'baseball,season': 127.0,\n",
       " 'game,victory': 146.0,\n",
       " 'game,team': 54.0,\n",
       " 'marathon,sprint': 161.0,\n",
       " 'game,series': 118.0,\n",
       " 'game,defeat': 154.0,\n",
       " 'seven,series': 122.0,\n",
       " 'seafood,sea': 153.0,\n",
       " 'seafood,food': 35.0,\n",
       " 'seafood,lobster': 17.0,\n",
       " 'lobster,food': 202.0,\n",
       " 'lobster,wine': 236.0,\n",
       " 'food,preparation': 140.0,\n",
       " 'video,archive': 124.0,\n",
       " 'start,year': 43.0,\n",
       " 'start,match': 98.0,\n",
       " 'game,round': 152.0,\n",
       " 'boxing,round': 205.0,\n",
       " 'championship,tournament': 4.0,\n",
       " 'fighting,defeating': 116.0,\n",
       " 'line,insurance': 178.0,\n",
       " 'day,summer': 19.0,\n",
       " 'summer,drought': 214.0,\n",
       " 'summer,nature': 165.0,\n",
       " 'day,dawn': 125.0,\n",
       " 'nature,environment': 58.0,\n",
       " 'environment,ecology': 81.0,\n",
       " 'nature,man': 149.0,\n",
       " 'man,woman': 5.0,\n",
       " 'man,governor': 248.0,\n",
       " 'murder,manslaughter': 60.0,\n",
       " 'soap,opera': 254.0,\n",
       " 'opera,performance': 181.0,\n",
       " 'life,lesson': 157.0,\n",
       " 'focus,life': 70.0,\n",
       " 'production,crew': 186.0,\n",
       " 'television,film': 77.0,\n",
       " 'lover,quarrel': 278.0,\n",
       " 'viewer,serial': 222.0,\n",
       " 'possibility,girl': 269.0,\n",
       " 'population,development': 164.0,\n",
       " 'morality,importance': 141.0,\n",
       " 'morality,marriage': 169.0,\n",
       " 'gender,equality': 62.0,\n",
       " 'change,attitude': 80.0,\n",
       " 'family,planning': 105.0,\n",
       " 'opera,industry': 287.0,\n",
       " 'sugar,approach': 300.0,\n",
       " 'practice,institution': 204.0,\n",
       " 'ministry,culture': 171.0,\n",
       " 'problem,challenge': 108.0,\n",
       " 'size,prominence': 312.0,\n",
       " 'country,citizen': 174.0,\n",
       " 'planet,people': 137.0,\n",
       " 'development,issue': 159.0,\n",
       " 'experience,music': 129.0,\n",
       " 'music,project': 158.0,\n",
       " 'glass,metal': 69.0,\n",
       " 'aluminum,metal': 13.0,\n",
       " 'chance,credibility': 207.0,\n",
       " 'exhibit,memorabilia': 261.0,\n",
       " 'concert,virtuoso': 271.0,\n",
       " 'rock,jazz': 61.0,\n",
       " 'museum,theater': 133.0,\n",
       " 'observation,architecture': 276.0,\n",
       " 'space,world': 115.0,\n",
       " 'preservation,world': 286.0,\n",
       " 'admission,ticket': 79.0,\n",
       " 'shower,thunderstorm': 304.0,\n",
       " 'shower,flood': 295.0,\n",
       " 'weather,forecast': 10.0,\n",
       " 'disaster,area': 218.0,\n",
       " 'governor,office': 166.0,\n",
       " 'architecture,century': 120.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "df = pd.read_csv('./sim353.csv')\n",
    "wd_list1 = df[\"Word 1\"].values\n",
    "wd_list2 = df[\"Word 2\"].values\n",
    "dict1 = {}\n",
    "for i in range(0,len(wd_list1)):\n",
    "    if wd_list1[i] in pickle_data and wd_list2[i] in pickle_data:\n",
    "        a = np.array(pickle_data.get(wd_list1[i]))\n",
    "        b = np.array(pickle_data.get(wd_list2[i]))\n",
    "        # manually compute cosine similarity\n",
    "        dot = np.dot(a, b)\n",
    "        norma = np.linalg.norm(a)\n",
    "        normb = np.linalg.norm(b)\n",
    "        cos = dot / (norma * normb)\n",
    "        word = wd_list1[i]+','+ wd_list2[i]\n",
    "        dict1[word] = cos\n",
    "# print(len(dict1))\n",
    "df1 = pd.DataFrame(list(dict1.items()), columns=['WordPair', 'CosValue'])\n",
    "df1['Rank'] = df1['CosValue'].rank(ascending=False)\n",
    "# print(df1)\n",
    "dict_rank1 =  dict(zip(df1['WordPair'], df1['Rank']))\n",
    "dict_rank1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Rank the word pairs based on the similarity values as determined by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the key that does not match dict1:\n",
      " {'love,sex'}\n",
      "                 WordPair  Human   Rank\n",
      "0               tiger,cat   7.35   70.0\n",
      "1             tiger,tiger  10.00    1.0\n",
      "2              book,paper   7.46   66.0\n",
      "3       computer,keyboard   7.62   55.0\n",
      "4       computer,internet   7.58   58.0\n",
      "..                    ...    ...    ...\n",
      "328          shower,flood   6.03  117.0\n",
      "329      weather,forecast   8.34   29.0\n",
      "330         disaster,area   6.25  111.0\n",
      "331       governor,office   6.34  108.0\n",
      "332  architecture,century   3.78  175.0\n",
      "\n",
      "[333 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# add a column called rank in ascending order\n",
    "df['Word']=df['Word 1'].astype(str) + ','+df['Word 2']\n",
    "dict2 = dict(zip(df['Word'], df['Human (mean)']))\n",
    "keys_1 = set(dict1.keys())\n",
    "keys_2 = set(dict2.keys())\n",
    "common_keys = keys_2 - keys_1\n",
    "# print(\"the key that does not match dict1:\\n\",common_keys)\n",
    "for k in common_keys:\n",
    "    dict2.pop(k, None)\n",
    "df2 = pd.DataFrame(list(dict2.items()), columns=['WordPair', 'Human'])\n",
    "df2['Rank'] = df2['Human'].rank(method = \"dense\", ascending=False)\n",
    "# print(df2)\n",
    "dict_rank2 =  dict(zip(df2['WordPair'], df2['Rank']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 3: Calculate the spearman rank correlation between the two ranked list of word-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6426166324080531\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "for key1, value1 in dict_rank1.items():\n",
    "    list1.append(value1)\n",
    "    list2.append(dict_rank2[key1])\n",
    "from scipy.stats import spearmanr\n",
    "corr, p_value = spearmanr(list1, list2)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a set of news articles which are to be labelled 1 or 0. The data is split into 3 groups (train/test/validation). Each group is further divided into 2 files which consists of the text(ending with \\_X.p) and the label (ending with \\_y.p) respectively. Each datapoint is a list of words and they are all accumulated in a list forming a list of lists. The label file is a list of labels (0/1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will design a character-level CNN. So the first task would be to obtain a one-hot encoding for the characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocabulary of caracters is provided for your reference.. \n",
    "vocabulary = list(\"\"\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterEncoding(vocabulary):\n",
    "    c2v = {} # dictionary with key as a character and one-hot encoding as value\n",
    "    # write your code snippet here..\n",
    "    length = len(vocabulary)\n",
    "    index = 0\n",
    "    for letter in vocabulary:\n",
    "        c2v[letter] = index\n",
    "        index = index + 1\n",
    "#     integer_encoded = [c2v[char] for char in vocabulary]\n",
    "\n",
    "    for letter,value in c2v.items():\n",
    "        one_hot = [0 for _ in range(len(vocabulary))]\n",
    "        one_hot[value] = 1\n",
    "        c2v[letter] =  one_hot\n",
    "    return c2v\n",
    "\n",
    "c2v = characterEncoding(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode a sentence as a 2D matrix with each row representing the one-hot encoding of a character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def sentence2tensor(sentence,c2v):\n",
    "    # write your code snippet here\n",
    "    char_list = list(sentence)\n",
    "    sentence_encode = []\n",
    "    for i in range(0,len(char_list)):\n",
    "        sentence_encode.append(c2v[char_list[i]])\n",
    "    return sentence_encode\n",
    "\n",
    "sentence = \" abc def\"\n",
    "ohs = sentence2tensor(sentence,c2v)\n",
    "print(ohs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataset class for feeding data to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    # write your code snippet here...\n",
    "    def __init__(self, encoding, input_dir: Path):\n",
    "        super().__init__()\n",
    "        self.encoding = encoding\n",
    "        self.input_dir = input_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model class with 2 layers of convolutions each followed by a ReLU unit. The model should have linear layer which maps to the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a train function which trains on the train dataset. You can use binary cross entropy as your loss function and Adam as your optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, epochs=5, batchsize=32, learning_rate=0.0001):\n",
    "    # write your code snippet here\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batchsize)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     for _ in range(epochs):\n",
    "#         with tqdm(total=int(len(train_dataset)/batch_size)) as pbar:\n",
    "#             for inp,labels in train_dataloader:\n",
    "#                 out = model(inp)\n",
    "#                 loss = criterion(out, labels)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a test function which takes the trained model and test dataset and outputs the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_datset, batchsize=32):\n",
    "    # write your code snippet here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it altogether\n",
    "\n",
    "train_dataset = \n",
    "test_dataset = \n",
    "\n",
    "model = Classifier()\n",
    "\n",
    "train(model, train_datset)\n",
    "test(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
