{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 3\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. Submissions of single students will not be accepted! Please use the Forum in case of doubt in order to find team mates!\n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Tuesday, 21st of January 23:55h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples.\n",
    "\n",
    "Do not forget to specify the (First_name, Last_name, student_id (matrikelnummer)) of all contributing students in the jupyter notebook here:\n",
    "\n",
    "Student 1: Ruixiang, Wang, 400655\n",
    "\n",
    "Student 2: Tian, Li, 391589\n",
    "\n",
    "Student 3: `First_name, Last_name, student_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "import csv\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "porter = PorterStemmer()\n",
    "tqdm.pandas(desc='Progress')\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train.csv\", header = None, names=[\"Sentiment\",\"SentimentText\"],usecols=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). Define a preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|███████████████████████████████████████████████████████████████| 650000/650000 [19:03<00:00, 568.46it/s]\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_hold = stopwords_set - set({\"not\",\"no\",\"nor\"})\n",
    "\n",
    "def preprocessing(sentence):\n",
    "        # change to lower case\n",
    "    lower=str(sentence).lower()\n",
    "    \n",
    "    # remove new line symbol and slash symbol\n",
    "    remove_line = lower.replace(\"\\\\n\",\" \")\n",
    "    \n",
    "    # remove words with numbers\n",
    "    remove_num = re.sub(r'\\w*\\d\\w*', '', remove_line).strip()\n",
    "    \n",
    "    # remove punctuation  \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    remove_punc = remove_num.translate(translator)\n",
    "    remove_punc = remove_punc.split()\n",
    "    \n",
    "    # remove some stopwords\n",
    "    filtered_words = [word for word in remove_punc if word not in stopwords_hold]\n",
    "    \n",
    "     # stemming\n",
    "    stem_sentence=[]\n",
    "    for word in filtered_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    # deal with empty string\n",
    "    if len(stem_sentence) == 0:\n",
    "        stem_sentence =['<pad>']\n",
    "    \n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "df['SentimentText'] = df.SentimentText.progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Split the dataset into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, val_size=0.1):\n",
    "    train, val = train_test_split(df, test_size=val_size,random_state=42)\n",
    "    return train.reset_index(drop=True), val.reset_index(drop=True)\n",
    "traindf, valdf = split_train_test(df, val_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Save the train and validation dataframes to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_csv('./traindf.csv', index=False)\n",
    "valdf.to_csv('./valdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is the preprocessing for test dataset: (For the supervisor of this assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please change the test.csv to the name of your test file\n",
    "df = pd.read_csv(\"./test.csv\", header = None, names=[\"Sentiment\",\"SentimentText\"],usecols=[0,1])\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopwords_hold = stopwords_set - set({\"not\",\"no\",\"nor\"})\n",
    "\n",
    "def preprocessing(sentence):\n",
    "        # change to lower case\n",
    "    lower=str(sentence).lower()\n",
    "    \n",
    "    # remove new line symbol and slash symbol\n",
    "    remove_line = lower.replace(\"\\\\n\",\" \")\n",
    "    \n",
    "    # remove words with numbers\n",
    "    remove_num = re.sub(r'\\w*\\d\\w*', '', remove_line).strip()\n",
    "    \n",
    "    # remove punctuation  \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    remove_punc = remove_num.translate(translator)\n",
    "    remove_punc = remove_punc.split()\n",
    "    \n",
    "    # remove some stopwords\n",
    "    filtered_words = [word for word in remove_punc if word not in stopwords_hold]\n",
    "    \n",
    "     # stemming\n",
    "    stem_sentence=[]\n",
    "    for word in filtered_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    # deal with empty string\n",
    "    if len(stem_sentence) == 0:\n",
    "        stem_sentence =['<pad>']\n",
    "    \n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "df['SentimentText'] = df.SentimentText.progress_apply(preprocessing)\n",
    "\n",
    "# save the preprocessed test dataset\n",
    "df.to_csv('./test_prep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define how to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_field = data.Field(batch_first = True, sequential=True,tokenize=None,include_lengths=True, use_vocab=True,pad_token='<pad>', unk_token='<unk>',fix_length=150)\n",
    "label_field = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
    "\n",
    "train_val_fields = [\n",
    "    ('Sentiment', label_field),\n",
    "    ('SentimentText', text_field)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Create torchtext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainds, valds = data.TabularDataset.splits(path='./', format='csv', train='traindf.csv', validation='valdf.csv', fields=train_val_fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train word vectors and build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). Train Word2Vec Model using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#get all the sentence in training.csv\n",
    "df = pd.read_csv(\"./traindf.csv\")\n",
    "sent = []\n",
    "for text in df['SentimentText']:\n",
    "    words = str(text).split(' ')\n",
    "    sent.append(words)\n",
    "\n",
    "#use gensim to train word vectors\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "#define and train the gensim model \n",
    "w2v_model = Word2Vec(sent, sg=1, # using skip-gram model\n",
    "                     size=100,  # vector size 100\n",
    "                     window=5, # context window size\n",
    "                     min_count=5, # minimal word frequency\n",
    "                     iter = 10, # itertaion 10 times\n",
    "                     workers=8 )\n",
    "\n",
    "#save the trained model as text\n",
    "w2v_model.save('./word2vec.model')\n",
    "w2v_model.wv.save_word2vec_format(\"./word2vec.txt\",binary=False)\n",
    "print(\"stage 1) finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). Build vectors for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 2) finished\n"
     ]
    }
   ],
   "source": [
    "#cache the data\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "vectors = vocab.Vectors(name='./word2vec.txt', cache=cache)\n",
    "text_field.build_vocab(trainds, valds, max_size=100000, vectors=vectors,unk_init=None)\n",
    "# text_field.vocab.vectors.unk_init = init.xavier_uniform\n",
    "label_field.build_vocab(trainds)\n",
    "print(\"stage 2) finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading the data in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl, valdl = data.BucketIterator.splits(datasets=(trainds, valds), \n",
    "                                            batch_sizes=(512,512), \n",
    "                                            sort_key=lambda x: len(x.SentimentText), \n",
    "                                            device=torch.device(\"cpu\"), \n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)\n",
    "batch = next(iter(traindl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). Convert batch to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    "train_batch_it = BatchGenerator(traindl, 'SentimentText', 'Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,dropout,vocab_size, embedding_dim, n_hidden, n_out, pretrained_vec, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.emb.weight.data.copy_(pretrained_vec) # load pretrained vectors\n",
    "        self.emb.weight.requires_grad = False # make embedding non trainable\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.n_hidden, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(self.n_hidden*2*2, self.n_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seq, lengths):\n",
    "        bs = seq.size(1)\n",
    "        self.h = self.init_hidden(bs)\n",
    "        seq = seq.transpose(0,1)\n",
    "        embs = self.emb(seq)\n",
    "        embs = embs.transpose(0,1)\n",
    "        embs = pack_padded_sequence(embs, lengths)\n",
    "        gru_out, self.h = self.gru(embs, self.h)\n",
    "        gru_out, lengths = pad_packed_sequence(gru_out)        \n",
    "        \n",
    "        avg_pool = F.adaptive_avg_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)\n",
    "        max_pool = F.adaptive_max_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)        \n",
    "        outp = self.out(torch.cat([avg_pool,max_pool],dim=1))\n",
    "        return F.log_softmax(outp, dim=-1)\n",
    "    \n",
    "    def init_hidden(self, batch_size): \n",
    "        return torch.zeros((2,batch_size,self.n_hidden)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Define the CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, pretrained_vec, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__() \n",
    "        self.n_hidden = n_hidden\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(pretrained_vec) # load pretrained vectors\n",
    "        self.embedding.weight.requires_grad = False # make embedding non trainable\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seq, lengths):     \n",
    "        embedded = self.embedding(seq)          \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        out = self.fc(cat)\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4). Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dl, val_dl, loss_fn, opt, epochs):\n",
    "    num_batch = len(train_dl)\n",
    "    for epoch in tnrange(epochs):      \n",
    "        y_true_train = list()\n",
    "        y_pred_train = list()\n",
    "        total_loss_train = 0\n",
    "        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n",
    "        for (X,lengths),y in t:\n",
    "            y = y-1\n",
    "            t.set_description(f'Epoch {epoch}')\n",
    "            lengths = lengths.cpu().numpy()  \n",
    "            opt.zero_grad()\n",
    "            pred = model(X, lengths)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            t.set_postfix(loss=loss.item())\n",
    "            pred_idx = torch.max(pred, dim=1)[1]\n",
    "            \n",
    "            y_true_train += list(y.cpu().data.numpy())\n",
    "            y_pred_train += list(pred_idx.cpu().data.numpy())\n",
    "            total_loss_train += loss.item()\n",
    "            \n",
    "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "        train_loss = total_loss_train/len(train_dl)\n",
    "        \n",
    "        if val_dl:\n",
    "            y_true_val = list()\n",
    "            y_pred_val = list()\n",
    "            total_loss_val = 0\n",
    "            for (X,lengths),y in tqdm_notebook(val_dl, leave=False):\n",
    "                y = y - 1\n",
    "                pred = model(X, lengths.cpu().numpy())\n",
    "                loss = loss_fn(pred, y)\n",
    "                pred_idx = torch.max(pred, 1)[1]\n",
    "                y_true_val += list(y.cpu().data.numpy())\n",
    "                y_pred_val += list(pred_idx.cpu().data.numpy())\n",
    "                total_loss_val += loss.item()\n",
    "            valacc = accuracy_score(y_true_val, y_pred_val)\n",
    "            valloss = total_loss_val/len(valdl)\n",
    "            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {valloss:.4f} val_acc: {valacc:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training for RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276b3381a5da4e93941f127cfc8ee12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss: 0.9620 train_acc: 0.5770 | val_loss: 0.9132 val_acc: 0.5965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.8887 train_acc: 0.6098 | val_loss: 0.8773 val_acc: 0.6153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.8585 train_acc: 0.6237 | val_loss: 0.8733 val_acc: 0.6136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.8381 train_acc: 0.6328 | val_loss: 0.8664 val_acc: 0.6173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.8211 train_acc: 0.6405 | val_loss: 0.8571 val_acc: 0.6228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(text_field.vocab)\n",
    "embedding_dim = 100\n",
    "n_hidden = 128\n",
    "n_out =  len(label_field.vocab)\n",
    "dropout = 0.5\n",
    "\n",
    "train_batch_it = BatchGenerator(traindl, 'SentimentText', 'Sentiment') # use the wrapper to convert Batch to data\n",
    "val_batch_it = BatchGenerator(valdl, 'SentimentText', 'Sentiment')\n",
    "\n",
    "model_RNN = RNN(dropout, vocab_size, embedding_dim, n_hidden, n_out,trainds.fields['SentimentText'].vocab.vectors).to(device)\n",
    "\n",
    "# use Adam as optimizer\n",
    "opt = optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), 1e-3)\n",
    "\n",
    "fit(model=model_RNN, train_dl=train_batch_it, val_dl=val_batch_it, loss_fn=F.nll_loss, opt=opt, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the GEU model\n",
    "path = '.\\GRU_model.pth'\n",
    "torch.save(m.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training for CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306e538db43848a2b61cf9558c29f65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1143), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss: 1.5926 train_acc: 0.2549 | val_loss: 1.5600 val_acc: 0.3031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1143), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 1.5281 train_acc: 0.3337 | val_loss: 1.4949 val_acc: 0.3608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536960748c5044c191aa06d058dcbc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1143), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIM = len(text_field.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "n_hidden = 128\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(label_field.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = text_field.vocab.stoi[text_field.pad_token]\n",
    "\n",
    "model_CNN = CNN(trainds.fields['SentimentText'].vocab.vectors, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "batch = next(iter(traindl))\n",
    "train_batch_it = BatchGenerator(traindl, 'SentimentText', 'Sentiment') # use the wrapper to convert Batch to data\n",
    "val_batch_it = BatchGenerator(valdl, 'SentimentText', 'Sentiment')\n",
    "\n",
    "\n",
    "opt= optim.Adam(model_CNN.parameters(), 0.00001)\n",
    "\n",
    "fit(model=model_CNN, train_dl=train_batch_it, val_dl=val_batch_it, loss_fn=F.nll_loss, opt=opt,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the CNN model\n",
    "path = '.\\CNN_model.pth'\n",
    "torch.save(model_CNN.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
