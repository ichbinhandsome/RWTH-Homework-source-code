{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You are given the text corpus 'bible.txt' as a list of sentences. Obtain the unique words in the text. As a pre-processing step convert all the words into lower case. You can consider the vocabulary as a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)\n",
    "sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'King', 'James', 'Bible', ']']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "for sent in sents:\n",
    "    for word in sent:\n",
    "#         v = Counter(word)\n",
    "        vocab[word.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(vocab.keys())\n",
    "vocab.index('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function that takes a word in the vocabulary as input and encodes it into a one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodingWord(word, vocab):\n",
    "    encoding = np.zeros(len(vocab))\n",
    "    encoding[vocab.index(word)] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodingWord('james',vocab)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function to generate a list of center word and context word pairs for each unique in the vocabulary with a given skip-gram window. You can represent a pair as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainPairs(sents, window=5):\n",
    "    pairs = []\n",
    "    for sent in sents:\n",
    "        for i,w in enumerate(sent):\n",
    "            for j in range(i-window,i+window):\n",
    "                if j>=0 and j<len(sent) and j!=i:\n",
    "                    pairs.append((w.lower(),sent[j].lower()))\n",
    "    return pairs                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = obtainPairs(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(set(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018981"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a dataset class which genertes a batch of context word center word pair. You have to return the one hot encoding of the center word and the index of the context word in the vocabulary as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vecDataset(Dataset):\n",
    "    def __init__(self, encoding, vocab, pairs):\n",
    "        super(word2vecDataset,self).__init__()\n",
    "        self.encoding = encoding\n",
    "        self.vocab = vocab\n",
    "        self.pairs = pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        w_1, w_2 = self.pairs[index]\n",
    "        w_1_e = self.encoding(w_1, self.vocab)\n",
    "        label = self.vocab.index(w_2)\n",
    "        return torch.tensor(w_1_e, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = word2vecDataset(encodingWord, vocab, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a model class which implements word2vec model. You will need two parameters W_1 and W_2 which represent the center word and the context word matrices. You can use nn.Parameters to initialize the two parameters. A sigmoid layer follows them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec(nn.Module):\n",
    "    def __init__(self, vocab_size, encoding_size):\n",
    "        super(Word2vec, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoding_size = encoding_size\n",
    "        \n",
    "        self.W_1 = nn.Parameter(torch.randn(self.vocab_size, self.encoding_size, requires_grad=True))\n",
    "        self.W_2 = nn.Parameter(torch.randn(self.encoding_size, self.vocab_size, requires_grad=True))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = torch.matmul(inp, self.W_1)\n",
    "        y = torch.matmul(x, self.W_2)\n",
    "        y = self.softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2vec(len(vocab), 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function to train the model with batches of context-center word pairs. You can use Adam for optimization and cross-entropy as loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size, epochs=1, learning_rate=0.0001):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        with tqdm(total=int(len(train_dataset)/batch_size)) as pbar:\n",
    "            for inp,labels in train_dataloader:\n",
    "                out = model(inp)\n",
    "                loss = criterion(out, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create a instance of the model and train it with batch size of 32 and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-58dfc0c77cf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function which when given a word generates the its embedding based on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(model, word, vocab):\n",
    "    ind = vocab.index(word)\n",
    "    params = model.state_dict()\n",
    "    return params['W_1'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0159, -0.5141,  1.7470,  0.6286,  0.3453, -0.4378, -0.9434, -3.3841,\n",
       "        -0.5757, -0.6783, -1.4207, -0.2772, -1.7561,  1.3408, -0.2505, -0.6332,\n",
       "         1.2268,  1.0868,  2.5314,  0.7935, -0.2361,  0.9152, -1.5618, -1.6053,\n",
       "        -3.3672, -1.3475,  0.6621,  0.4263, -0.3119,  0.3514,  0.0674,  0.4105,\n",
       "         1.5939, -0.0116, -0.3430, -0.7105,  0.0786, -1.7043,  0.6105,  3.1155,\n",
       "        -0.4178, -2.0308,  1.2899,  1.0814, -0.5028,  0.9320, -0.7964,  1.7560,\n",
       "         0.4185, -0.5805,  0.8947,  0.0873, -0.7984,  1.1038, -0.6423, -0.2161,\n",
       "         1.5204, -0.3924, -0.9964, -0.6141,  1.0638,  0.4743, -0.9689, -0.3962,\n",
       "        -1.0791,  0.8241, -0.2425, -0.5392,  1.1395, -0.0305, -1.3192,  0.2712,\n",
       "        -1.4570,  0.3039, -1.9089,  0.1500,  0.7335,  0.7298, -0.6969,  0.5375,\n",
       "        -0.5575, -0.8123, -1.4375, -1.2186, -0.2160,  0.0677,  0.5877,  1.6579,\n",
       "         0.9000,  2.9497, -1.8123, -0.4993, -0.7505,  1.3070, -2.7847, -0.3067,\n",
       "        -0.0383, -0.0071,  1.0467, -0.1546, -0.5752,  0.1732, -0.8233, -0.1159,\n",
       "         0.2694, -0.2079,  1.0088, -0.3159,  0.5012,  0.3414,  0.5454, -0.6937,\n",
       "        -0.8569, -1.4417, -1.6720,  0.6173,  1.2295, -0.4334, -0.4093,  0.5847,\n",
       "        -0.7305, -0.8035, -0.4019,  0.7458,  0.1499,  1.3924,  0.2143,  1.4793,\n",
       "        -0.6842, -0.7387, -1.1998,  0.9249, -0.8995,  0.1894, -0.4651, -1.1632,\n",
       "        -0.0494,  0.4254, -0.5319,  1.5628, -0.9363, -0.3238, -0.2253, -0.0149,\n",
       "         0.2699, -0.7557, -1.3690,  1.9314, -0.9905,  2.1223, -0.4030,  0.5892,\n",
       "         1.5769,  0.1594, -1.4714, -0.0897, -1.1681,  1.4347,  0.8712,  0.9610,\n",
       "        -0.4424,  0.2055, -0.9864, -0.6912, -0.8002, -1.4912,  1.1929,  1.0419,\n",
       "        -0.1555, -0.4601,  0.0821, -0.0314,  0.6203,  1.2785,  0.2288,  0.2953,\n",
       "        -0.2622, -0.2094,  0.5380,  0.1492,  0.8802,  0.7290,  0.5537, -2.2101,\n",
       "        -0.0299,  1.2946, -1.7391,  1.2592, -2.1439, -0.4170,  0.5828,  0.4168,\n",
       "        -1.2367,  0.6601,  0.2252,  0.0304, -1.0931, -0.7106,  2.1256, -0.7599])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEmbedding(model, 'the', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
