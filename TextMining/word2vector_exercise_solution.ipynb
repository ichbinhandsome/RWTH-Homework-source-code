{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM based language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to develop an LSTM based language model. We will do it in the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[w.lower() for w in sent] for sent in sents if len(sent)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each unique word (including punctuations) in the vocabulary create a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(sents):\n",
    "    words = Counter()\n",
    "    for sent in sents:\n",
    "        for w in sent:\n",
    "            words[w] = 1\n",
    "    \n",
    "    cnt = 0\n",
    "    w2i = {}  # maps each unique word to index\n",
    "    i2w = {}  # maps a index to its corresponding word (will be required during generation)\n",
    "    for w in words:\n",
    "        w2i[w] = cnt\n",
    "        i2w[cnt] = w\n",
    "        cnt+=1\n",
    "    return w2i, i2w    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = word2index(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4715"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an LSTM based language model. The model should include an embedding module for mapping from the word to the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size): # note output_size is same as vocabulary size\n",
    "        super(LSTMLM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp)\n",
    "        hidden, cell_state = self.initHidden()\n",
    "        inp_1 = x.view(inp.shape[0],1,-1)\n",
    "        out, (h_n, c_n) = self.lstm(x.view(inp.shape[0],1,-1), (hidden, cell_state))\n",
    "        out = self.h2o(out)\n",
    "        return out\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size), torch.zeros(1,1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a sententes and creates a tensor with the words/tokens replace by their corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeSentence(sent, w2i):\n",
    "    return torch.tensor([w2i[w] for w in sent],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ophe', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([130,  11])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodeSentence(sents[15], w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "hidden_size = 200\n",
    "output_size = len(w2i)\n",
    "model = LSTMLM(embedding_size,hidden_size,output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model considering the each sentence as a sequence and predicting the next word in the sequence. You can follow the training model discussed in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"model.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, sents, w2i, epochs=1, learning_rate=0.0001):\n",
    "    optimizer = Adam(model.parameters(),lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        random.shuffle(sents)\n",
    "        for sent in sents:\n",
    "            e_sent = encodeSentence(sent, w2i)\n",
    "            inp = e_sent[:-1]\n",
    "            labels = e_sent[1:]\n",
    "            out = model(inp)\n",
    "            loss = criterion(out.squeeze(1), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, sents, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes the trained model and generate senetnces starting from a random word. You can continue generating until '.' is generated or a predefined length is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,w2i,i2w,length, start_word = None):\n",
    "    if start_word is None:\n",
    "        sequence = torch.tensor([random.randint(0,len(w2i)-1)],dtype=torch.long) # initialize sequence with a random word\n",
    "    else:\n",
    "        sequence = torch.tensor([w2i[start_word]],dtype=torch.long)\n",
    "    \n",
    "    i = 1\n",
    "    while i<length:\n",
    "        out = model(sequence)\n",
    "        val, ind = out.squeeze(1).max(1)\n",
    "        sequence = torch.cat((sequence, ind))\n",
    "        i+=1\n",
    "        if i2w[ind.item()]=='.':\n",
    "            break\n",
    "    return ' '.join([i2w[i] for i in sequence.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sworne .'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, w2i, i2w, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
