{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webmining - Assignment 0\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with basics of *statistics* and basics of the *sklearn* package as well as the general setup for home assignments.\n",
    "This first home assignment is shorter and also less difficult than upcoming ones.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 11.05.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints and output instead of returning it!)\n",
    "- ...\n",
    "\n",
    "\n",
    "\n",
    "### Isolation\n",
    "Functions that are expected to run in isolation are marked with [Isolation] Warning. For these additionally you are **not** allowed to:\n",
    "- do imports of any kind (also _not_ from the python standard library)\n",
    "- use imported stuff (e.g. import numpy somewhere, now use numpy)\n",
    "- call other functions that you have defined (when you write a variance function you are not allowed to call your previously defined mean function)\n",
    "- use other global variables/names\n",
    "Think of these functions as running in a seperate scripts that is not allowed to use any import statements of any kinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Alice',\n",
    "        'last_name': 'Foo',\n",
    "        'student_id': 12345\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Bob',\n",
    "        'last_name': 'Bar',\n",
    "        'student_id': 54321\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "To refresh your knowledge on basic statistics we are going to implement mean, mode, median and standard deviation. All these functions should leave the input argument intact.\n",
    "\n",
    "**[Isolation] Warning: We expect that all functions for this task to work in isolation!**\n",
    "\n",
    "\n",
    "### 1a) Mean (0.5 points)\n",
    "Write a function my_mean that takes a list of numeric values and returns the mean. \n",
    "\n",
    "\n",
    "### 1b) Std (0.5 points)\n",
    "Write a function my_std that takes a list of numeric values and returns the standard deviation. Divide by n and not by n-1.\n",
    "\n",
    "\n",
    "### 1c) Mode (1.0 points)\n",
    "Write a function my_mode that takes a list and returns the mode.\n",
    "If there is no unique mode, raise a ValueError.\n",
    "\n",
    "\n",
    "### 1d) Median (0.5 points)\n",
    "Write a function my_median that takes a list of numeric values and returns the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mean(nums):\n",
    "    if not nums:\n",
    "        print('Not valid list')\n",
    "        return\n",
    "    sum_num = 0\n",
    "    for num in nums:\n",
    "        sum_num += num\n",
    "    return sum_num/len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.625\n"
     ]
    }
   ],
   "source": [
    "#test my_mean\n",
    "print(my_mean([1,2,3,4.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_std(nums):\n",
    "    mean = sum(nums)/len(nums)\n",
    "    var = 0\n",
    "    for num in nums:\n",
    "        var += (num - mean)**2\n",
    "    return (var/(len(nums)))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2930100540985752\n"
     ]
    }
   ],
   "source": [
    "#test my_std\n",
    "print(my_std([1,2,3,4.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mode(list_value):\n",
    "    hash_table = {}\n",
    "    for v in list_value:\n",
    "        if v in hash_table:\n",
    "            hash_table[v] += 1\n",
    "        else:\n",
    "            hash_table[v] = 1\n",
    "    mode = [key for key,value in hash_table.items() if value == max(hash_table.values())]\n",
    "    try:\n",
    "        if len(mode) == 1:\n",
    "            return mode[0]\n",
    "        else:\n",
    "            raise ValueError('ValueError: no unique mode')\n",
    "    except ValueError as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: no unique mode\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "#test my_mode\n",
    "print(my_mode([1,2,3,4]))\n",
    "print(my_mode(['a','b','a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_median(nums):\n",
    "    n = len(nums)\n",
    "    sort_nums = sorted(nums)\n",
    "    return sort_nums[n//2] if n%2==1 else (sort_nums[n//2-1]+sort_nums[n//2])/2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test my_median\n",
    "print(my_median([1,2,3,4]))\n",
    "my_median([1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "In this task we are will explore basic classifiers and the sklearn package.\n",
    "### 2a) Preprocessing (1 point)\n",
    "Write a function ```preprocess```. It takes no input.\n",
    "\n",
    "It does:\n",
    "\n",
    "- read the credit_g dataset assume into a pandas dataframe. The file is located in the same folder as the notebook and called ```credit-g.csv```\n",
    "- compute the boolean target vector (True if 'class' is 'good')\n",
    "- remove the target column from the dataframe\n",
    "- convert the categorical variables to numeric ones using pd.get_dummies\n",
    "- perform a (80/20) train/test split using sklearn.model_selection.train_test_split with a seed 123456\n",
    "- returns the results of the train test split in order\n",
    "\n",
    "\n",
    "### 2b) Train linear SVM classifier (0.5 points)\n",
    "Write a function ```train_LinearSVM_classifier``` that trains a Linear Support Vector classifier.\n",
    "\n",
    "It takes two arguments, the first one is the train dataset, the second the target array. It returns the trained classifier.\n",
    "Use the Linear support vector classifier from sklearn with seed of 123456.\n",
    "\n",
    "\n",
    "### 2c) Train logistic regression classifier (0.5 points)\n",
    "Write a function ```train_LogisticRegression_classifier``` that trains a logistic regression classifier.\n",
    "\n",
    "It takes two arguments, the first one is the train dataset, the second the target array. It returns the trained classifier.\n",
    "Use the logistic regression classifier from sklearn with seed of 123456.\n",
    "\n",
    "### 2d) Evaluate the results  (1 point)\n",
    "Write a function ```get_scores``` that computes the precision, recall, accuracy and F1 scores.\n",
    "It takes three arguments. The first one is a trained classifier, the second one is the test dataset to evaluate the classifier on, the third is the ground truth target vector.\n",
    "The function returns a dictionary like this:\n",
    "\n",
    "```\n",
    "{'accuracy' : accuracy,\n",
    " 'recall' : recall,\n",
    " 'precision' : precision,\n",
    " 'F1' : F1}\n",
    " ```\n",
    " \n",
    "**[Isolation] Warning! We expect this function (2d) to work in isolation**\n",
    "\n",
    "\n",
    "\n",
    "### 2 e) Bringing it all together  (0.25 points each)\n",
    "Write two functions: ```run_SVM``` and ```run_Log``` that use the above functions to train and evaluate a SVM classifier and Logistic regression classifier respectively.\n",
    "It therefor\n",
    "\n",
    "1. loads the dataset & performs a train test split\n",
    "2. trains the respectiv classifier\n",
    "3. returns the scores dictionary\n",
    "\n",
    "Thereby use the functions ```preprocess```, ```train_LinearSVM_classifier```, ```train_LogisticRegression_classifier```, ```get_scores``` you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess():\n",
    "    # read csv file\n",
    "    df = pd.read_csv('./credit-g.csv',index_col = False)\n",
    "    # compute boolean target\n",
    "    target_vector = [True if target == 'good' else False for target in df['class']]\n",
    "    # remove target column 'class'\n",
    "    df = df.drop(columns = ['class'])\n",
    "    # conver categorical value in numrical value\n",
    "    df = pd.get_dummies(df)\n",
    "    # tarin test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target_vector, test_size = 0.2, random_state = 123456)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "def train_LinearSVM_classifier(X_train,y_train):\n",
    "    clf = LinearSVC(random_state=123456)\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(classfier, X_test, y_test):\n",
    "    result = {'accuracy' : None,\n",
    "              'recall' : None,\n",
    "              'precision' : None,\n",
    "              'F1' : None}\n",
    "    pred = classfier.predict(X_test)\n",
    "    i = 0\n",
    "    TP, FP, FN, TN = 0,0,0,0\n",
    "    while i < len(y_test):\n",
    "        if pred[i] == True and y_test[i] == True:\n",
    "            TP += 1\n",
    "        elif pred[i] == True and y_test[i] == False:\n",
    "            FP += 1\n",
    "        elif pred[i] == False and y_test[i] == True:\n",
    "            FN += 1\n",
    "        elif pred[i] == False and y_test[i] == False:\n",
    "            TN += 1\n",
    "        i += 1\n",
    "    percision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    f1 = 2*percision*recall/(percision+recall)\n",
    "    result['accuracy'] = acc\n",
    "    result['recall'] = recall\n",
    "    result['precision'] = percision\n",
    "    result['F1'] = f1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def train_LogisticRegression_classifier(X_train,y_train):\n",
    "    clf = LogisticRegression(random_state=123456)\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Log():\n",
    "    X_train, X_test, y_train, y_test = preprocess()\n",
    "    log_classifier = train_LogisticRegression_classifier(X_train,y_train)\n",
    "#     print('scores:', log_classifier.score(X_test,y_test))\n",
    "    scores = get_scores(log_classifier,X_test,y_test)\n",
    "    return scores\n",
    "\n",
    "def run_SVM():\n",
    "    X_train, X_test, y_train, y_test = preprocess()\n",
    "    SVM_classifier = train_LinearSVM_classifier(X_train,y_train)\n",
    "#     print('scores:', SVM_classifier.score(X_test,y_test))\n",
    "    scores = get_scores(SVM_classifier,X_test,y_test)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.735, 'recall': 0.8382352941176471, 'precision': 0.7862068965517242, 'F1': 0.8113879003558719}\n",
      "{'accuracy': 0.35, 'recall': 0.051470588235294115, 'precision': 0.875, 'F1': 0.09722222222222222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python_conda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\python_conda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#test case\n",
    "print(run_Log())\n",
    "print(run_SVM())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
