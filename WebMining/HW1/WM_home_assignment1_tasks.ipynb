{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webmining - Assignment 1\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with basics of *web crawling* and standard text preprocessing. It then takes a deep dive into *GloVe* one approach for obtaining word embeddings. To train GloVe we will frist construct the co-occurence matrix and then we will use adaptive stochastic gradient descent to minimize the cost function.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 27.05.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Alice',\n",
    "        'last_name': 'Foo',\n",
    "        'student_id': 12345\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Bob',\n",
    "        'last_name': 'Bar',\n",
    "        'student_id': 54321\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawling web pages (total of 4 points)\n",
    "Consider the top 150 stackoverflow questions tagged with data-mining ordered by votes (e.g. the  questions from the 10 first pages accessible from here: https://stackoverflow.com/questions/tagged/data-mining?tab=votes&pagesize=15). Use the `BeautifulSoup`  and `requests` package.\n",
    "\n",
    "### a) Simple spidering (1.5)\n",
    "\n",
    "Write a function ```get_questions``` that takes a tag (like ```\"data-mining\"```) and a number `n` and returns a list containing the hyperlinks (as strings) to the top n questions as explained above. Assume the tag exists.\n",
    "\n",
    "(the first link for data-mining is: https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression)\n",
    "\n",
    "### b) Processing a page (1.5)\n",
    "\n",
    "Write a function ```process_question``` that takes a string hyperlink to a stackoverflow questions and returns a dictionary. It contains the text of the question, their comments and the answers to the question and comments. Keep in mind to remove: All images, tags, html tags, user_information, _code_ sections. Also remove information on edits, dates etc. Finally remove all functional text-content like share, edit, follow, flag, add comment (the kind of button things). Remove everything that is not inside the div with `id=\"mainbar\"`.\n",
    "\n",
    "The structure of the result is:\n",
    "\n",
    "```python\n",
    "{'title': 'The title',\n",
    " 'question': {'text' : 'How to learn web-mining?',\n",
    "              'comments':['Good question', 'Sounds\\n interesting']},\n",
    " 'answers' : [{'text':'Do a course at CSSH!', \n",
    "               'comments' : ['You will learn a lot', 'Good stuff']},\n",
    "              {'text':'Learn on youtube', 'comments' : []}, ]}\n",
    "```\n",
    "You can also find an example of the  processed page https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression on moodle.\n",
    "\n",
    "### c) Flatten the document (0.5)\n",
    "\n",
    "Write a function `flatten_question` that takes a dict of the structure from c) and returns a list of strings for that dict. Therefor merge the title with the flattened question. Then add the one string for each answer.\n",
    "The answer and question are flattened by first joining the comment strings with a `\" \"` and then joining the text and the comments in the same way. The text should preceed the comments.\n",
    "The returned list should look like:\n",
    "\n",
    "```\n",
    "['The title How to learn web-mining? Good question Sounds\\n interesting', 'Do a course at CSSH! You will learn a lot Good stuff', 'Learn on youtube ']\n",
    "```\n",
    "\n",
    "### d) Bringing it all together (0.5)\n",
    "\n",
    "Write a function `process_top_questions` that takes a tag and a number n and that processes the top n questions by votes as explained above. It returns a single list of strings (concatenated from the list of strings for each single answer). Thereby use the previously defined functions.\n",
    "\n",
    "Execute the function with the tag `\"data-mining\"` and n=150. Store the result in ```result_1```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(tag, n):\n",
    "    # pass\n",
    "    urls = []\n",
    "    d = {'tab':'votes','pagesize': 50,'page':1 }\n",
    "    request_time = 1\n",
    "    while n > len(urls):\n",
    "        r = requests.get(url = 'https://stackoverflow.com/questions/tagged/'+tag, params=d)\n",
    "        print(str(request_time) + 'th ', 'request address :', r.url)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        questions = soup.findAll(class_ = \"question-hyperlink\")\n",
    "        for q in questions:\n",
    "            if q['class'] == ['question-hyperlink']:\n",
    "                urls.append('https://stackoverflow.com' + q['href'])\n",
    "        d['page'] += 1\n",
    "        request_time += 1\n",
    "    return urls[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1th  request address : https://stackoverflow.com/questions/tagged/data-mining?tab=votes&pagesize=50&page=1\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression',\n 'https://stackoverflow.com/questions/1746501/can-someone-give-an-example-of-cosine-similarity-in-a-very-simple-graphical-wa',\n 'https://stackoverflow.com/questions/5064928/difference-between-classification-and-clustering-in-data-mining',\n 'https://stackoverflow.com/questions/2323768/how-does-the-amazon-recommendation-feature-work',\n 'https://stackoverflow.com/questions/17469835/why-does-one-hot-encoding-improve-machine-learning-performance',\n 'https://stackoverflow.com/questions/11808074/what-is-an-intuitive-explanation-of-the-expectation-maximization-technique',\n 'https://stackoverflow.com/questions/26355942/why-is-the-f-measure-a-harmonic-mean-and-not-an-arithmetic-mean-of-the-precision',\n 'https://stackoverflow.com/questions/11513484/1d-number-array-clustering',\n 'https://stackoverflow.com/questions/12066761/what-is-the-difference-between-gradient-descent-and-newtons-gradient-descent',\n 'https://stackoverflow.com/questions/7694978/how-to-approach-a-number-guessing-game-with-a-twist-algorithm',\n 'https://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea',\n 'https://stackoverflow.com/questions/10317885/decision-tree-vs-naive-bayes-classifier',\n 'https://stackoverflow.com/questions/736514/r-random-forests-variable-importance',\n 'https://stackoverflow.com/questions/4903092/calculate-auc-in-r',\n 'https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression',\n 'https://stackoverflow.com/questions/8180296/what-information-can-we-access-from-the-client',\n 'https://stackoverflow.com/questions/12067446/how-many-principal-components-to-take',\n 'https://stackoverflow.com/questions/1575246/how-do-i-extract-keywords-used-in-text',\n 'https://stackoverflow.com/questions/26182980/can-anyone-give-a-real-life-example-of-supervised-learning-and-unsupervised-lear',\n 'https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining',\n 'https://stackoverflow.com/questions/6615665/kmeans-without-knowing-the-number-of-clusters',\n 'https://stackoverflow.com/questions/17112719/how-can-i-find-the-center-of-a-cluster-of-data-points',\n 'https://stackoverflow.com/questions/5515675/python-implementation-of-optics-clustering-algorithm',\n 'https://stackoverflow.com/questions/27822752/scikit-learn-predicting-new-points-with-dbscan',\n 'https://stackoverflow.com/questions/12893492/choosing-eps-and-minpts-for-dbscan-r',\n 'https://stackoverflow.com/questions/40795141/pca-for-categorical-features',\n 'https://stackoverflow.com/questions/8196371/how-does-clustering-especially-string-clustering-work',\n 'https://stackoverflow.com/questions/835754/data-mining-open-source-tools',\n 'https://stackoverflow.com/questions/16145448/difference-between-closed-and-open-sequential-pattern-mining-algorithms',\n 'https://stackoverflow.com/questions/21619794/what-makes-the-distance-measure-in-k-medoid-better-than-k-means',\n 'https://stackoverflow.com/questions/16381577/scikit-learn-dbscan-memory-usage',\n 'https://stackoverflow.com/questions/4404081/fast-n2-clustering-algorithm',\n 'https://stackoverflow.com/questions/35094454/how-would-one-use-kernel-density-estimation-as-a-1d-clustering-method-in-scikit',\n 'https://stackoverflow.com/questions/28416408/scikit-learn-how-to-run-kmeans-on-a-one-dimensional-array',\n 'https://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package',\n 'https://stackoverflow.com/questions/7047555/using-frequent-itemset-mining-to-build-association-rules',\n 'https://stackoverflow.com/questions/41170348/guided-mining-of-common-substructures-in-large-set-of-graphs',\n 'https://stackoverflow.com/questions/22419958/what-is-the-difference-between-big-data-and-data-mining',\n 'https://stackoverflow.com/questions/11651081/javascript-and-scientific-processing',\n 'https://stackoverflow.com/questions/2783033/text-mining-with-php',\n 'https://stackoverflow.com/questions/3573872/how-to-find-out-if-a-sentence-is-a-question-interrogative',\n 'https://stackoverflow.com/questions/6283080/random-unit-vector-in-multi-dimensional-space',\n 'https://stackoverflow.com/questions/3417709/information-retrieval-ir-vs-data-mining-vs-machine-learning-ml',\n 'https://stackoverflow.com/questions/12157881/is-it-ok-to-define-your-own-cost-function-for-logistic-regression',\n 'https://stackoverflow.com/questions/9156961/hierarchical-clustering-of-1-million-objects',\n 'https://stackoverflow.com/questions/25217065/scikit-learn-clustering-text-documents-using-dbscan',\n 'https://stackoverflow.com/questions/12688312/matlab-pca-analysis-and-reconstruction-of-multi-dimensional-data',\n 'https://stackoverflow.com/questions/1836853/machine-learning-challenge-diagnosing-program-in-java-groovy-datamining-machi',\n 'https://stackoverflow.com/questions/715181/what-kind-of-artificial-intelligence-jobs-are-out-there',\n 'https://stackoverflow.com/questions/853139/best-clustering-algorithm-simply-explained']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#test_case\n",
    "get_questions('data-mining', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(api_url):\n",
    "    data = {} # store all information of this page\n",
    "    response = requests.get(url=api_url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    # title\n",
    "    data['title'] = soup.find('a', class_ = \"question-hyperlink\").string\n",
    "    # quetions\n",
    "    question = soup.find('div', class_ = 'question')\n",
    "    s = question.find('div', class_='post-text')\n",
    "    ques = {}\n",
    "    ques['text'] = s.text # question text\n",
    "    ques['comments'] = [] #question comments\n",
    "    #find the first comments list\n",
    "    ques_comm = question.find('ul',class_ = 'comments-list')\n",
    "    q_coms = ques_comm.findAll('li', class_ = 'comment js-comment')\n",
    "    for q_c in q_coms:\n",
    "        com = q_c.find('span',class_ = 'comment-copy')\n",
    "        ques['comments'].append(com.text)\n",
    "    data['question'] = ques\n",
    "    # all answers (including comments)\n",
    "    all_answers = soup.find(id='answers')\n",
    "    answers = all_answers.findAll(class_ = 'answer')\n",
    "    data['answers'] = [] # the list of answers and corresponing comments\n",
    "    for ans in answers:\n",
    "        ans_com_dict = {} # {'test:'###', 'comments': [XX,XXXX]}\n",
    "        temp = ans.find(class_ = 'post-text')\n",
    "        comms = ans.findAll('li', class_ = 'comment js-comment')\n",
    "        try:\n",
    "            ans_com_dict['text'] = temp.text\n",
    "        except:\n",
    "            pass\n",
    "        # each question may have multiple comments\n",
    "        ans_com_dict['comments'] = []\n",
    "        for c in comms:\n",
    "            com = c.find('span',class_ = 'comment-copy')\n",
    "            ans_com_dict['comments'].append(com.text)\n",
    "        data['answers'].append(ans_com_dict)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'title': 'What is the difference between linear regression and logistic regression?',\n 'question': {'text': '\\nWhen we have to predict the value of a categorical (or discrete) outcome we use logistic regression. I believe we use linear regression to also predict the value of an outcome given the input values.\\nThen, what is the difference between the two methodologies?\\n',\n  'comments': []},\n 'answers': [{'text': \"\\n\\nLinear regression output as probabilities\\nIt's tempting to use the linear regression output as probabilities but it's a mistake because the output can be negative, and greater than 1 whereas probability can not. As regression might actually\\nproduce probabilities that could be less than 0, or even bigger than\\n1, logistic regression was introduced. \\nSource: http://gerardnico.com/wiki/data_mining/simple_logistic_regression\\n\\nOutcome\\nIn linear regression, the outcome (dependent variable) is continuous.\\nIt can have any one of an infinite number of possible values. \\nIn logistic regression, the outcome (dependent variable) has only a limited number of possible values. \\nThe dependent variable\\nLogistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue,\\n1st/2nd/3rd/4th, etc.  \\nLinear regression is used when your response variable is continuous. For instance, weight, height, number of hours,  etc.\\nEquation\\nLinear regression gives an equation which is of the form Y = mX + C,\\nmeans equation with degree 1. \\nHowever, logistic regression gives an equation which is of the form \\nY = eX + e-X\\nCoefficient interpretation\\nIn linear regression, the coefficient interpretation of independent variables are quite straightforward (i.e. holding all other variables constant, with a unit increase in this variable, the dependent variable is expected to increase/decrease by xxx). \\nHowever, in logistic regression, depends on the family (binomial, Poisson,\\netc.) and link (log, logit, inverse-log, etc.) you use, the interpretation is different. \\nError minimization technique\\nLinear regression uses ordinary least squares method to minimise the\\nerrors and arrive at a best possible fit, while logistic regression\\nuses maximum likelihood method to arrive at the solution.\\nLinear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically. \\nLogistic regression is just the opposite. Using the logistic loss function causes large errors to be penalized to an asymptotically constant.\\nConsider linear regression on categorical {0, 1} outcomes to see why this is a problem. If your model predicts the outcome is 38, when the truth is 1, you've lost nothing. Linear regression would try to reduce that 38, logistic wouldn't (as much)2.\\n\\n\",\n   'comments': ['Is there a difference between Y = e^X/1 + e^-X and Y = e^X + e^-X ?',\n    'e^X/1 ? anything divide by 1 is the same. so there is no difference. I am sure you were meaning to ask something else.',\n    'I know this is an old thread but given your statement \"Logistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue, 1st/2nd/3rd/4th, etc. \"; what\\'s the difference between this and classification then?',\n    '@kingJulian Logistic regression is indeed used for classification. Check this out, you might find it useful as I have',\n    '@kingJulian: Logistic regression is a classification technique and classification stands for several algorithms that try to predict few outcomes.']},\n  {'text': '\\nIn linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values.\\nFor instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen.\\nIf, instead, you wanted to predict, based on size, whether a house would sell for more than $200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than $200K, or No, the house will not.\\n',\n   'comments': ['In andrews logistic regression example of cancer, I can draw a horizontal line y=.5, (which obviously passes through  y=.5 ), ten if any point is above this line y=.5 => +ve , else -ve. So then why do I need a logistic regression. Im just trying to understand  the best case explanation to use logistic regression ?',\n    '@vinita: here or here is a simple example  for not using linear regression and then thresh holding, for classification problems.',\n    'logistic regression is the better classifier on categorical data than linear regression. It uses a cross-entropy error function instead of least squares. Therfore it isn\\'t that sensitify to outliers and also doesn\\'t punish \"too correct\" data points like least-squares does.']},\n  {'text': \"\\nJust to add on the previous answers. \\nLinear regression \\nIs meant to resolve the problem of predicting/estimating the output value for a given element X (say f(x)). The result of the prediction is a cotinuous function where the values may be positive or negative. In this case you normally have an input dataset with a lot of examples and the the output value for each one of them. The goal is to be able to fit a model to this data set so you are able to predict that output for new different/never seen elements. Following is the classical example of fitting a line to set of points, but in general linear regression could be used to fit more complex models (using higher polynomial degrees):\\n\\nResolving the problem \\nLinea regression can be solved in two different ways: \\n\\nNormal equation (direct way to solve the problem)\\nGradient descent (Iterative approach)\\n\\nLogistic regression\\nIs meant to resolve classification problems where given an element you have to classify the same in N categories. Typical examples are for example given a mail to classify it as spam or not, or given a vehicle find to wich category it belongs (car, truck, van, etc ..). That's basically the output is a finite set of descrete values.  \\nResolving the problem \\nLogistic regression problems could be resolved only by using Gradient descent. The formulation in general is very similar to linear regression the only difference is the usage of different hypothesis function. In linear regression the hypothesis has the form: \\nh(x) = theta_0 + theta_1*x_1 + theta_2*x_2 .. \\n\\nwhere theta is the model we are trying to fit and [1, x_1, x_2, ..] is the input vector. In logistic regression the hypothesis function is different: \\ng(x) = 1 / (1 + e^-x)\\n\\n\\nThis function has a nice property, basically it maps any value to the range [0,1] which is appropiate to handle propababilities during the classificatin. For example in case of a binary classification g(X) could be interpreted as the probability to belong to the positive class. In this case normally you have different classes that are separated with a decision boundary which basically a curve that decides the separation between the different classes. Following is an example of dataset separated in two classes.\\n\\n\",\n   'comments': []},\n  {'text': '\\nThey are both quite similar in solving for the solution, but as others have said, one (Logistic Regression) is for predicting a category \"fit\" (Y/N or 1/0), and the other (Linear Regression) is for predicting a value.\\nSo if you want to predict if you have cancer Y/N (or a probability) - use logistic.  If you want to know how many years you will live to - use Linear Regression !\\n',\n   'comments': []},\n  {'text': '\\nThe basic difference :\\nLinear regression is basically a regression model which means its will give a non discreet/continuous output of a function. So this approach gives the value. For example : given x what is f(x)\\nFor example given a training set of different factors and the price of a property after training we can provide the required factors to determine what will be the property price.\\nLogistic regression is basically a binary classification algorithm which means that here there will be discreet valued output for the function . For example : for a given x if f(x)>threshold classify it to be 1 else classify it to be 0.\\nFor example given a set of brain tumour size as training data we can use the size as input to determine whether its a benine or malignant tumour. Therefore here the output is discreet either 0 or 1.\\n*here the function is basically the hypothesis function\\n',\n   'comments': []},\n  {'text': \"\\nSimply put, linear regression is a regression algorithm, which outpus a possible continous and infinite value; logistic regression is considered as a binary classifier algorithm, which outputs the 'probability' of the input belonging to a label (0 or 1).\\n\",\n   'comments': ['Thank goodness I read your note about probability. Was about to write off logistic as a binary classifier.']},\n  {'text': '\\nRegression means continuous variable, Linear means there is linear relation between y and x. \\nEx= You are trying to predict salary from no of years of experience. So here salary is independent variable(y) and yrs of experience is dependent variable(x).\\ny=b0+ b1*x1\\n\\nWe are trying to find optimum value of constant b0 and b1 which will give us best fitting line for your observation data.\\nIt is a equation of line which gives continuous value from x=0 to very large value.\\nThis line is called Linear regression model.\\nLogistic regression is type of classification technique. Dnt be misled by term regression. Here we predict whether y=0 or 1.\\nHere we first need to find p(y=1) (wprobability of y=1) given x from formuale below.\\n\\nProbaibility p is related to y by below formuale\\n\\nEx=we can make classification of tumour having more than 50% chance of having cancer  as 1 and tumour having less than 50% chance of having cancer as 0.\\n\\nHere red point will be predicted as 0 whereas green point will be predicted as 1.\\n',\n   'comments': []},\n  {'text': '\\nIn short:\\nLinear Regression gives continuous output. i.e. any value between a range of values.\\nLogistic Regression gives discrete output. i.e. Yes/No, 0/1 kind of outputs.\\n',\n   'comments': []},\n  {'text': '\\nCannot agree more with the above comments. \\nAbove that, there are some more differences like\\nIn Linear Regression, residuals are assumed to be normally distributed. \\nIn Logistic Regression, residuals need to be independent but not normally distributed. \\nLinear Regression assumes that a constant change in the value of the explanatory variable results in constant change in the response variable. \\nThis assumption does not hold if the value of the response variable represents a probability (in Logistic Regression)\\nGLM(Generalized linear models) does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model.\\n',\n   'comments': []},\n  {'text': '\\n| Basis                                                           | Linear                                                                         | Logistic                                                                                                            |\\n|-----------------------------------------------------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|\\n| Basic                                                           | The data is modelled using a straight line.                                    | The probability of some obtained event is represented as a linear function of a combination of predictor variables. |\\n| Linear relationship between dependent and independent variables | Is required                                                                    | Not required                                                                                                        |\\n| The independent variable                                        | Could be correlated with each other. (Specially in multiple linear regression) | Should not be correlated with each other (no multicollinearity exist).                                              |\\n\\n',\n   'comments': []},\n  {'text': '\\nTo put it simply, if in linear regression model more test cases arrive which are far away from the threshold(say =0.5)for a prediction of y=1 and y=0. Then in that case the hypothesis will change and become worse.Therefore linear regression model is not used for classification problem.\\nAnother Problem is that if the classification is y=0 and y=1, h(x) can be > 1 or < 0.So we use Logistic regression were 0<=h(x)<=1.\\n',\n   'comments': []},\n  {'text': '\\nLogistic Regression is used in predicting categorical outputs like Yes/No, Low/Medium/High etc. You have basically 2 types of logistic regression Binary Logistic Regression (Yes/No, Approved/Disapproved) or Multi-class Logistic regression (Low/Medium/High, digits from 0-9 etc)\\nOn the other hand, linear regression is if your dependent variable (y) is continuous. \\ny = mx + c is a simple linear regression equation (m = slope and c is the y-intercept). Multilinear regression has more than 1 independent variable (x1,x2,x3 ... etc) \\n',\n   'comments': []},\n  {'text': '\\nIn linear regression the outcome is continuous whereas in logistic regression, the outcome has only a limited number of possible values(discrete).\\nexample:\\nIn a scenario,the given value of x is size of a plot in square feet then predicting y ie rate of the plot comes under linear regression. \\nIf, instead, you wanted to predict, based on size, whether the plot would sell for more than 300000 Rs, you would use logistic regression. The possible outputs are either Yes, the plot will sell for more than 300000 Rs, or No.\\n',\n   'comments': []},\n  {'text': '\\nIn case of Linear Regression the outcome is continuous while in case of Logistic Regression outcome is discrete (not continuous)\\nTo perform Linear regression we require a linear relationship between the dependent and independent variables. But to perform Logistic regression we do not require a linear relationship between the dependent and independent variables.\\nLinear Regression is all about fitting a straight line in the data while Logistic Regression is about fitting a curve to the data.\\nLinear Regression is a regression algorithm for Machine Learning while Logistic Regression is a classification Algorithm for machine learning.\\nLinear regression assumes gaussian (or normal) distribution of dependent variable. Logistic regression assumes binomial distribution of dependent variable.\\n',\n   'comments': []}]}"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#test_case\n",
    "api_url = 'https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression'\n",
    "process_question(api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(result.headers['Content-Type'])\n",
    "# soup = BeautifulSoup(result.text,'html.parser')\n",
    "# #title\n",
    "# soup.find('a', class_ = \"question-hyperlink\").string\n",
    "# #questions\n",
    "# s = soup.find('div', class_='post-text')\n",
    "# s.text\n",
    "# # questions comments\n",
    "\n",
    "# # answers text\n",
    "# answers = soup.find(id='answers')\n",
    "# ids  = answers.findAll('a')\n",
    "#get all answer ids\n",
    "# for i in ids:\n",
    "#     try:\n",
    "#         if int(i['name']):\n",
    "#             index  = i['name']\n",
    "#             print(index)\n",
    "#             ans = answers.find(id = 'answer-'+idnex)\n",
    "#             print(ans.text)\n",
    "#     except:\n",
    "#         continue\n",
    "# answers = answers.findAll(class_ = 'answer')\n",
    "# res = []\n",
    "# for ans in answers:\n",
    "#     re = ans.find(class_ = 'post-text')\n",
    "#     try:\n",
    "#         res.append(re.text)\n",
    "#         # print(re.text)\n",
    "#     except:\n",
    "#         continue\n",
    "# print(res)\n",
    "# answer comments:\n",
    "# for ans in answers:\n",
    "#     re = ans.findAll('li', class_ = 'comment js-comment')\n",
    "#     for comments in re:\n",
    "#         com = comments.find('span',class_ = 'comment-copy')\n",
    "#         print(com.text)\n",
    "#         print('####')\n",
    "#     print('!!!!!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input\n",
    "d={'title': 'The title',\n",
    " 'question': {'text' : 'How to learn web-mining?',\n",
    "              'comments':['Good question', 'Sounds\\n interesting']},\n",
    " 'answers' : [{'text':'Do a course at CSSH!', \n",
    "               'comments' : ['You will learn a lot', 'Good stuff']},\n",
    "              {'text':'Learn on youtube', 'comments' : []}, ]}\n",
    "\n",
    "def flatten_question(data):\n",
    "    res = []\n",
    "    # process title and question\n",
    "    s = data['title'] + ' ' + data['question']['text']\n",
    "    # process question comments\n",
    "    for comm in data['question']['comments']:\n",
    "        s += (' ' + comm)\n",
    "    res.append(s)\n",
    "    for ans in data['answers']:\n",
    "        temp = ''\n",
    "        temp += ans['text']\n",
    "        for c in ans['comments']:\n",
    "            temp += (' ' + c)\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The title How to learn web-mining? Good question Sounds\\n interesting',\n 'Do a course at CSSH! You will learn a lot Good stuff',\n 'Learn on youtube']"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#test_case\n",
    "flatten_question(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top_questions(tag, n):\n",
    "    # pass\n",
    "    urls = get_questions(tag, n)\n",
    "    result = []\n",
    "    for u in tqdm(urls):\n",
    "        data = process_question(u)\n",
    "        re = flatten_question(data)\n",
    "        result.extend(re)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1threquest address :https://stackoverflow.com/questions/tagged/data-mining?tab=votes&pagesize=50&page=1\n2threquest address :https://stackoverflow.com/questions/tagged/data-mining?tab=votes&pagesize=50&page=2\n3threquest address :https://stackoverflow.com/questions/tagged/data-mining?tab=votes&pagesize=50&page=3\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=150), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78fad7390d0540cf8da9379c9a227696"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "eryone exactly everything im doing, I only wanted a simple way to remove some noise from Kmeans lol @JungleBoogie: don\\'t be too quick to judge. I just tried it myself and it is working as advertised... And I have no idea what Javascript you are talking about, the code is right there in front of you! You don\\'t like it, I can see three other implementations in the first page of a google search. Better yet implement your own...', 'Cosine similarity when one of vectors is all zeros \\nHow to express the cosine similarity ( http://en.wikipedia.org/wiki/Cosine_similarity ) \\nwhen one of the vectors is all zeros?\\nv1 = [1, 1, 1, 1, 1]\\nv2 = [0, 0, 0, 0, 0]\\nWhen we calculate according to the classic formula we get division by zero:\\nLet d1 = 0 0 0 0 0 0\\nLet d2 = 1 1 1 1 1 1\\nCosine Similarity (d1, d2) =  dot(d1, d2) / ||d1|| ||d2||dot(d1, d2) = (0)*(1) + (0)*(1) + (0)*(1) + (0)*(1) + (0)*(1) + (0)*(1) = 0\\n\\n||d1|| = sqrt((0)^2 + (0)^2 + (0)^2 + (0)^2 + (0)^2 + (0)^2) = 0\\n\\n||d2|| = sqrt((1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2) = 2.44948974278\\n\\nCosine Similarity (d1, d2) = 0 / (0) * (2.44948974278)\\n                           = 0 / 0\\n\\nI want to use this similarity measure in a clustering application.\\nAnd I often will need to compare such vectors.\\nAlso [0, 0, 0, 0, 0] vs. [0, 0, 0, 0, 0]\\nDo you have any experience?\\nSince this is a similarity (not a distance) measure should I use special case for\\nd( [1, 1, 1, 1, 1]; [0, 0, 0, 0, 0] ) = 0\\nd([0, 0, 0, 0, 0]; [0, 0, 0, 0, 0] ) = 1\\nwhat about \\nd([1, 1, 1, 0, 0]; [0, 0, 0, 0, 0] ) = ? etc.\\n', '\\nIf you have 0 vectors, cosine is the wrong similarity function for your application.\\nCosine distance is essentially equivalent to squared Euclidean distance on L_2 normalized data. I.e. you normalize every vector to unit length 1, then compute squared Euclidean distance.\\nThe other benefit of Cosine is performance - computing it on very sparse, high-dimensional data is faster than Euclidean distance. It benefits from sparsity to the square, not just linear.\\nWhile you obviously can try to hack the similarity to be 0 when exactly one is zero, and maximal when they are identical, it won\\'t really solve the underlying problems.\\nDon\\'t choose the distance by what you can easily compute.\\nInstead, choose the distance such that the result has a meaning on your data. If the value is undefined, you don\\'t have a meaning...\\nSometimes, it may work to discard constant-0 data as meaningless data anyway (e.g. analyzing Twitter noise, and seeing a Tweet that is all numbers, no words). Sometimes it doesn\\'t.\\n What would a more appropriate similarity measure be in this case then? Hamming distance? There is no context given. Euclidean distance could also be \"more appropriate\".', '\\nIt is undefined.\\nThink you have a vector C that is not zero in place your zero vector. Multiply it by epsilon > 0 and let run epsilon to zero. The result will depend on C, so the function is not continuous when one of the vectors is zero.\\n', \"Analyzing noisy data \\nI recently launched a rocket with a barometric altimeter that is accurate to roughly 10 ft (calculated via data acquired during flight).  The recorded data is in time increments of 0.05 sec per sample and a graph of altitude vs. time looks pretty much like it should when zoomed out over the entire flight.\\nThe problem is when I try to calculate other values such as velocity or acceleration from the data, the accuracy of the measurements makes the calculated values pretty much worthless.  What techniques can I use to smooth out the data so that I can calculate (or approximate) reasonable values for the velocity and acceleration?  It is important that major events remain in place in time, most notably the 0 for for the first entry and the highest point during flight (2707).\\nThe altitude data follows and is measured in ft above ground level.  The first time would be 0.00 and each sample is 0.05 seconds after the previous sample.  The spike at the beginning of the flight is due to a technical problem that occurred during liftoff and removing the spike is optimal.\\nI originally tried using linear interpolation, averaging nearby data points, but it took many iterations to smooth the data enough for integration and the flattening of the curve removed the important apogee and ground level events.  \\nAll help is greatly appreciated.  Please note this is not the complete data set and I am looking for suggestions on better ways to analyze the data, not for someone to reply with a transformed data set.  It would be nice to use an algorithm on board future rockets which can predict current altitude/velocity/acceleration without knowing the full flight data, though that is not required.\\n00000\\n00000\\n00000\\n00076\\n00229\\n00095\\n00057\\n00038\\n00048\\n00057\\n00057\\n00076\\n00086\\n00095\\n00105\\n00114\\n00124\\n00133\\n00152\\n00152\\n00171\\n00190\\n00200\\n00219\\n00229\\n00248\\n00267\\n00277\\n00286\\n00305\\n00334\\n00343\\n00363\\n00363\\n00382\\n00382\\n00401\\n00420\\n00440\\n00459\\n00469\\n00488\\n00517\\n00527\\n00546\\n00565\\n00585\\n00613\\n00633\\n00652\\n00671\\n00691\\n00710\\n00729\\n00759\\n00778\\n00798\\n00817\\n00837\\n00856\\n00885\\n00904\\n00924\\n00944\\n00963\\n00983\\n01002\\n01022\\n01041\\n01061\\n01080\\n01100\\n01120\\n01139\\n01149\\n01169\\n01179\\n01198\\n01218\\n01238\\n01257\\n01277\\n01297\\n01317\\n01327\\n01346\\n01356\\n01376\\n01396\\n01415\\n01425\\n01445\\n01465\\n01475\\n01495\\n01515\\n01525\\n01545\\n01554\\n01574\\n01594\\n01614\\n01614\\n01634\\n01654\\n01664\\n01674\\n01694\\n01714\\n01724\\n01734\\n01754\\n01764\\n01774\\n01794\\n01804\\n01814\\n01834\\n01844\\n01854\\n01874\\n01884\\n01894\\n01914\\n01924\\n01934\\n01954\\n01954\\n01975\\n01995\\n01995\\n02015\\n02015\\n02035\\n02045\\n02055\\n02075\\n02075\\n02096\\n02096\\n02116\\n02126\\n02136\\n02146\\n02156\\n02167\\n02177\\n02187\\n02197\\n02207\\n02217\\n02227\\n02237\\n02237\\n02258\\n02268\\n02278\\n02278\\n02298\\n02298\\n02319\\n02319\\n02319\\n02339\\n02349\\n02359\\n02359\\n02370\\n02380\\n02380\\n02400\\n02400\\n01914\\n02319\\n02420\\n02482\\n02523\\n02461\\n02502\\n02543\\n02564\\n02595\\n02625\\n02666\\n02707\\n02646\\n02605\\n02605\\n02584\\n02574\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02554\\n02543\\n02554\\n02554\\n02554\\n02554\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02533\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02543\\n02533\\n02523\\n02523\\n02523\\n02523\\n02523\\n02523\\n02523\\n02523\\n02543\\n02523\\n02523\\n02523\\n02523\\n02523\\n02523\\n02523\\n02523\\n02513\\n02513\\n02502\\n02502\\n02492\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02482\\n02472\\n02472\\n02472\\n02461\\n02461\\n02461\\n02461\\n02451\\n02451\\n02451\\n02461\\n02461\\n02451\\n02451\\n02451\\n02451\\n02451\\n02451\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02441\\n02431\\n02441\\n02431\\n02441\\n02431\\n02420\\n02431\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02420\\n02410\\n02420\\n02410\\n02410\\n02410\\n02410\\n02400\\n02400\\n02410\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02400\\n02390\\n02390\\n02390\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02380\\n02370\\n02370\\n02380\\n02370\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02359\\n02349\\n02349\\n02349\\n02349\\n02349\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n02339\\n\\n please use the metric system. We don't want you to accidentally crash on the moon when your target was the neighbor cornfield ;)\", '\\nHere is my solution, using a Kalman filter. You will need to tune the parameters (even +- orders of magnitude) if you want to smooth more or less.\\n#!/usr/bin/env octave\\n\\n% Kalman filter to smooth measures of altitude and estimate\\n% speed and acceleration. The continuous time model is more or less as follows:\\n% derivative of altitude := speed\\n% derivative of speed := acceleration\\n% acceleration is a Wiener process\\n\\n%------------------------------------------------------------\\n% Discretization of the continuous-time linear system\\n% \\n%   d  |x|   | 0 1 0 | |x|\\n%  --- |v| = | 0 0 1 | |v|   + \"noise\"\\n%   dt |a|   | 0 0 0 | |a|\\n%\\n%   y = [1 0 0] |x|     + \"measurement noise\"\\n%               |v|\\n%               |a|\\n%\\nst = 0.05;    % Sampling time\\nA = [1  st st^2/2;\\n     0  1  st    ;\\n     0  0  1];\\nC = [1 0 0];\\n\\n%------------------------------------------------------------\\n% Fine-tune these parameters! (in particular qa and R)\\n% The acceleration follows a \"random walk\". The greater is the variance qa,\\n% the more \"reactive\" the system is expected to be, i.e.\\n% the more the acceleration is expected to vary\\n% The greater is R, the more noisy is your measurement instrument\\n% (less \"accuracy\" of the barometric altimeter);\\n% if you increase R, you will smooth the estimate more\\nqx = 1.0;                      % Variance of model noise for position\\nqv = 1.0;                      % Variance of model noise for speed\\nqa = 50.0;                     % Variance of model noise for acceleration\\nQ  = diag([qx, qv, qa]);\\nR  = 100.0;                    % Variance of measurement noise\\n                               % (10^2, if 10ft is the standard deviation)\\n\\nload data.txt  % Put your measures in this file\\n\\nest_position     = zeros(length(data), 1);\\nest_speed        = zeros(length(data), 1);\\nest_acceleration = zeros(length(data), 1);\\n\\n%------------------------------------------------------------\\n% Kalman filter\\nxhat = [0;0;0];     % Initial estimate\\nP    = zeros(3,3);  % Initial error variance\\nfor i=1:length(data),\\n   y = data(i);\\n   xpred = A*xhat;                                    % Prediction\\n   Ppred = A*P*A\\' + Q;                                % Prediction error variance\\n   Lambdainv = 1/(C*Ppred*C\\' + R);\\n   xhat  = xpred + Ppred*C\\'*Lambdainv*(y - C*xpred);  % Update estimation\\n   P = Ppred - Ppred*C\\'*Lambdainv*C*Ppred;            % Update estimation error variance\\n   est_position(i)     = xhat(1);\\n   est_speed(i)        = xhat(2);\\n   est_acceleration(i) = xhat(3);\\nend\\n\\n%------------------------------------------------------------\\n% Plot\\nfigure(1);\\nhold on;\\nplot(data, \\'k\\');               % Black: real data\\nplot(est_position, \\'b\\');       % Blue:  estimated position\\nplot(est_speed, \\'g\\');          % Green: estimated speed\\nplot(est_acceleration, \\'r\\');   % Red:   estimated acceleration\\npause\\n\\n I\\'ve started reading about this and it looks very promising.  I like how this can be adapted to take multiple input sources. The code ran fine and plots. Nice job. But I\\'m not sure the acceleration plot is right - it imitates the velocity too closely, not its derivative.  A flaw in the code, or is this a quirk of Kalman filtering?', '\\nYou could try running the data through a low-pass filter. This will smooth out high frequency noise. Maybe a simple FIR.\\nAlso, you could pull your major events from the raw data, but use a polynomial fit for velocity and acceleration data.\\n I like your comment about the polynomial fit.  Perhaps the flight could be divided in two: before the thrust is finished and after.  After the thrust a parabola would be a natural fit for a polynomial and before that some polynomial with a slightly higher order (3 or 4?).  Extreme values such as the early \"229\" should become outliers and disappear. This answer is right on track. It just needs a specific name to look up... since acceleration and velocity are derivatives respect to time, you should look into Savitzky-Golay.  It\\'s described in Numerical Recipes and online in many places.  Defined as a low-order polynomial fit at each point, it smooths the data and takes a derivative order as a parameter.  This is better numerically than smoothing and then differentiating in separate steps.  S.G. is especially good at preserving peaks, inflection points etc whereas naive attempts at smoothing typically mush out peaks and other fine detail.', '\\nhave you tried performing a scrolling window average of your values ? Basically you perform a window of, say 10 values (from 0 to 9), and calculate its average. then you scroll the window one point (from 1 to 10) and recalculate. This will smooth the values while keeping the number of points relatively unchanged. Larger windows give smoother data at the price of loosing more high-frequency information.\\nYou can use the median instead of the average if your data happen to present outlier spikes.\\nYou can also try with Autocorrelation.\\n I actually did try this, however, I only tried it over a window size of 3.  Perhaps some bigger window tests would make for better data, +1 The shape of the window matters.  If you take a plain simple average of n points to the left and n to the right, along with the point at the center, you get some noise in the output because the points at the ends are entering/leaving the range as you move to calculate the next output point.  Is better to calculate a weighted average - less weight given to points the further they are from the central point.  See other answers for good ways to do this. @DarenW : that\\'s a very good idea. as you said, this way you reduce the \"all-or-nothing\" presence of a point...', '\\nOne way you can approach analyzing you data is to try to match it too some model, generate a function, and then test its fitness to your data set.... This can be rather complicated and is probably unnecessary... but the point is that instead of generating acceleration/velocity data directly from you data you can match it to your model (rather simple for a rocket, some acceleration upwards followed by a slow constant speed descent.)  At least that how i would do it in a physics experiment.\\nAs for generating some sense of velocity and acceleration during flight this should be as simple averaging the velocity from several different results.  Something along the lines of:\\nEsitimatedV = Vmeasured*(1/n) + (1 - 1/n)*EstimatedV.  Set n based on how quickly you want your velocity to adjust by.  \\n', '\\nI know nothing about rockets. I plotted your points and they look lovely.\\nBased on what I see in that plot let me assume that there is usually a single apogee and that the function that gave rise to your points has no derivative wrt time at that apogee.\\nSuggestion:\\n\\nMonitor maximum altitude throughout the flight.\\nContinuously watch for the apogee by (say, simply) comparing the most recent few points with the current maximum.\\nUntil you reach the maximum, with (0,0) fixed and some arbitrary set of knots calculate a collection of natural splines up to the current altitude. Use the residuals wrt the splines to decide which data to discard. Recalculate the splines.\\nAt the maximum retain the most recently calculate splines. Start calculating a new set of splines for the curve beyond the apogee.\\n\\n From my understanding of splines is that they are more useful for generating exact replicas of continuous data than they are interpolating missing or noisy data points.  Am I missing something?  To determine apogee, we take every data point for the last 20 samples and compare it to the last 20 samples before each.  Once all samples show a decline in altitude, we say apogee is the highest recorded value during that interval. You\\'re right. Sloppy thinking. Apologies. Was trying to express: it appears to me that this is not the jagged set of data one often sees in a time series and you might omit the task of identifying a complicated error structure. From the beginning of a flight keep fitting easy smooth curves until you hit the apogee, discarding outliers--which appear obvious from the plot that I made. (Are they obvious to you though?) Then, after you hit the apogee start fitting a distinct set of easy smooth curves for the rest of the flight, again discarding outliers.   In this instance, spline=\"brain f__t\"', '\\nARIMA model and look for autocorrelation in the residual is standard procedure. Volatility model another.\\n', \"What is the meaning of jitter in visualize tab of weka \\nIn weka I load an arff file. I can view the relationship between attributes using the visualize tab.\\nHowever I can't understand the meaning of the jitter slider. What is its purpose?\\n\", \"\\nYou can find the answer in the mailing list archives:\\nThe jitter function in the Visualize panel just adds artificial random\\nnoise to the coordinates of the plotted points in order to spread the\\ndata out a bit (so that you can see points that might have been\\nobscured by others).\\n Load the contact lenses example from Weka, and visualize it. In the first plot, some points are exactly on top of each other. By adding jitter, you can see how many points there are for each combination. This won't work that good for larger data sets, but you can still get an impression. For discrete values, this is quite important.\", \"\\nI don't know weka, but generally jitter is a term for the variation of a periodic signal to some reference interval. I'm guessing the slider allows you to set some range or threshold below which data points are treated as being regular, or to modify the output to introduce some variation. The wikipedia entry can give you some background.\\nUpdate: from this pdf, the jitter slider is for this purpose:\\n\\nJitter option to deal with nominal attributes (and to detect hiddendata points)\\n\\nBased on the accompanying slide it looks like it introduces some variation in the visualisation, perhaps to show when two data points overlap.\\nUpdate 2: This google books extract (to Data mining By Ian H. Witten, Eibe Frank) seems to confirm my guess: \\n\\n[jitter] is a random displacement applied to X and Y values to separate points that lie on top of one another. Without jitter, 1000 instances at the same data point would look just the same as 1 instance\\n\\n\", \"\\nI don't know the products you mention, but jittering generally means randomising the sample positions. Eg, in ray tracing you would normally render a ray though each pixel on the screen. Jittering adds a random offset to each ray to reduce issues caused by regular aliasing.\\n\", 'pandas pivot table rename columns \\nHow to rename columns with multiple levels after pandas pivot operation?\\nHere\\'s some code to generate test data:\\nimport pandas as pd\\ndf = pd.DataFrame({\\n    \\'c0\\': [\\'A\\',\\'A\\',\\'B\\',\\'C\\'],\\n    \\'c01\\': [\\'A\\',\\'A1\\',\\'B\\',\\'C\\'],\\n    \\'c02\\': [\\'b\\',\\'b\\',\\'d\\',\\'c\\'],\\n    \\'v1\\': [1, 3,4,5],\\n    \\'v2\\': [1, 3,4,5]})\\n\\nprint(df)\\n\\ngives a test dataframe: \\n   c0 c01 c02  v1  v2\\n0  A   A   b   1   1\\n1  A  A1   b   3   3\\n2  B   B   d   4   4\\n3  C   C   c   5   5\\n\\napplying pivot\\ndf2 = pd.pivot_table(df, index=[\"c0\"], columns=[\"c01\",\"c02\"], values=[\"v1\",\"v2\"])\\ndf2 = df2.reset_index()\\n\\ngives\\n\\nhow to rename the columns by joining levels?\\nwith format \\n<c01 value>_<c02 value>_<v1>\\nfor example first column should look like \\n\"A_b_v1\"\\nThe order of joining levels  isn\\'t really important to me.\\n', '\\nIf you want to coalesce the multi-index into a single string index without caring about the index level order, you can simply map a join function over the columns, and assign the result list back:\\ndf2.columns = list(map(\"_\".join, df2.columns))\\n\\n\\nAnd for your question, you can loop through the columns where each element is a tuple, unpack the tuple and join them back in the order you want:\\ndf2 = pd.pivot_table(df, index=[\"c0\"], columns=[\"c01\",\"c02\"], values=[\"v1\",\"v2\"])\\n\\n# Use the list comprehension to make a list of new column names and assign it back\\n# to the DataFrame columns attribute.\\ndf2.columns = [\"_\".join((j,k,i)) for i,j,k in df2.columns]\\ndf2.reset_index()\\n\\n\\n thanks [\\'_\\'.join(str(s).strip() for s in col if s) for col in df2.columns] worked as a general solution, independent of number of levels @muon I like your general solution! It\\'s perfect.']\n"
    }
   ],
   "source": [
    "# test_case\n",
    "result_1 = process_top_questions('data-mining', 150)\n",
    "print(result_1)\n",
    "# store result\n",
    "with open('./result_1.txt', 'w', encoding='utf-8') as w:\n",
    "    w.writelines(result_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing (total of 2 points)\n",
    "It is/was common to use stopword removal and stemming as a preparation for word_embeddings. You should use the stopword list provided by Python-nltk library.\n",
    "\n",
    "## a) The usual preprocessing (1)\n",
    "\n",
    "Write a function `to_tokens` that takes a string and performs tokenization, stopword removal and stemming. It returns a list of tokens. The tokens are in the same order they were in the initial string. Use the functions from the nltk library.\n",
    "To transform a string into tokens use the function `nltk.word_tokenize`.\n",
    "\n",
    "## b) Reduce vocabulary (1)\n",
    "Write a function `process_and_filter_corpus` that takes a list of strings and an integer as input. It applies the `to_tokens` function on each of those. The return values are\n",
    "\n",
    "1) the list of list of tokens. All tokens that appear less than min_support times (in the entire corpus) are __removed__\n",
    "\n",
    "2) a set of tokens that were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['cat', 'love', 'fish', 'chip']\n"
    }
   ],
   "source": [
    "def to_tokens(sent):\n",
    "    sent = sent.lower()\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(stop_words)\n",
    "    words = word_tokenize(sent)\n",
    "    filtered_words = [ps.stem(w) for w in words if w not in stop_words]\n",
    "    return filtered_words\n",
    "#test case\n",
    "print(to_tokens('I am a cat and I love fish and chips'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using any extern library except nltk\n",
    "def process_and_filter_corpus(sents, min_support):\n",
    "    hash_table = {}\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        temp = []\n",
    "        words = to_tokens(sent)\n",
    "        for w in words:\n",
    "            temp.append(w)\n",
    "            if w not in hash_table:\n",
    "                hash_table[w] = 1\n",
    "            else:\n",
    "                hash_table[w] += 1\n",
    "        res.append(temp)\n",
    "    removed_tokens = set(k for k,v in hash_table.items() if v < min_support)\n",
    "    result = []\n",
    "    for words in res:\n",
    "        temp = []\n",
    "        for w in words:\n",
    "            if w not in removed_tokens:\n",
    "                temp.append(w)\n",
    "        result.append(temp)\n",
    "    return result, removed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([['see', 'bigram'], ['see'], ['bigram']],\n {'colloc',\n  'essenti',\n  'frequent',\n  'later',\n  'pair',\n  'than-don',\n  'word',\n  'work'})"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "sents = ['Here we see that the pair of words than-done is a bigram','We will see how it works later','collocations are essentially just frequent bigrams']\n",
    "process_and_filter_corpus(sents,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Glove (total of 10 points)\n",
    "### a) Computing the global word co-occurence matrix (3 = 2.5 + 0.5)\n",
    "We will now explore some steps of required for the GloVe word embedding.\n",
    "\n",
    "Write a function `get_cooc_matrix` that takes a list of list of tokens and an integer _context_size_ as input. It returns 1) a sparse co-occurrence matrix (a dict mapping pairs of integer indices to their cooc-score) and 2) a dictionary that maps a word to an index in the cooc matrix/dict. Do not use any non standard library for that. If two tokens are d places apart they get a score of 1/d. Only take into account words that are at most context_size apart from the central word.\n",
    "\n",
    "Example:\n",
    "\n",
    "For the corpus `[['bad', 'dog', 'bad', 'cat', 'thing'],['bad', 'dog']]` and context_size=2 the tokens 'dog' and 'thing' do not co-occur. While the tokens 'dog' and 'bad' have a cooc score of 3. The token pair 'bad' and 'bad' has a cooc score of 1.0.\n",
    "\n",
    "Additionally write a function `cooc_to_numpy` that takes the dict-sparse-representation and returns three numpy arrays. The first two are of type int and they contain the values for i and j respectively. The third array contains the cooc-score for that entry. They are sorted in a way that num it is first sorted ascending according to i and then ascending according to j. This sorting is only for reproducability, not necessary for the glove. We call the output of this function a coord_tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{(0, 1): 3.0, (0, 0): 1.0, (1, 0): 3.0, (1, 2): 0.5, (0, 2): 1.0, (0, 3): 0.5, (2, 1): 0.5, (2, 0): 1.0, (2, 3): 1.0, (3, 0): 0.5, (3, 2): 1.0} \n {'bad': 0, 'dog': 1, 'cat': 2, 'thing': 3}\n(array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3]), array([0, 1, 2, 3, 0, 2, 0, 1, 3, 0, 2]), array([1. , 3. , 1. , 0.5, 3. , 0.5, 1. , 0.5, 1. , 0.5, 1. ]))\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_cooc_matrix(context, context_size):\n",
    "    #compute word to index dict\n",
    "    w2i = {}\n",
    "    text = []\n",
    "    for sent in context:\n",
    "        for word in sent:\n",
    "            text.append(word)\n",
    "    #store unique words\n",
    "    text = list(dict.fromkeys(text))\n",
    "    for i, w in enumerate(text):\n",
    "        w2i[w] = i\n",
    "    #compute co-courrence dict\n",
    "    cs = context_size\n",
    "    co_occurence = {} # mapping pairs of integer indices to their cooc-score\n",
    "    for sent in context:\n",
    "        for i, w in enumerate(sent):\n",
    "            # get left context words and right context words\n",
    "            center_word_id = w2i[w]\n",
    "            left_context_words = sent[max(0,i-cs):i]\n",
    "            right_context_words = sent[i+1:min(len(sent),i+cs+1)]\n",
    "            # left context words score\n",
    "            for left_i, left_w in enumerate(left_context_words):\n",
    "                #calculate distance\n",
    "                dist = len(left_context_words) - left_i\n",
    "                incremment = 1/float(dist) # score\n",
    "                left_word_id = w2i[left_w]\n",
    "                # add score into co_occurence matrix dict\n",
    "                if (center_word_id, left_word_id) not in co_occurence:\n",
    "                    co_occurence[(center_word_id, left_word_id)] = incremment\n",
    "                else:\n",
    "                     co_occurence[(center_word_id, left_word_id)] += incremment\n",
    "                # here we can also use this following calculation if we don't want to us right_context_words\n",
    "\n",
    "                # if (left_word_id, center_word_id) not in co_occurence:\n",
    "                #     co_occurence[(left_word_id,center_word_id)] = incremment\n",
    "                # else:\n",
    "                #     co_occurence[(left_word_id,center_word_id)] += incremment\n",
    "                \n",
    "            # right context words score\n",
    "            for right_i, right_w in enumerate(right_context_words):\n",
    "                dist = right_i + 1\n",
    "                incremment = 1/float(dist)\n",
    "                right_word_id = w2i[right_w]\n",
    "                if (center_word_id, right_word_id) not in co_occurence:\n",
    "                    co_occurence[(center_word_id, right_word_id)] = incremment\n",
    "                else:\n",
    "                     co_occurence[(center_word_id, right_word_id)] += incremment\n",
    "    return co_occurence, w2i\n",
    "\n",
    "def cooc_to_numpy(coocs):\n",
    "    list_i = []\n",
    "    list_j = []\n",
    "    scores = []\n",
    "    sorted_coocs = sorted(coocs.items(), key = lambda item: item[0][0])\n",
    "    hash_table = {}\n",
    "    for pair in sorted_coocs:\n",
    "        i, j = pair[0]\n",
    "        if i not in hash_table:\n",
    "            hash_table[i] = [j]\n",
    "        else:\n",
    "            hash_table[i].append(j)\n",
    "    for i, js in hash_table.items():\n",
    "        for j in sorted(js):\n",
    "            list_i.append(i)\n",
    "            list_j.append(j)\n",
    "            scores.append(coocs[(i,j)])\n",
    "    np_i = np.array(list_i, dtype=int)\n",
    "    np_j = np.array(list_j, dtype=int)\n",
    "    np_scores = np.array(scores, dtype=float)\n",
    "    # coord_tql = np.vstack((np_i, np_j, np_scores))\n",
    "    return np_i, np_j, np_scores\n",
    "\n",
    "#test_case\n",
    "mini_corpus=[['bad', 'dog', 'bad', 'cat', 'thing'], ['bad', 'dog']]\n",
    "#results for mini_corpus:\n",
    "d,vocab = get_cooc_matrix(mini_corpus, 2)\n",
    "print(d,'\\n', vocab)\n",
    "cooc_tpl = cooc_to_numpy(d)\n",
    "print(cooc_tpl)\n",
    "# print('cooc_tpl',(\n",
    "#  np.array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3]),\n",
    "#  np.array([0, 1, 2, 3, 0, 2, 0, 1, 3, 0, 2]),\n",
    "#  np.array([1. , 3. , 1. , 0.5, 3. , 0.5, 1. , 0.5, 1. , 0.5, 1. ])))\n",
    "# print('vocab', {'bad': 0, 'dog': 1, 'cat': 2, 'thing': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the global co-occurence matrix, it is time to use this information to train GloVe vectors.\n",
    "For this task we will use an (adaptive) stochastic gradient descent method.\n",
    "\n",
    "### b) Initialize Vectors and Gradients (1)\n",
    "Write a function `init_matrix`  that takes three arguments: 1) the size of the vocabulary $v$ 2) the size of the desired Glove vectors $n$ and 3) a random number generator which is initialized like: (https://numpy.org/doc/1.18/reference/random/index.html). It returns two matrices: 1) The matrix of glove vectors where each row is of size $n+1$ reflecting one glove vector + bias. It is initialized with random values such that they lie uniformly on the interval $[-l, l)$ with $l=\\frac{0.5}{n}$. Use one call to the generator only. 2) A matrix of similar shape initialized to all values being one. These are our corresponding squared gradients.\n",
    "\n",
    "### c) Compute loss (1.5)\n",
    "Write a function `compute_loss(v1, v2, b1, b2, cooc_score, max_score, alpha)` that computes the GloVe Loss for a single pair of vectors v1 and v2 their weights b1 and b2 with associated cooc_score and weighting parameters max_score and alpha. The function returns 1) the loss and 2) the part $g$ that is the part of the gradient, that is the same in both\n",
    "\n",
    "The GloVe Loss for a pair of indices i,j is: $loss(i,j) = f(C_{i,j}) \\cdot \\left(v_i \\cdot v_j + b_i + b_j -log(C_{i,j})\\right)^2$ with $C_{i,j}$ the cooc-score. The function function $f$ is defined as\n",
    "\n",
    "$f(x) = (\\frac{x}{max\\_score})^{\\alpha}$ if $x< max\\_score$\n",
    "\n",
    "$f(x) = 1 $ otherwise\n",
    "\n",
    "The shared part is: $g(i,j) = f(C_{i,j}) \\cdot \\left(v_i \\cdot v_j + b_i + b_j -log(C_{i,j})\\right)$\n",
    "\n",
    "### d) Computing the updates (0.5 each)\n",
    "\n",
    "Write a function `calc_gradient_vi(g, vj, eta, grad_clip)`  that takes the value $g$ and a vector $v_j$, the learning rate $\\eta$ and a gradient clipping value. It computes the gradient update for the $v_i$ vector without the bias. Applies gradient clipping such that the absolute value of each element of the gradient vector is at most grad_clip. Thereafter the gradient is multiplied with the learning rate and finally the validify function is applied to the gradient vector which is then returned.\n",
    "\n",
    "Write a similar function to compute the update for the bias b `calc_gradient_b(g, grad_clip)`. It computes the gradient update for the bias b. By clipping the gradient and therafter validifying the gradient. It then returns the gradient update.\n",
    "To compute the gradient update, drop the factor 2 that arises when differentiating the squared term. Also the gradient update for the bias terms does not include the learning rate.\n",
    "\n",
    "\n",
    "### e) Apply the update (1.5)\n",
    "\n",
    "Write a function `one_update(W, W_grad, pair, cooc_score, max_score, alpha, eta, grad_clip)`\n",
    "That performs an update of the vector matrix W and gradient matrix W_grad for a pair of word indices $(i,j)$=pair which with associated cooc_score. The remaining parameters should be clear from the previous functions.\n",
    "\n",
    "The matrix W is updated using the previously computed updates divided by the square root of the corresponding entries in the W_grad matrix. Keep in mind you have to walk into the opposite direction of the gradient. \n",
    "\n",
    "The W_grad matrix is updated using the sqared entry of the corresponding update (prior to dividing).\n",
    "\n",
    "The function returns the loss prior to upgrading. (The matrices W and W_grad are updated in place)\n",
    "\n",
    "### f) Write a training function (1)\n",
    "\n",
    "Write function `train(W, W_grad, cooc_tpl, n_epochs, rng, max_score, alpha, eta, grad_clip)` that trains W and W_grad matrix using the cooc_tpl for n_epochs. Before each epoch shuffle all arrays in the cocc_tpl in unison using one call to `rng.permutation`. \n",
    "\n",
    "### g) Bringing it all together (1)\n",
    "\n",
    "Write a function `train_corpus` that takes a corpus in the form of a list of strings.\n",
    "Preprocess each string as in task 2). Keep only words that have at least three occurences. \n",
    "\n",
    "Internally use: max_score=100, alpha=3/4, window size of 4, learning rate of 0.05, dimension of glove vectors = 50, grad_clip=100. Create a new `numpy.random.default_rng` instance, feed it a seed of 1. Train for 25 epochs.\n",
    "\n",
    "The function returns the trained W matrix.\n",
    "\n",
    "Apply it to your corpus that you obtained in task 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(v1, v2, b1, b2, cooc_score, max_score, alpha):\n",
    "    if cooc_score < max_score:\n",
    "        f = (cooc_score/max_score) ** alpha\n",
    "    else:\n",
    "        f = 1\n",
    "    cost_inner = np.dot(v1,v2) + b1 + b2 - np.log(cooc_score)\n",
    "    loss = f * (cost_inner **2)\n",
    "    shared_gradient = f * cost_inner\n",
    "    return loss, shared_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "np.log(1)\n",
    "np.dot(np.array([1,2]),np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validify(grad):\n",
    "    if np.isnan(grad).sum()>0 or np.isinf(grad).sum()>0:\n",
    "        print('Warning: invalid value')\n",
    "        return np.zeros(np.shape(grad))\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calc_gradient_vi(fdiff, v, eta, grad_clip):\n",
    "    grad_vi = v * fdiff\n",
    "    grad_vi[abs(grad_vi) > grad_clip] = grad_clip\n",
    "    grad_vi_update = eta*grad_vi\n",
    "    return validify(grad_vi_update)\n",
    "\n",
    "\n",
    "def calc_gradient_b(fdiff, grad_clip):\n",
    "    grad_b = fdiff\n",
    "    if grad_b > grad_clip:\n",
    "        grad_b = grad_clip\n",
    "    return validify(grad_b)\n",
    "\n",
    "\n",
    "def one_update(W, W_grad, pair, cooc_score, max_score, alpha, eta, grad_clip):\n",
    "    (i, j) = pair\n",
    "    w_i, b_i = W[i][:-1], W[i][-1]\n",
    "    w_j, b_j = W[j][:-1], W[j][-1]\n",
    "    grad_vi, grad_bi = W_grad[i][:-1], W_grad[i][-1]\n",
    "    grad_vj, grad_bj = W_grad[j][:-1], W_grad[j][-1]\n",
    "    #compute loss and shared gradient\n",
    "    loss, g = compute_loss(w_i, w_j, b_i, b_j, cooc_score, max_score, alpha)\n",
    "    #computr gradient update\n",
    "    grad_vi_update = calc_gradient_vi(g, w_j, eta, grad_clip)\n",
    "    grad_vj_update = calc_gradient_vi(g, w_i, eta, grad_clip)\n",
    "    grad_bi_update = calc_gradient_b(g, grad_clip)\n",
    "    grad_bj_update = calc_gradient_b(g, grad_clip)\n",
    "    #update W\n",
    "    W[i][:-1] -= (grad_vi_update/np.sqrt(grad_vi))\n",
    "    W[j][:-1] -= (grad_vj_update/np.sqrt(grad_vj))\n",
    "    W[i][-1] -= (grad_bi_update/np.sqrt(grad_bi))\n",
    "    W[j][-1] -= (grad_bj_update/np.sqrt(grad_bj))\n",
    "    #update W_grad\n",
    "    W_grad[i][:-1] += np.square(grad_vi_update)\n",
    "    W_grad[j][:-1] += np.square(grad_vj_update)\n",
    "    W_grad[i][-1]  += grad_bi_update**2\n",
    "    W_grad[j][-1] += grad_bj_update**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def init_matrices(n_vocab, n_dim, rng):\n",
    "    l = 0.5/float(n_dim)\n",
    "    # uniformly between [-l,l)\n",
    "    W = 2*l*rng.random((n_vocab, n_dim+1)) - l\n",
    "    grandient_squard = np.ones((n_vocab, n_dim+1), dtype=np.float64)\n",
    "    return W, grandient_squard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 1 2]\n [3 4 5]\n [6 7 8]]\n(array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3]), array([0, 1, 2, 3, 0, 2, 0, 1, 3, 0, 2]), array([1. , 3. , 1. , 0.5, 3. , 0.5, 1. , 0.5, 1. , 0.5, 1. ]))\n[[0.  3.  1.  0.  0.  2.  2.  3.  0.  1.  2. ]\n [0.  0.  2.  2.  3.  1.  3.  2.  1.  0.  0. ]\n [1.  0.5 0.5 1.  0.5 0.5 1.  1.  3.  3.  1. ]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[2. , 0. , 2. , 3. , 0. , 1. , 0. , 1. , 0. , 3. , 2. ],\n       [3. , 3. , 0. , 2. , 1. , 0. , 2. , 2. , 0. , 0. , 1. ],\n       [1. , 0.5, 1. , 1. , 3. , 3. , 1. , 0.5, 1. , 0.5, 0.5]])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "import numpy as np\n",
    "rg = default_rng(12345)\n",
    "2*rg.random((10,1))-1\n",
    "# d,f = init_matrices(4,2,rg)\n",
    "rng = np.random.default_rng()\n",
    "# rg.permutation(d, axis=0)\n",
    "arr = np.arange(9).reshape((3, 3))\n",
    "print(arr)\n",
    "rng.permutation(arr,axis=1)\n",
    "print(cooc_tpl)\n",
    "s =rng.permutation(cooc_tpl,axis=1)\n",
    "print(s)\n",
    "rng.permutation(s,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W, W_grad, cooc_tpl, n_epochs, rng, max_score, alpha, eta, grad_clip):\n",
    "    for i in range(n_epochs):\n",
    "        cooc_tpl = rng.permutation(cooc_tpl,axis=1)\n",
    "        for j in range(len(cooc_tpl[0])):\n",
    "        #select first pair of word as\n",
    "            pair = (int(cooc_tpl[0][j]), int(cooc_tpl[1][j]))\n",
    "            cooc_score = cooc_tpl[2][j]\n",
    "            loss = one_update(W, W_grad, pair, cooc_score, max_score, alpha, eta, grad_clip)\n",
    "        if i == n_epochs-1:\n",
    "            print('the last loss is ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.00591081  0.22523185 -0.17792019]\n [ 0.22432472 -0.09408427 -0.03833678]\n [ 0.1638513  -0.04540043  0.02479684]\n [-0.23622044  0.12675655  0.01907166]]\nthe last loss is  0.16155238876341566\n[[ 0.42636017  0.07392391  0.11268705]\n [ 0.86475964  0.05931378  0.48346455]\n [-0.58417307  0.04107205 -0.12712688]\n [-0.60295216  0.01171958 -0.35638232]]\n[[ 1.01811777  1.0011147  17.32257018]\n [ 1.00686823  1.00206274 14.91833432]\n [ 1.01327751  1.00043868 11.30258586]\n [ 1.00239736  1.00057218  6.84297285]]\n"
    }
   ],
   "source": [
    "# Small example of how it works:\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# cooc_stuff\n",
    "cooc_dict, vocab = get_cooc_matrix(mini_corpus, 2)\n",
    "cooc_tpl = cooc_to_numpy(cooc_dict)\n",
    "\n",
    "rng = default_rng(1)\n",
    "W, W_grad = init_matrices(len(vocab), 2, rng)\n",
    "print(W)\n",
    "train(W, W_grad, cooc_tpl, n_epochs=25, rng=rng, max_score=2, alpha=0.8, eta=0.1, grad_clip=1)\n",
    "print(W)\n",
    "print(W_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I obtain for code above\n",
    "\n",
    "# W init\n",
    "#[[ 0.00591081  0.22523185 -0.17792019]\n",
    "# [ 0.22432472 -0.09408427 -0.03833678]\n",
    "# [ 0.1638513  -0.04540043  0.02479684]\n",
    "# [-0.23622044  0.12675655  0.01907166]]\n",
    "\n",
    "\n",
    "# after training:\n",
    "#loss on last pass 0.7861722683546024\n",
    "#[[ 0.37861754  0.05811192  0.13090998]\n",
    "# [ 0.67430958 -0.00918572  0.58465184]\n",
    "# [-0.4713854   0.08926127 -0.31768868]\n",
    "# [-0.58445333  0.02673079 -0.0975816 ]]\n",
    "#[[ 1.01016694  1.0008865  12.1206226 ]\n",
    "# [ 1.003497    1.00143482  9.65947768]\n",
    "# [ 1.00864141  1.0002788   9.17531255]\n",
    "# [ 1.00211807  1.00053237  6.48303266]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}