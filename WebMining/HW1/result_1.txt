What is the difference between linear regression and logistic regression? 
When we have to predict the value of a categorical (or discrete) outcome we use logistic regression. I believe we use linear regression to also predict the value of an outcome given the input values.
Then, what is the difference between the two methodologies?


Linear regression output as probabilities
It's tempting to use the linear regression output as probabilities but it's a mistake because the output can be negative, and greater than 1 whereas probability can not. As regression might actually
produce probabilities that could be less than 0, or even bigger than
1, logistic regression was introduced. 
Source: http://gerardnico.com/wiki/data_mining/simple_logistic_regression

Outcome
In linear regression, the outcome (dependent variable) is continuous.
It can have any one of an infinite number of possible values. 
In logistic regression, the outcome (dependent variable) has only a limited number of possible values. 
The dependent variable
Logistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue,
1st/2nd/3rd/4th, etc.  
Linear regression is used when your response variable is continuous. For instance, weight, height, number of hours,  etc.
Equation
Linear regression gives an equation which is of the form Y = mX + C,
means equation with degree 1. 
However, logistic regression gives an equation which is of the form 
Y = eX + e-X
Coefficient interpretation
In linear regression, the coefficient interpretation of independent variables are quite straightforward (i.e. holding all other variables constant, with a unit increase in this variable, the dependent variable is expected to increase/decrease by xxx). 
However, in logistic regression, depends on the family (binomial, Poisson,
etc.) and link (log, logit, inverse-log, etc.) you use, the interpretation is different. 
Error minimization technique
Linear regression uses ordinary least squares method to minimise the
errors and arrive at a best possible fit, while logistic regression
uses maximum likelihood method to arrive at the solution.
Linear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically. 
Logistic regression is just the opposite. Using the logistic loss function causes large errors to be penalized to an asymptotically constant.
Consider linear regression on categorical {0, 1} outcomes to see why this is a problem. If your model predicts the outcome is 38, when the truth is 1, you've lost nothing. Linear regression would try to reduce that 38, logistic wouldn't (as much)2.

 Is there a difference between Y = e^X/1 + e^-X and Y = e^X + e^-X ? e^X/1 ? anything divide by 1 is the same. so there is no difference. I am sure you were meaning to ask something else. I know this is an old thread but given your statement "Logistic regression is used when the response variable is categorical in nature. For instance, yes/no, true/false, red/green/blue, 1st/2nd/3rd/4th, etc. "; what's the difference between this and classification then? @kingJulian Logistic regression is indeed used for classification. Check this out, you might find it useful as I have @kingJulian: Logistic regression is a classification technique and classification stands for several algorithms that try to predict few outcomes.
In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values.
For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen.
If, instead, you wanted to predict, based on size, whether a house would sell for more than $200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than $200K, or No, the house will not.
 In andrews logistic regression example of cancer, I can draw a horizontal line y=.5, (which obviously passes through  y=.5 ), ten if any point is above this line y=.5 => +ve , else -ve. So then why do I need a logistic regression. Im just trying to understand  the best case explanation to use logistic regression ? @vinita: here or here is a simple example  for not using linear regression and then thresh holding, for classification problems. logistic regression is the better classifier on categorical data than linear regression. It uses a cross-entropy error function instead of least squares. Therfore it isn't that sensitify to outliers and also doesn't punish "too correct" data points like least-squares does.
Just to add on the previous answers. 
Linear regression 
Is meant to resolve the problem of predicting/estimating the output value for a given element X (say f(x)). The result of the prediction is a cotinuous function where the values may be positive or negative. In this case you normally have an input dataset with a lot of examples and the the output value for each one of them. The goal is to be able to fit a model to this data set so you are able to predict that output for new different/never seen elements. Following is the classical example of fitting a line to set of points, but in general linear regression could be used to fit more complex models (using higher polynomial degrees):

Resolving the problem 
Linea regression can be solved in two different ways: 

Normal equation (direct way to solve the problem)
Gradient descent (Iterative approach)

Logistic regression
Is meant to resolve classification problems where given an element you have to classify the same in N categories. Typical examples are for example given a mail to classify it as spam or not, or given a vehicle find to wich category it belongs (car, truck, van, etc ..). That's basically the output is a finite set of descrete values.  
Resolving the problem 
Logistic regression problems could be resolved only by using Gradient descent. The formulation in general is very similar to linear regression the only difference is the usage of different hypothesis function. In linear regression the hypothesis has the form: 
h(x) = theta_0 + theta_1*x_1 + theta_2*x_2 .. 

where theta is the model we are trying to fit and [1, x_1, x_2, ..] is the input vector. In logistic regression the hypothesis function is different: 
g(x) = 1 / (1 + e^-x)


This function has a nice property, basically it maps any value to the range [0,1] which is appropiate to handle propababilities during the classificatin. For example in case of a binary classification g(X) could be interpreted as the probability to belong to the positive class. In this case normally you have different classes that are separated with a decision boundary which basically a curve that decides the separation between the different classes. Following is an example of dataset separated in two classes.


They are both quite similar in solving for the solution, but as others have said, one (Logistic Regression) is for predicting a category "fit" (Y/N or 1/0), and the other (Linear Regression) is for predicting a value.
So if you want to predict if you have cancer Y/N (or a probability) - use logistic.  If you want to know how many years you will live to - use Linear Regression !

The basic difference :
Linear regression is basically a regression model which means its will give a non discreet/continuous output of a function. So this approach gives the value. For example : given x what is f(x)
For example given a training set of different factors and the price of a property after training we can provide the required factors to determine what will be the property price.
Logistic regression is basically a binary classification algorithm which means that here there will be discreet valued output for the function . For example : for a given x if f(x)>threshold classify it to be 1 else classify it to be 0.
For example given a set of brain tumour size as training data we can use the size as input to determine whether its a benine or malignant tumour. Therefore here the output is discreet either 0 or 1.
*here the function is basically the hypothesis function

Simply put, linear regression is a regression algorithm, which outpus a possible continous and infinite value; logistic regression is considered as a binary classifier algorithm, which outputs the 'probability' of the input belonging to a label (0 or 1).
 Thank goodness I read your note about probability. Was about to write off logistic as a binary classifier.
Regression means continuous variable, Linear means there is linear relation between y and x. 
Ex= You are trying to predict salary from no of years of experience. So here salary is independent variable(y) and yrs of experience is dependent variable(x).
y=b0+ b1*x1

We are trying to find optimum value of constant b0 and b1 which will give us best fitting line for your observation data.
It is a equation of line which gives continuous value from x=0 to very large value.
This line is called Linear regression model.
Logistic regression is type of classification technique. Dnt be misled by term regression. Here we predict whether y=0 or 1.
Here we first need to find p(y=1) (wprobability of y=1) given x from formuale below.

Probaibility p is related to y by below formuale

Ex=we can make classification of tumour having more than 50% chance of having cancer  as 1 and tumour having less than 50% chance of having cancer as 0.

Here red point will be predicted as 0 whereas green point will be predicted as 1.

In short:
Linear Regression gives continuous output. i.e. any value between a range of values.
Logistic Regression gives discrete output. i.e. Yes/No, 0/1 kind of outputs.

Cannot agree more with the above comments. 
Above that, there are some more differences like
In Linear Regression, residuals are assumed to be normally distributed. 
In Logistic Regression, residuals need to be independent but not normally distributed. 
Linear Regression assumes that a constant change in the value of the explanatory variable results in constant change in the response variable. 
This assumption does not hold if the value of the response variable represents a probability (in Logistic Regression)
GLM(Generalized linear models) does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model.

| Basis                                                           | Linear                                                                         | Logistic                                                                                                            |
|-----------------------------------------------------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| Basic                                                           | The data is modelled using a straight line.                                    | The probability of some obtained event is represented as a linear function of a combination of predictor variables. |
| Linear relationship between dependent and independent variables | Is required                                                                    | Not required                                                                                                        |
| The independent variable                                        | Could be correlated with each other. (Specially in multiple linear regression) | Should not be correlated with each other (no multicollinearity exist).                                              |


To put it simply, if in linear regression model more test cases arrive which are far away from the threshold(say =0.5)for a prediction of y=1 and y=0. Then in that case the hypothesis will change and become worse.Therefore linear regression model is not used for classification problem.
Another Problem is that if the classification is y=0 and y=1, h(x) can be > 1 or < 0.So we use Logistic regression were 0<=h(x)<=1.

Logistic Regression is used in predicting categorical outputs like Yes/No, Low/Medium/High etc. You have basically 2 types of logistic regression Binary Logistic Regression (Yes/No, Approved/Disapproved) or Multi-class Logistic regression (Low/Medium/High, digits from 0-9 etc)
On the other hand, linear regression is if your dependent variable (y) is continuous. 
y = mx + c is a simple linear regression equation (m = slope and c is the y-intercept). Multilinear regression has more than 1 independent variable (x1,x2,x3 ... etc) 

In linear regression the outcome is continuous whereas in logistic regression, the outcome has only a limited number of possible values(discrete).
example:
In a scenario,the given value of x is size of a plot in square feet then predicting y ie rate of the plot comes under linear regression. 
If, instead, you wanted to predict, based on size, whether the plot would sell for more than 300000 Rs, you would use logistic regression. The possible outputs are either Yes, the plot will sell for more than 300000 Rs, or No.

In case of Linear Regression the outcome is continuous while in case of Logistic Regression outcome is discrete (not continuous)
To perform Linear regression we require a linear relationship between the dependent and independent variables. But to perform Logistic regression we do not require a linear relationship between the dependent and independent variables.
Linear Regression is all about fitting a straight line in the data while Logistic Regression is about fitting a curve to the data.
Linear Regression is a regression algorithm for Machine Learning while Logistic Regression is a classification Algorithm for machine learning.
Linear regression assumes gaussian (or normal) distribution of dependent variable. Logistic regression assumes binomial distribution of dependent variable.
Can someone give an example of cosine similarity, in a very simple, graphical way? 
Cosine Similarity article on Wikipedia
Can you show the vectors here (in a list or something)
and then do the math, and let us see how it works?
I'm a beginner.
 Try picking up a copy of Geometry and Meaning by Widdows (press.uchicago.edu/presssite/…), I read it through a while back and wished I had it a number of years ago, great introductory text.
Here are two very short texts to compare:

Julie loves me more than Linda loves me
Jane likes me more than Julie loves me

We want to know how similar these texts are, purely in terms of word counts (and ignoring word order). We begin by making a list of the words from both texts:
me Julie loves Linda than more likes Jane

Now we count the number of times each of these words appears in each text:
   me   2   2
 Jane   0   1
Julie   1   1
Linda   1   0
likes   0   1
loves   2   1
 more   1   1
 than   1   1

We are not interested in the words themselves though. We are interested only in
those two vertical vectors of counts. For instance, there are two instances of
'me' in each text. We are going to decide how close these two texts are to each
other by calculating one function of those two vectors, namely the cosine of
the angle between them. 
The two vectors are, again:
a: [2, 0, 1, 1, 0, 2, 1, 1]

b: [2, 1, 1, 0, 1, 1, 1, 1]

The cosine of the angle between them is about 0.822.
These vectors are 8-dimensional. A virtue of using cosine similarity is clearly
that it converts a question that is beyond human ability to visualise to one
that can be. In this case you can think of this as the angle of about 35
degrees which is some 'distance' from zero or perfect agreement.
 This is exactly what I was looking for. Exactly.  Is this considered the simplest form of "vector space model"? I am really glad this was useful to you, Alex. Sorry for the delay in responding. I haven't visited StackOverflow in a while. Actually this is an example of an "inner product space". There's a basic discussion on wikipedia. Is there any way to normalize for document length? You have to use length normalization and before that, try to use log frequency weighting on all term vectors. If your already dealing with normalized vectors, then it's the dot product of A.B. More detailed example with use of length normalization and TF-IDF: site.uottawa.ca/~diana/csi4107/cosine_tf_idf_example.pdf
I'm guessing you are more interested in getting some insight into "why" the cosine similarity works (why it provides a good indication of similarity), rather than "how" it is calculated (the specific operations used for the calculation).  If your interest is in the latter, see the reference indicated by Daniel in this post, as well as a related SO Question.
To explain both the how and even more so the why, it is useful, at first, to simplify the problem and to work only in two dimensions. Once you get this in 2D, it is easier to think of it in three dimensions, and of course harder to imagine in many more dimensions, but by then we can use linear algebra to do the numeric calculations and also to help us think in terms of lines / vectors / "planes" / "spheres" in n dimensions, even though we can't draw these.
So, in two dimensions: with regards to text similarity this means that we would focus on two distinct terms, say the words "London" and "Paris", and we'd count how many times each of these words is found in each of the two documents we wish to compare.  This gives us, for each document, a point in the the x-y plane. For example, if Doc1 had Paris once, and London four times, a point at (1,4) would present this document (with regards to this diminutive evaluation of documents).  Or, speaking in terms of vectors, this Doc1 document would be an arrow going from the origin to point (1,4).  With this image in mind, let's think about what it means for two documents to be similar and how this relates to the vectors.
VERY similar documents (again with regards to this limited set of dimensions) would have the very same number of references to Paris, AND the very same number of references to London, or maybe, they could have the same ratio of these references. A Document, Doc2, with 2 refs to Paris and 8 refs to London, would also be very similar, only with maybe a longer text or somehow more repetitive of the cities' names, but in the same proportion. Maybe both documents are guides about London, only making passing references to Paris (and how uncool that city is ;-) Just kidding!!!.
Now, less similar documents may also include references to both cities, but in different proportions. Maybe Doc2 would only cite Paris once and London seven times.
Back to our x-y plane, if we draw these hypothetical documents, we see that when they are VERY similar, their vectors overlap (though some vectors may be longer), and as they start to have less in common, these vectors start to diverge, to have a wider angle between them.
By measuring the angle between the vectors, we can get a good idea of their similarity, and to make things even easier, by taking the Cosine of this angle, we have a nice 0 to 1 or -1 to 1 value that is indicative of this similarity, depending on what and how we account for.  The smaller the angle, the bigger (closer to 1) the cosine value, and also the higher the similarity.
At the extreme, if Doc1 only cites Paris and Doc2 only cites London, the documents have absolutely nothing in common.  Doc1 would have its vector on the x-axis, Doc2 on the y-axis, the angle 90 degrees, Cosine 0. In this case we'd say that these documents are orthogonal to one another.
Adding dimensions:
With this intuitive feel for similarity expressed as a small angle (or large cosine), we can now imagine things in 3 dimensions, say by bringing the word "Amsterdam" into the mix, and visualize quite well how a document with two references to each would have a vector going in a particular direction, and we can see how this direction would compare to a document citing Paris and London three times each, but not Amsterdam, etc. As said, we can try and imagine the this fancy space for 10 or 100 cities. It's hard to draw, but easy to conceptualize.
I'll wrap up just by saying a few words about the formula itself. As I've said, other references provide good information about the calculations.
First in two dimensions. The formula for the Cosine of the angle between two vectors is derived from the trigonometric difference (between angle a and angle b):
cos(a - b) = (cos(a) * cos(b)) + (sin (a) * sin(b))

This formula looks very similar to the dot product formula:
Vect1 . Vect2 =  (x1 * x2) + (y1 * y2)

where cos(a) corresponds to the x value and sin(a) the y value, for the first vector, etc.  The only problem, is that x, y, etc. are not exactly the cos and sin values, for these values need to be read on the unit circle. That's where the denominator of the formula kicks in: by dividing by the product of the length of these vectors, the x and y coordinates become normalized.

Here's my implementation in C#.
using System;

namespace CosineSimilarity
{
    class Program
    {
        static void Main()
        {
            int[] vecA = {1, 2, 3, 4, 5};
            int[] vecB = {6, 7, 7, 9, 10};

            var cosSimilarity = CalculateCosineSimilarity(vecA, vecB);

            Console.WriteLine(cosSimilarity);
            Console.Read();
        }

        private static double CalculateCosineSimilarity(int[] vecA, int[] vecB)
        {
            var dotProduct = DotProduct(vecA, vecB);
            var magnitudeOfA = Magnitude(vecA);
            var magnitudeOfB = Magnitude(vecB);

            return dotProduct/(magnitudeOfA*magnitudeOfB);
        }

        private static double DotProduct(int[] vecA, int[] vecB)
        {
            // I'm not validating inputs here for simplicity.            
            double dotProduct = 0;
            for (var i = 0; i < vecA.Length; i++)
            {
                dotProduct += (vecA[i] * vecB[i]);
            }

            return dotProduct;
        }

        // Magnitude of the vector is the square root of the dot product of the vector with itself.
        private static double Magnitude(int[] vector)
        {
            return Math.Sqrt(DotProduct(vector, vector));
        }
    }
}

 this is awesome thank you I loved how you explained Magnitude =) That's great but what if we are working with files or strings.
For simplicity I am reducing the vector a and b:
Let :
    a : [1, 1, 0]
    b : [1, 0, 1]

Then cosine similarity (Theta):
 (Theta) = (1*1 + 1*0 + 0*1)/sqrt((1^2 + 1^2))* sqrt((1^2 + 1^2)) = 1/2 = 0.5

then inverse of cos 0.5 is 60 degrees.

This Python code is my quick and dirty attempt to implement the algorithm:
import math
from collections import Counter

def build_vector(iterable1, iterable2):
    counter1 = Counter(iterable1)
    counter2 = Counter(iterable2)
    all_items = set(counter1.keys()).union(set(counter2.keys()))
    vector1 = [counter1[k] for k in all_items]
    vector2 = [counter2[k] for k in all_items]
    return vector1, vector2

def cosim(v1, v2):
    dot_product = sum(n1 * n2 for n1, n2 in zip(v1, v2) )
    magnitude1 = math.sqrt(sum(n ** 2 for n in v1))
    magnitude2 = math.sqrt(sum(n ** 2 for n in v2))
    return dot_product / (magnitude1 * magnitude2)


l1 = "Julie loves me more than Linda loves me".split()
l2 = "Jane likes me more than Julie loves me or".split()


v1, v2 = build_vector(l1, l2)
print(cosim(v1, v2))

 Can you explain why you used set in the line "all_items = set(counter1.keys()).union(set(counter2.keys()))". @Ghos3t , that is to get list of distinct words from both documents
Using @Bill Bell example, two ways to do this in [R]
a = c(2,1,0,2,0,1,1,1)

b = c(2,1,1,1,1,0,1,1)

d = (a %*% b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))

or taking advantage of crossprod() method's performance...
e = crossprod(a, b) / (sqrt(crossprod(a, a)) * sqrt(crossprod(b, b)))


This is a simple Python code which implements cosine similarity.
from scipy import linalg, mat, dot
import numpy as np

In [12]: matrix = mat( [[2, 1, 0, 2, 0, 1, 1, 1],[2, 1, 1, 1, 1, 0, 1, 1]] )

In [13]: matrix
Out[13]: 
matrix([[2, 1, 0, 2, 0, 1, 1, 1],
        [2, 1, 1, 1, 1, 0, 1, 1]])
In [14]: dot(matrix[0],matrix[1].T)/np.linalg.norm(matrix[0])/np.linalg.norm(matrix[1])
Out[14]: matrix([[ 0.82158384]])


import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

/**
 * 
* @author Xiao Ma
* mail : 409791952@qq.com
*
*/
  public class SimilarityUtil {

public static double consineTextSimilarity(String[] left, String[] right) {
    Map<String, Integer> leftWordCountMap = new HashMap<String, Integer>();
    Map<String, Integer> rightWordCountMap = new HashMap<String, Integer>();
    Set<String> uniqueSet = new HashSet<String>();
    Integer temp = null;
    for (String leftWord : left) {
        temp = leftWordCountMap.get(leftWord);
        if (temp == null) {
            leftWordCountMap.put(leftWord, 1);
            uniqueSet.add(leftWord);
        } else {
            leftWordCountMap.put(leftWord, temp + 1);
        }
    }
    for (String rightWord : right) {
        temp = rightWordCountMap.get(rightWord);
        if (temp == null) {
            rightWordCountMap.put(rightWord, 1);
            uniqueSet.add(rightWord);
        } else {
            rightWordCountMap.put(rightWord, temp + 1);
        }
    }
    int[] leftVector = new int[uniqueSet.size()];
    int[] rightVector = new int[uniqueSet.size()];
    int index = 0;
    Integer tempCount = 0;
    for (String uniqueWord : uniqueSet) {
        tempCount = leftWordCountMap.get(uniqueWord);
        leftVector[index] = tempCount == null ? 0 : tempCount;
        tempCount = rightWordCountMap.get(uniqueWord);
        rightVector[index] = tempCount == null ? 0 : tempCount;
        index++;
    }
    return consineVectorSimilarity(leftVector, rightVector);
}

/**
 * The resulting similarity ranges from −1 meaning exactly opposite, to 1
 * meaning exactly the same, with 0 usually indicating independence, and
 * in-between values indicating intermediate similarity or dissimilarity.
 * 
 * For text matching, the attribute vectors A and B are usually the term
 * frequency vectors of the documents. The cosine similarity can be seen as
 * a method of normalizing document length during comparison.
 * 
 * In the case of information retrieval, the cosine similarity of two
 * documents will range from 0 to 1, since the term frequencies (tf-idf
 * weights) cannot be negative. The angle between two term frequency vectors
 * cannot be greater than 90°.
 * 
 * @param leftVector
 * @param rightVector
 * @return
 */
private static double consineVectorSimilarity(int[] leftVector,
        int[] rightVector) {
    if (leftVector.length != rightVector.length)
        return 1;
    double dotProduct = 0;
    double leftNorm = 0;
    double rightNorm = 0;
    for (int i = 0; i < leftVector.length; i++) {
        dotProduct += leftVector[i] * rightVector[i];
        leftNorm += leftVector[i] * leftVector[i];
        rightNorm += rightVector[i] * rightVector[i];
    }

    double result = dotProduct
            / (Math.sqrt(leftNorm) * Math.sqrt(rightNorm));
    return result;
}

public static void main(String[] args) {
    String left[] = { "Julie", "loves", "me", "more", "than", "Linda",
            "loves", "me" };
    String right[] = { "Jane", "likes", "me", "more", "than", "Julie",
            "loves", "me" };
    System.out.println(consineTextSimilarity(left,right));
}
}


Simple JAVA code to calculate cosine similarity
/**
   * Method to calculate cosine similarity of vectors
   * 1 - exactly similar (angle between them is 0)
   * 0 - orthogonal vectors (angle between them is 90)
   * @param vector1 - vector in the form [a1, a2, a3, ..... an]
   * @param vector2 - vector in the form [b1, b2, b3, ..... bn]
   * @return - the cosine similarity of vectors (ranges from 0 to 1)
   */
  private double cosineSimilarity(List<Double> vector1, List<Double> vector2) {

    double dotProduct = 0.0;
    double normA = 0.0;
    double normB = 0.0;
    for (int i = 0; i < vector1.size(); i++) {
      dotProduct += vector1.get(i) * vector2.get(i);
      normA += Math.pow(vector1.get(i), 2);
      normB += Math.pow(vector2.get(i), 2);
    }
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

 It's not a "simple, graphical way" but yet just code. Although others have made the same error too :/
Two Vectors A and B exists in a 2D space or 3D space, the angle between those vectors is cos similarity. 
If the angle is more (can reach max 180 degree) which is Cos 180=-1 and the minimum angle is 0 degree. cos 0 =1 implies the vectors are aligned to each other and hence the vectors are similar. 
cos 90=0 (which is sufficient to conclude that the vectors A and B are not similar at all and since distance cant be negative, the cosine values will lie from 0 to 1. Hence, more angle implies implies reducing similarity (visualising also it makes sense)
Difference between classification and clustering in data mining? [closed] 






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                
Closed 11 months ago.




Can someone explain what the difference is between classification and clustering in data mining?
If you can, please give examples of both to understand the main idea.

In general, in classification you have a set of predefined classes and want to know which class a new object belongs to.
Clustering tries to group a set of objects and find whether there is some relationship between the objects.
In the context of machine learning, classification is supervised learning and clustering is unsupervised learning.
Also have a look at Classification and Clustering at Wikipedia.
 Thanks for reply. So as I understand. In classification I have examples and I qroup these examples into one or another class. But in clustering I have examples but have not classes where to group examples. So in clustering based on examples I need to find clases? Am I right or is there anything important to take in mind? And pleas can You give example?  Best regards, Kristaps @Kristaps: I think you are are right so far. But you don't necessarily find classes with clustering. It is more that you want to see whether some set of items form some kind of relationship (by being closer together in some model). You normally don't find classes (if you think that you use clustering to find classes for classification). That is not the case. Instead, you have a training set at the beginning which consist of labelled items (so you know which class they belong to). You then train a classification algorithm to assign the items to the right classes and you verify the correctness (which you can do, as the data is labelled). A daily example of classification would be spam filtering. The spam filter has to decide if a mail is spam or not. You can also tell the filter if a mail has been wrongly classified. For clustering, I don't really know an example. But you could e.g. take 1000 Twitter messages, try to clustering and then examine what (and if) relation the clusters expose. I think both Classification and Clustering are Classify method, it that right ? @MrROY If by Classify you mean: "arrange (a group of people or things) in classes or categories according to shared qualities or characteristics." , then yes.
Please read the following information:



 Clustering does not require to know the number of classes. @D1X True. That's what this answer says.. (unknown number of classes)
If you have asked this question to any data mining or machine learning persons they will use the term supervised learning and unsupervised learning to explain you the difference between clustering and classification. So let me first explain you about the key word supervised and unsupervised.
Supervised learning:
suppose you have a basket and it is filled with some fresh fruits and your task is to arrange the same type fruits at one place. suppose the fruits are apple,banana,cherry, and grape.
so you already know from your previous work that, the shape of each and every fruit so it is easy to arrange the same type of fruits at one place.
here your previous work is called as trained data in data mining.
so you already learn the things from your trained data, This is because of you have a response variable which says you that if some fruit have so and so features it is grape, like that for each and every fruit.
This type of data you will get from the trained data.
This type of learning is called as supervised learning.
This type solving problem comes under Classification.
So you already learn the things so you can do you job confidently.
unsupervised :
suppose you have a basket and it is filled with some fresh fruits and your task is to arrange the same type fruits at one place.
This time you don't  know any thing about that fruits, you are first time seeing these fruits so how  will you arrange the same type of fruits.
What you will do first  is you take on the fruit and you will select any physical character of that particular fruit. suppose you taken color.
Then you will arrange them based on the color, then the  groups will be some thing like this.
RED COLOR GROUP: apples & cherry fruits.
GREEN COLOR GROUP: bananas & grapes.
so now you will take another physical character as size, so now the groups will be some thing like this.
RED COLOR AND BIG SIZE: apple.
RED COLOR AND SMALL SIZE: cherry fruits.
GREEN COLOR AND BIG SIZE: bananas.
GREEN COLOR AND SMALL SIZE: grapes.
job done happy ending.
here you didn't  learn any thing before ,means no train data and no response variable.
This type of learning is known unsupervised learning.
clustering comes under unsupervised learning.

+Classification:
you are given some new data, you have to set new label for them.
For example, a company wants to classify their prospect customers. When a new customer comes, they have to determine if this is a customer who is going to buy their products or not.
+Clustering:
you're given a set of history transactions which recorded who bought what.
By using clustering techniques, you can tell the segmentation of your customers. 
 I would argue that "When a new customer comes, they have to determine if this is a customer who is going to buy their products or not." is a better candidate for logistic regression. An example of classification would be to predict whether the customer is going to buy the "premium","standard" or "economy" model. Airline example: coach, coach with early boarding, coach with extra leg room.
I am sure a number of you have heard about machine learning. A dozen of you might even know what it is. And a couple of you might have worked with machine learning algorithms too. 
You see where this is going? Not a lot of people are familiar with the technology that will be absolutely essential 5 years from now. Siri is machine learning. Amazon’s Alexa is machine learning. Ad and shopping item recommender systems are machine learning. 
Let’s try to understand machine learning with a simple analogy of a 2 year old boy. Just for fun, let’s call him Kylo Ren

Let’s assume Kylo Ren saw an elephant. What will his brain tell him ?(Remember he has minimum thinking capacity, even if he is the successor to Vader). His brain will tell him that he saw a big moving creature which was grey in color. He sees a cat next, and his brain tells him that it is a small moving creature which is golden in color. Finally, he sees a light saber next and his brain tells him that it is a non-living object which he can play with!
His brain at this point knows that saber is different from the elephant and the cat, because the saber is something to play with and doesn’t move on its own. His brain can figure this much out even if Kylo doesn’t know what movable means. This simple phenomenon is called Clustering .

Machine learning is nothing but the mathematical version of this process.
A lot of people who study statistics realized that they can make some equations work in the same way as brain works. 
Brain can cluster similar objects, brain can learn from mistakes and brain can learn to identify things.
All of this can be represented with statistics, and the computer based simulation of this process is called Machine Learning. Why do we need the computer based simulation? because computers can do heavy math faster than human brains. 
I would love to go into the mathematical/statistical part of machine learning but you don’t wanna jump into that without clearing some concepts first.
Let’s get back to Kylo Ren. Let’s say Kylo picks up the saber and starts playing with it. He accidentally hits a stormtrooper and the stormtrooper gets injured. He doesn’t understand what’s going on and continues playing. Next he hits a cat and the cat gets injured. This time Kylo is sure he has done something bad, and tries to be somewhat careful. But given his bad saber skills, he hits the elephant and is absolutely sure that he is in trouble. 
He becomes extremely careful thereafter, and only hits his dad on purpose as we saw in Force Awakens!!

This entire process of learning from your mistake can be mimicked with equations, where the feeling of doing something wrong is represented by an error or cost. This process of identifying what not to do with a saber is called Classification . 
Clustering and Classification are the absolute basics of machine learning. Let’s look at the difference between them.
Kylo differentiated between animals and light saber because his brain decided that light sabers cant move by themselves and are therefore, different. The decision was based solely upon the objects present (data) and no external help or advice was provided. 
In contrast to this, Kylo differentiated the importance of being careful with light saber by first observing what hitting an object can do. The decision wasn’t completely based on the saber, but on what it could do to different objects . In short, there was some help here.

Because of this difference in learning, Clustering is called an unsupervised learning method and Classification is called a supervised learning method. 
They are very different in the machine learning world, and are often dictated by the kind of data present. Obtaining labelled data (or things that help us learn , like stormtrooper,elephant and cat in Kylo’s case) is often not easy and becomes very complicated when the data to be differentiated is large. On the other hand, learning without labels can have it’s own disadvantages , like not knowing what are the label titles. 
If Kylo was to learn being careful with the saber without any examples or help, he wouldn’t know what it would do. He would just know that it is not suppose to be done. It’s kind of a lame analogy but you get the point!
We are just getting started with Machine Learning. Classification itself can be classification of continuous numbers or classification of labels. For instance, if Kylo had to classify what each stormtrooper’s height is, there would be a lot of answers because the heights can be 5.0, 5.01, 5.011, etc. But a simple classification like types of light sabers (red,blue.green) would have very limited answers. Infact they can be represented with simple numbers. Red can be 0 , Blue can be 1 and Green can be 2.
If you know basic math, you know that 0,1,2 and 5.1,5.01,5.011 are different and are called discrete and continuous numbers respectively. The classification of discrete numbers is called Logistic Regression , and classification of continuous numbers is called Regression. 
Logistic Regression is also known as categorical classification, so don’t be confused when you read this term elsewhere
This was a very basic introduction to Machine Learning. I will dwell into the statistical side in my next post. Please let me know if I need any corrections :)
Second part posted here.

 Hey Amit, why don't you add your blog post to the answer instead of just a link. Link only answers are frowned upon because sites go down and with it the answer. Thanks. I will do that :) "The classification of discrete numbers is called Logistic Regression" is massive oversimplification; LR is just one of many possible techniques that could be used for classification (others being tree-based (RF, GBT/XGB), NN-based, etc. "The classification of continuous numbers is called Regression" is plain self-contradictory; you mean "The prediction of continuous numbers".
I'm a new comer to Data Mining, but as my textbook says, CLASSICIATION is supposed to be supervised learning, and CLUSTERING unsupervised learning. The difference between supervised learning and unsupervised learning can be found here.

Classification
Is the assignment of predefined classes to new observations, based on learning from examples.
It is one of the key tasks in machine learning.
Clustering (or Cluster Analysis)
While popularly dismissed as "unsupervised classification" it is quite different.
In contrast to what many machine learners will teach you, it is not about assigning "classes" to objects, but without having them predefined. This is the very limited view of people who did too much classification; a typical example of if you have a hammer (classifier), everything looks like a nail (classification problem) to you. But it is also why classification people do not get a hang of clustering.
Instead, consider it as structure discovery. The task of clustering is to find structure (e.g. groups) in your data that you did not know before. Clustering has been successful if you learned something new. It failed, if you only got the structure you already knew.
Cluster analysis is a key task of data mining (and the ugly duckling in machine-learning, so don't listen to machine learners dismissing clustering).
"Unsupervised learning" is somewhat an Oxymoron
This has been iterated up and down the literature, but unsupervised learning is bllsht. It does not exist, but it is an oxymoron like "military intelligence".
Either the algorithm learns from examples (then it is "supervised learning"), or it does not learn. If all the clustering methods are "learning", then computing the minimum, maximum and average of a data set is "unsupervised learning", too. Then any computation "learned" its output. Thus the term 'unsupervised learning' is totally meaningless, it means everything and nothing.
Some "unsupervised learning" algorithms do, however, fall into the optimization category. For example k-means is a least-squares optimization. Such methods are all over statistics, so I don't think we need to label them "unsupervised learning", but instead should continue to call them "optimization problems". It's more precise, and more meaningful.
There are plenty of clustering algorithms who do not involve optimization, and who do not fit into machine-learning paradigms well. So stop squeezing them in there under the umbrella "unsupervised learning".
There is some "learning" associated with clustering, but it is not the program that learns. It is the user that is supposed to learn new things about his data set.
 This answer made me realize that I was a classification person. It really helped me. Every clustering algorithm assumes a general meta model. Btw, the same goes for supervised learning, e.g. neural networks (NN). In both cased (supervised and unsupervised), we optimize the general meta model's parameters to fit the data according to a (sometimes hidden) cost function. (This part is called "learning".) This goes for both, clustering and classifcation. In both cases we learn a specific model (based on a assumed general meta model) via optimized according to the presented data. Using the specific optimized clustering model we are able to cluster the data into groups. TL;DR: You are right that clustering is not "unsupervised classifcation", but usupervised learning is not an oxymoron - learning is happening! Where is the "learning" in DBSCAN, for example?
By clustering, you can group data with your desired properties such as the number, the shape, and other properties of extracted clusters. While, in classification, the number and the shape of groups are fixed.
Most of the clustering algorithms give the number of clusters as a parameter. However, there are some approaches to find out the appropriate number of clusters.

First of all, like many answers state here: classification is supervised learning and clustering is unsupervised. This means:

Classification needs labeled data so the classifiers can be trained on this data, and after that start classifying new unseen data based on what he knows. Unsupervised learning like clustering does not uses labeled data, and what it actually does is to discover intrinsic structures in the data like groups. 
Another difference between both techniques (related to the previous one), is the fact that classification is a form of discrete regression problem where the output is a categorical dependent variable. Whereas clustering's output yields a set of subsets called groups. The way to evaluate these two models is also different for the same reason: in classification you often have to check for the precision and recall, things like overfitting and underfitting, etc. Those things will tell you how good is the model. But in clustering you usually need the vision of and expert to interpret what you find, because you don't know what type of structure you have (type of group or cluster). That's why clustering belongs to exploratory data analysis. 
Finally, i would say that applications are the main difference between both. Classification as the word says, is used to discriminate instances that belong to a class or another, for example a man or a woman, a cat or a dog, etc. Clustering is often used in the diagnosis of medical illness, discovery of patterns, etc. 


Classification
– Predicts categorical class labels
– Classifies data (constructs a model) based on a training set and the values (class labels) in a class label attribute 
– Uses the model in classifying new data
Cluster: a collection of data objects
– Similar to one another within the same cluster
– Dissimilar to the objects in other clusters

Clustering aims at finding groups in data. “Cluster” is an intuitive concept and does
not have a mathematically rigorous definition. The members of one cluster should be
similar to one another and dissimilar to the members of other clusters. A clustering
algorithm operates on an unlabeled data set Z and produces a partition on it.
For Classes and Class Labels,
class contains similar objects, whereas objects from different classes
are dissimilar. Some classes have a clear-cut meaning, and in the simplest case
are mutually exclusive. For example, in signature verification, the signature is either
genuine or forged. The true class is one of the two, no matter that we might not be
able to guess correctly from the observation of a particular signature.

Clustering is a method of grouping objects in such a way that objects with similar features come together, and objects with dissimilar features go apart. It is a common technique for statistical data analysis used in machine learning and data mining.. 
Classification is a process of categorization where objects are recognized, differentiated and understood on the basis of the training set of data. Classification is a supervised learning technique where a training set and correctly defined observations are available.

From book Mahout in Action, and I think it explains the difference very well:

Classification algorithms are related to, but still quite different from, clustering algorithms such as the k-means algorithm. 
Classification algorithms are a form of supervised learning, as opposed to unsupervised learning, which happens with clustering algorithms. 
A supervised learning algorithm is one that’s given examples that contain the desired value of a target variable. Unsupervised algorithms aren’t given the desired answer, but instead must find something plausible on their own.


One liner for Classification:
Classifying data into pre-defined categories
One liner for Clustering:
Grouping data into a set of categories
Key difference:
Classification is taking data and putting it into pre-defined categories and in Clustering the set of categories, that you want to group the data into, is not known beforehand.
Conclusion:

Classification assigns the category to 1 new item, based on already
labeled items while Clustering takes a bunch of unlabeled items and
divide them into the categories
In Classification, the categories\groups to be divided are known
beforehand while in Clustering, the categories\groups to be divided
are unknown beforehand
In Classification, there are 2 phases – Training phase and then the
test phase while in Clustering, there is only 1 phase – dividing of
training data in clusters
Classification is Supervised Learning while Clustering is
Unsupervised Learning

I have written a long post on the same topic which you can find here: 
https://neelbhatt40.wordpress.com/2017/11/21/classification-and-clustering-machine-learning-interview-questions-answers-part-i/

Classification: Predict results in a discrete output => map input variables into discrete categories

Popular use cases:

Email classification : Spam or non-Spam
Sanction loan to customer : Yes if he is capable of paying EMI for the sanctioned loan amount. No if he can't
Cancer tumour cells identification : Is it critical or non-critical?
Sentiment analysis of tweets : Is the tweet positive or negative or neutral
Classification of news  : Classify the news into one of predefined classes - Politics, Sports, Health etc

Clustering: is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)


Popular use cases:

Marketing : Discover customer segments for marketing purposes
Biology : Classification among different species of plants and animals
Libraries : Clustering different books on the basis of topics and information
Insurance : Acknowledge the customers, their policies and identifying the frauds
City Planning : Make groups of houses and to study their values based on their geographical locations and other factors.
Earthquake studies : Identify  dangerous zones
Recommendation system : 

References:
geeksforgeeks
dataaspirant
3leafnodes

If you are trying to file up a large number of sheets on to your shelf(based on date or some other specification of the file), you are CLASSIFYING.
If you were to create clusters from the set of sheets, it would mean that there is something similar among the sheets.

There are two definitions in data mining "Supervised" and "Unsupervised".
When someone tells the computer, algorithm, code, ... that this thing is like an apple and that thing is like an orange, this is supervised learning and using supervised learning (like tags for each sample in a data set) for classifying the data, you'll get classification. But on the other hand if you let the computer find out what is what and differentiate between features of the given data set, in fact learning unsupervised, for classifying the data set this would be called clustering. In this case data that are fed to the algorithm don't have tags and the algorithm should find out different classes.

Machine Learning or AI is largely perceived by the task it Performs/achieves.
In my opinion, by thinking about Clustering and Classification in notion of task they achieve can really help to understand the difference between the two.
Clustering is to Group things and Classification is to, kind of, label things.
Let's assume you are in a party hall where all men are in Suits and women are in Gowns.
Now, you ask your friend few questions:
Q1: Heyy, can you help me group people?
Possible answers that your friend can give are:
1: He can group people based on Gender, Male or Female
2: He can group people based on their clothes, 1 wearing suits other wearing gowns
3: He can group people based on color of their hairs
4: He can group people based on their age group, etc. etc. etc.
Their are numerous ways your friend can complete this task.
Of course, you can influence his decision making process by providing extra inputs like:
Can you help me group these people based on gender (or age group, or hair color or dress etc.)
Q2:
Before Q2, you need to do some pre-work.
You have to teach or inform your friend so that he can take informed decision. So, let's say you said to your friend that:

People with long hair are Women.
People with short hair are Men.

Q2.  Now, you point out to a Person with long hair and ask your friend - Is it a Man or a Woman?
The only answer that you can expect is: Woman.
Of course, there can be men with long hairs and women with short hairs in the party. But, the answer is correct based on the learning you provided to your friend. You can further improve the process by teaching more to your friend on how to differentiate between the two.
In above example,
Q1 represents the task what Clustering achieves.
In Clustering you provide the data(people) to the algorithm(your friend) and ask it to group the data. 
Now, it's up to algorithm to decide what's the best way to group is? (Gender, Color or age group).
Again,you can definitely influence the decision made by the algorithm by providing extra inputs.
Q2 represents the task Classification achieves.
There, you give your algorithm(your friend) some data(People), called as Training data, and made him learn which data corresponds to which label(Male or Female). Then you point your algorithm to certain data, called as Test data, and ask it to determine whether it is Male or Female. The better your teaching is, the better it's prediction.
And the Pre-work in Q2 or Classification is nothing but just training your model so that it can learn how to differentiate. In Clustering or Q1 this pre-work is the part of grouping.
Hope this helps someone.
Thanks


Classification- A data-set can have different groups/ classes. red, green and black. Classification will try to find rules that divides them in different classes.     
Custering- if a data-set is not having any class and you want to put them in some class/grouping, you do clustering. The purple circles above. 
If classification rules are not good, you will have mis-classification in testing or ur rules are not correct enough.
  if clustering is not good, you will have lot of outliers ie. data points not able to fall in any cluster.

The Key Differences Between Classification and Clustering  are:
Classification is the process of classifying the data with the help of class labels. On the other hand, Clustering is similar to classification but there are no predefined class labels.
Classification is geared with supervised learning. As against, clustering is also known as unsupervised learning.
Training sample is provided in the classification method while in the case of clustering training data is not provided.
Hope this will help!

I believe classification is classifying records in a data set into predefined classes or even defining classes on the go. I look at it as pre-requisite for any valuable data mining, I like to think of it at unsupervised learning i.e. one does not know what he/she is looking for while mining the data and classification serves as a good starting point
Clustering on the other end falls under supervised learning i.e. one know what parameters to look for, the correlation between them along with critical levels. I believe it requires some understanding of statistics and maths
 It is actually the other way around. Classification is usually supervised and clustering is usually unsupervised.How does the Amazon Recommendation feature work? 
What technology goes in behind the screens of Amazon recommendation technology? I believe that Amazon recommendation is currently the best in the market, but how do they provide us with such relevant recommendations?
Recently, we have been involved with similar recommendation kind of project, but would surely like to know about the in and outs of the Amazon recommendation technology from a technical standpoint. 
Any inputs would be highly appreciated. 
Update:
This patent explains how personalized recommendations are done but it is not very technical, and so it would be really nice if some insights could be provided. 
From the comments of Dave, Affinity Analysis forms the basis for such kind of Recommendation Engines. Also here are some good reads on the Topic

Demystifying Market Basket Analysis
Market Basket Analysis
Affinity Analysis

Suggested Reading:

Data Mining: Concepts and Technique

 Did you try sifting through their pile of patents? google.com/patents Yes I did going through the Patent but it is not very technical and so would appreciate some insights on technical aspect of the mechanism @Dave: This question is open now and I would really appreciate if you can give some more insights on the technology behind recommendation engine architecture and functionality which Amazon uses for providing recommendations. I wouldn't call Amazon recommendation system "the best" or even useful. You order a mouse, it suggests to buy another one as well. Why would a normal user need two mice at once??? @Rachel: Given that you know Amazon's system is patented and you want to develop something similar, I would not even glance at their patents---and of course not looking at them doesn't protect you either, but it's a start.
It is both an art and a science.  Typical fields of study revolve around market basket analysis (also called affinity analysis) which is a subset of the field of data mining.  Typical components in such a system include identification of primary driver items and the identification of affinity items (accessory upsell, cross sell).    
Keep in mind the data sources they have to mine...  

Purchased shopping carts = real money from real people spent on real items = powerful data and a lot of it.  
Items added to carts but abandoned.  
Pricing experiments online (A/B testing, etc.) where they offer the same products at different prices and see the results  
Packaging experiments (A/B testing, etc.) where they offer different products in different "bundles" or discount various pairings of items  
Wishlists - what's on them specifically for you - and in aggregate it can be treated similarly to another stream of basket analysis data  
Referral sites (identification of where you came in from can hint other items of interest)  
Dwell times (how long before you click back and pick a different item)  
Ratings by you or those in your social network/buying circles - if you rate things you like you get more of what you like and if you confirm with the "i already own it" button they create a very complete profile of you  
Demographic information (your shipping address, etc.) - they know what is popular in your general area for your kids, yourself, your spouse, etc.  
user segmentation = did you buy 3 books in separate months for a toddler?  likely have a kid or more.. etc.  
Direct marketing click through data - did you get an email from them and click through?  They know which email it was and what you clicked through on and whether you bought it as a result.  
Click paths in session - what did you view regardless of whether it went in your cart
Number of times viewed an item before final purchase  
If you're dealing with a brick and mortar store they might have your physical purchase history to go off of as well (i.e. toys r us or something that is online and also a physical store)
etc. etc. etc.  

Luckily people behave similarly in aggregate so the more they know about the buying population at large the better they know what will and won't sell and with every transaction and every rating/wishlist add/browse they know how to more personally tailor recommendations.  Keep in mind this is likely only a small sample of the full set of influences of what ends up in recommendations, etc.  
Now I have no inside knowledge of how Amazon does business (never worked there) and all I'm doing is talking about classical approaches to the problem of online commerce - I used to be the PM who worked on data mining and analytics for the Microsoft product called Commerce Server.  We shipped in Commerce Server the tools that allowed people to build sites with similar capabilities.... but the bigger the sales volume the better the data the better the model - and Amazon is BIG.  I can only imagine how fun it is to play with models with that much data in a commerce driven site.  Now many of those algorithms (like the predictor that started out in commerce server) have moved on to live directly within Microsoft SQL.
The four big take-a-ways you should have are:  

Amazon (or any retailer) is looking at aggregate data for tons of transactions and tons of people... this allows them to even recommend pretty well for anonymous users on their site.  
Amazon (or any sophisticated retailer) is keeping track of behavior and purchases of anyone that is logged in and using that to further refine on top of the mass aggregate data.  
Often there is a means of over riding the accumulated data and taking "editorial" control of suggestions for product managers of specific lines (like some person who owns the 'digital cameras' vertical or the 'romance novels' vertical or similar) where they truly are experts  
There are often promotional deals (i.e. sony or panasonic or nikon or canon or sprint  or verizon pays additional money to the retailer, or gives a better discount at larger quantities or other things in those lines) that will cause certain "suggestions" to rise to the top more often than others - there is always some reasonable business logic and business reason behind this targeted at making more on each transaction or reducing wholesale costs, etc.  

In terms of actual implementation?  Just about all large online systems boil down to some set of pipelines (or a filter pattern implementation or a workflow, etc.  you call it what you will) that allow for a context to be evaluated by a series of modules that apply some form of business logic.  
Typically a different pipeline would be associated with each separate task on the page - you might have one that does recommended "packages/upsells" (i.e. buy this with the item you're looking at) and one that does "alternatives" (i.e. buy this instead of the thing you're looking at) and another that pulls items most closely related from your wish list (by product category or similar).  
The results of these pipelines are able to be placed on various parts of the page (above the scroll bar, below the scroll, on the left, on the right, different fonts, different size images, etc.) and tested to see which perform best.  Since you're using nice easy to plug and play modules that define the business logic for these pipelines you end up with the moral equivalent of lego blocks that make it easy to pick and choose from the business logic you want applied when you build another pipeline which allows faster innovation, more experimentation, and in the end higher profits.  
Did that help at all?  Hope that give you a little bit of insight how this works in general for just about any ecommerce site - not just Amazon.  Amazon (from talking to friends that have worked there) is very data driven and continually measures the effectiveness of it's user experience and the pricing, promotion, packaging, etc. - they are a very sophisticated retailer online and are likely at the leading edge of a lot of the algorithms they use to optimize profit - and those are likely proprietary secrets (you know like the formula to KFC's secret spices) and guaarded as such.
 Yes. It did helped me alot and I really appreciate your inputs on the topic. amazing insights
This isn't directly related to Amazon's recommendation system, but it might be helpful to study the methods used by people who competed in the Netflix Prize, a contest to develop a better recommendation system using Netflix user data. A lot of good information exists in their community about data mining techniques in general. 
The team that won used a blend of the recommendations generated by a lot of different models/techniques. I know that some of the main methods used were principal component analysis, nearest neighbor methods, and neural networks. Here are some papers by the winning team:
R. Bell, Y. Koren, C. Volinsky, "The BellKor 2008 Solution to the Netflix Prize", (2008).
A. Töscher, M. Jahrer, “The BigChaos Solution to the Netflix Prize 2008", (2008).
A. Töscher, M. Jahrer, R. Legenstein, "Improved Neighborhood-Based Algorithms for Large-Scale Recommender Systems",  SIGKDD Workshop on Large-Scale Recommender Systems and the Netflix Prize Competition  (KDD’08) , ACM Press (2008).
Y. Koren, "The BellKor Solution to the Netflix Grand Prize", (2009).
A. Töscher, M. Jahrer, R. Bell, "The BigChaos Solution to the Netflix Grand Prize", (2009).
M. Piotte, M. Chabbert, "The Pragmatic Theory solution to the Netflix Grand Prize", (2009).
The 2008 papers are from the first year's Progress Prize. I recommend reading the earlier ones first because the later ones build upon the previous work.
 What I like about this answer is that it points to the fact that there is no "perfect" answer and that people keep innovating in this area - there is always some room for improvement and as times change and new methods are applied to the problems it will keep getting solved differently.  And if you read the detailed links you can see how there is a "blend" of several approches to prediction within each of the big contenders for the prize.  Great references.
I bumped on this paper today:

Amazon.com Recommendations: Item-to-Item Collaborative Filtering

Maybe it provides additional information.

(Disclamer: I used to work at Amazon, though I didn't work on the recommendations team.)
ewernli's answer should be the correct one -- the paper links to Amazon's original recommendation system, and from what I can tell (both from personal experience as an Amazon shopper and having worked on similar systems at other companies), very little has changed: at its core, Amazon's recommendation feature is still very heavily based on item-to-item collaborative filtering.
Just look at what form the recommendations take: on my front page, they're all either of the form "You viewed X...Customers who also viewed this also viewed...", or else a melange of items similar to things I've bought or viewed before. If I specifically go to my "Recommended for You" page, every item describes why it's recommended for me: "Recommended because you purchased...", "Recommended because you added X to your wishlist...", etc. This is a classic sign of item-to-item collaborative filtering.
So how does item-to-item collaborative filtering work? Basically, for each item, you build a "neighborhood" of related items (e.g., by looking at what items people have viewed together or what items people have bought together -- to determine similarity, you can use metrics like the Jaccard index; correlation is another possibility, though I suspect Amazon doesn't use ratings data very heavily). Then, whenever I view an item X or make a purchase Y, Amazon suggests me things  in the same neighborhood as X or Y.
Some other approaches that Amazon could potentially use, but likely doesn't, are described here: http://blog.echen.me/2011/02/15/an-overview-of-item-to-item-collaborative-filtering-with-amazons-recommendation-system/
A lot of what Dave describes is almost certainly not done at Amazon. (Ratings by those in my social network? Nope, Amazon doesn't have any of my social data. This would be a massive privacy issue in any case, so it'd be tricky for Amazon to do even if they had that data: people don't want their friends to know what books or movies they're buying. Demographic information? Nope, nothing in the recommendations suggests they're looking at this. [Unlike Netflix, who does surface what other people in my area are watching.])
 Amazon for years exposed purchase circles - which were anonymized buying trends based on company domain name and zip code.  There had to be at least 50 distinct buyers in a purchase circle before you could get information about it - so amazon certainly did keep, track and use that information to help themselves (and others) understand what was popular.  You could look at cds, electronics, and book purchases in your area or at your company.  They were exposed from 1999-2002.  forum.dvdtalk.com/archive/t-122436.html even has a copy and paste of the text from amazon describing them.
I don't have any knowledge of Amazon's algorithm specifically, but one component of such an algorithm would probably involve tracking groups of items frequently ordered together, and then using that data to recommend other items in the group when a customer purchases some subset of the group.
Another possibility would be to track the frequency of item B being ordered within N days after ordering item A, which could suggest a correlation.

As far I know, it's use Case-Based Reasoning as an engine for it.
You can see in this sources: here, here and here.
There are many sources in google searching for amazon and case-based reasoning.

Someone did a presentation at our University on something similar last week, and referenced the Amazon recommendation system. I believe that it uses a form of K-Means Clustering to cluster people into their different buying habits. Hope this helps :)
Check this out too: http://www.almaden.ibm.com/cs/people/dmodha/ml02.ps and as HTML.

If you want a hands-on tutorial (using open-source R) then you could do worse than going through this:
https://gist.github.com/yoshiki146/31d4a46c3d8e906c3cd24f425568d34e
It is a run-time optimised version of another piece of work: 
http://www.salemmarafi.com/code/collaborative-filtering-r/
However, the variation of the code on the first link runs MUCH faster so I recommend using that (I found the only slow part of yoshiki146's code is the final routine which generates the recommendation at user level - it took about an hour with my data on my machine). 
I adapted this code to work as a recommendation engine for the retailer I work for.
The algorithm used is - as others have said above - collaborative filtering. This method of CF calculates a cosine similarity matrix and then sorts by that similarity to find the 'nearest neighbour' for each element (music band in the example given, retail product in my application).
The resulting table can recommend a band/product based on another chosen band/product.
The next section of the code goes a step further with USER (or customer) based collaborative filtering.
The output of this is a large table with the top 100 bands/products recommended for a given user/customer
 While this link may answer the question, it is better to include the essential parts of the answer here and provide the link for reference.  Link-only answers can become invalid if the linked page changes. - From Review True...   The algorithm used is - as others have said above - collaborative filtering. This method of CF calculates a cosine similarity matrix and then sorts by that similarity to find the 'nearest neighbour' for each element (music band in the example given, retail product in my application).  The resulting table can recommend a band/product based on another chosen band/product.  The next section of the code goes a step further with USER (or customer) based collaborative filtering. The output of this is a large table with the top 100 bands/products recommended for a given user/customer please edit you answer and add this information there, not in comments section, thanksWhy does one hot encoding improve machine learning performance? 
I have noticed that when One Hot encoding is used on a particular data set (a matrix) and used as training data for learning algorithms, it gives significantly better results with respect to prediction accuracy, compared to using the original matrix itself as training data. How does this performance increase happen? 

Many learning algorithms either learn a single weight per feature, or they use distances between samples. The former is the case for linear models such as logistic regression, which are easy to explain.
Suppose you have a dataset having only a single categorical feature "nationality", with values "UK", "French" and "US". Assume, without loss of generality, that these are encoded as 0, 1 and 2. You then have a weight w for this feature in a linear classifier, which will make some kind of decision based on the constraint w×x + b > 0, or equivalently w×x < b.
The problem now is that the weight w cannot encode a three-way choice. The three possible values of w×x are 0, w and 2×w. Either these three all lead to the same decision (they're all < b or ≥b) or "UK" and "French" lead to the same decision, or "French" and "US" give the same decision. There's no possibility for the model to learn that "UK" and "US" should be given the same label, with "French" the odd one out.
By one-hot encoding, you effectively blow up the feature space to three features, which will each get their own weights, so the decision function is now w[UK]x[UK] + w[FR]x[FR] + w[US]x[US] < b, where all the x's are booleans. In this space, such a linear function can express any sum/disjunction of the possibilities (e.g. "UK or US", which might be a predictor for someone speaking English).
Similarly, any learner based on standard distance metrics (such as k-nearest neighbors) between samples will get confused without one-hot encoding. With the naive encoding and Euclidean distance, the distance between French and US is 1. The distance between US and UK is 2. But with the one-hot encoding, the pairwise distances between [1, 0, 0], [0, 1, 0] and [0, 0, 1] are all equal to √2.
This is not true for all learning algorithms; decision trees and derived models such as random forests, if deep enough, can handle categorical variables without one-hot encoding.
 Thanks for this Lars, but when we do a OneHotEncoding which is effectively increase the number of features, do we not need to increase the samples too, to make sure it does not overfit. @Manoj Compared to the obvious alternative representation of categorical variables, encoding each level as a distinct integer, I don't think it matters: you need sufficient statistics either way. Is there any literature you could point to so I could read further into this? Thanks. Is there a benefit to using a less than full rank matrix (which you wouldn't do when building a regular statistical model) when employing machine learning techniques such as boosting? This looks to me just like what a statistician would call "dummy variables." But maybe there is some saving of storage space.
Regarding the increase of the features by doing one-hot-encoding one can use feature hashing. When you do hashing, you can specify the number of buckets to be much less than the number of the newly introduced features.  

When you want to predict categories, you want to predict items of a set. Not using one-hot encoding is akin to letting the categories have neighbour categories (e.g.: if you did a regression with the integers of the categories instead) organized in a certain way and in a certain order. 
Now, what happens if you assign category 0 to 0, category 1 to 1, and category 2 to 2 without one-hot encoding, and that your algorithm's prediction isn't sure if it should choose 0 or 2: should he predict 1 despite he thinks it's either 0 or 2?
You see where it goes. The same goes for your data inputs: if they shouldn't be supposed to be neighbours, then don't show them to your algorithm as neighbours. 
What is an intuitive explanation of the Expectation Maximization technique? [closed] 






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                
Closed 2 years ago.




Expectation Maximization (EM) is a kind of probabilistic method to classify data. Please correct me if I am wrong if it is not a classifier. 
What is an intuitive explanation of this EM technique? What is expectation here and what is being maximized?
 What is the expectation maximization algorithm?, Nature Biotechnology 26, 897–899 (2008) has a nice picture that illustrates how the algorithm works. @chl In part b of the nice picture, how did they get the values of the probability distribution on the Z (i.e., 0.45xA, 0.55xB, etc.)? You can look at this question math.stackexchange.com/questions/25111/… Updated link to the picture that @chl mentioned.
Note: the code behind this answer can be found here.

Suppose we have some data sampled from two different groups, red and blue:

Here, we can see which data point belongs to the red or blue group. This makes it easy to find the parameters that characterise each group. For example, the mean of the red group is around 3, the mean of the blue group is around 7 (and we could find the exact means if we wanted).
This is, generally speaking, known as maximum likelihood estimation. Given some data, we compute the value of a parameter (or parameters) that best explains that data.
Now imagine that we cannot see which value was sampled from which group. Everything looks purple to us:

Here we have the knowledge that there are two groups of values, but we don't know which group any particular value belongs to. 
Can we still estimate the means for the red group and blue group that best fit this data?
Yes, often we can! Expectation Maximisation gives us a way to do it. The very general idea behind the algorithm is this:

Start with an initial estimate of what each parameter might be.
Compute the likelihood that each parameter produces the data point.
Calculate weights for each data point indicating whether it is more red or more blue based on the likelihood of it being produced by a parameter. Combine the weights with the data (expectation).
Compute a better estimate for the parameters using the weight-adjusted data (maximisation).
Repeat steps 2 to 4 until the parameter estimate converges (the process stops producing a different estimate).

These steps need some further explanation, so I'll walk through the  problem described above.
Example: estimating mean and standard deviation
I'll use Python in this example, but the code should be fairly easy to understand if you're not familiar with this language. 
Suppose we have two groups, red and blue, with the values distributed as in the image above. Specifically, each group contains a value drawn from a normal distribution with the following parameters:
import numpy as np
from scipy import stats

np.random.seed(110) # for reproducible results

# set parameters
red_mean = 3
red_std = 0.8

blue_mean = 7
blue_std = 2

# draw 20 samples from normal distributions with red/blue parameters
red = np.random.normal(red_mean, red_std, size=20)
blue = np.random.normal(blue_mean, blue_std, size=20)

both_colours = np.sort(np.concatenate((red, blue))) # for later use...

Here is an image of these red and blue groups again (to save you from having to scroll up):

When we can see the colour of each point (i.e. which group it belongs to), it's very easy to estimate the mean and standard deviation for each each group. We just pass the red and blue values to the builtin functions in NumPy. For example:
>>> np.mean(red)
2.802
>>> np.std(red)
0.871
>>> np.mean(blue)
6.932
>>> np.std(blue)
2.195

But what if we can't see the colours of the points? That is, instead of red or blue, every point has been coloured purple.
To try and recover the mean and standard deviation parameters for the red and blue groups, we can use Expectation Maximisation.
Our first step (step 1 above) is to guess at the parameter values for each group's mean and standard deviation. We don't have to guess intelligently; we can pick any numbers we like:
# estimates for the mean
red_mean_guess = 1.1
blue_mean_guess = 9

# estimates for the standard deviation
red_std_guess = 2
blue_std_guess = 1.7

These parameter estimates produce bell curves that look like this:

These are bad estimates. Both means (the vertical dotted lines) look far off any kind of "middle" for sensible groups of points, for instance. We want to improve these estimates.
The next step (step 2) is to compute the likelihood of each data point appearing under the current parameter guesses:
likelihood_of_red = stats.norm(red_mean_guess, red_std_guess).pdf(both_colours)
likelihood_of_blue = stats.norm(blue_mean_guess, blue_std_guess).pdf(both_colours)

Here, we have simply put each data point into the probability density function for a normal distribution using our current guesses at the mean and standard deviation for red and blue. This tells us, for example, that with our current guesses the data point at 1.761 is much more likely to be red (0.189) than blue (0.00003). 
For each data point, we can turn these two likelihood values into weights (step 3) so that they sum to 1 as follows:
likelihood_total = likelihood_of_red + likelihood_of_blue

red_weight = likelihood_of_red / likelihood_total
blue_weight = likelihood_of_blue / likelihood_total

With our current estimates and our newly-computed weights, we can now compute new estimates for the mean and standard deviation of the red and blue groups (step 4).
We twice compute the mean and standard deviation using all data points, but with the different weightings: once for the red weights and once for the blue weights.
The key bit of intuition is that the greater the weight of a colour on a data point, the more the data point influences the next estimates for that colour's parameters. This has the effect of "pulling" the parameters in the right direction.
def estimate_mean(data, weight):
    """
    For each data point, multiply the point by the probability it
    was drawn from the colour's distribution (its "weight").

    Divide by the total weight: essentially, we're finding where 
    the weight is centred among our data points.
    """
    return np.sum(data * weight) / np.sum(weight)

def estimate_std(data, weight, mean):
    """
    For each data point, multiply the point's squared difference
    from a mean value by the probability it was drawn from
    that distribution (its "weight").

    Divide by the total weight: essentially, we're finding where 
    the weight is centred among the values for the difference of
    each data point from the mean.

    This is the estimate of the variance, take the positive square
    root to find the standard deviation.
    """
    variance = np.sum(weight * (data - mean)**2) / np.sum(weight)
    return np.sqrt(variance)

# new estimates for standard deviation
blue_std_guess = estimate_std(both_colours, blue_weight, blue_mean_guess)
red_std_guess = estimate_std(both_colours, red_weight, red_mean_guess)

# new estimates for mean
red_mean_guess = estimate_mean(both_colours, red_weight)
blue_mean_guess = estimate_mean(both_colours, blue_weight)

We have new estimates for the parameters. To improve them again, we can jump back to step 2 and repeat the process. We do this until the estimates converge, or after some number of iterations have been performed (step 5).
For our data, the first five iterations of this process look like this (recent iterations have stronger appearance):

We see that the means are already converging on some values, and the shapes of the curves (governed by the standard deviation) are also becoming more stable. 
If we continue for 20 iterations, we end up with the following:

The EM process has converged to the following values, which turn out to very close to the actual values (where we can see the colours - no hidden variables):
          | EM guess | Actual |  Delta
----------+----------+--------+-------
Red mean  |    2.910 |  2.802 |  0.108
Red std   |    0.854 |  0.871 | -0.017
Blue mean |    6.838 |  6.932 | -0.094
Blue std  |    2.227 |  2.195 |  0.032

In the code above you may have noticed that the new estimation for standard deviation was computed using the previous iteration's estimate for the mean. Ultimately it does not matter if we compute a new value for the mean first as we are just finding the (weighted) variance of values around some central point. We will still see the estimates for the parameters converge.
 what if we even dont know the number of normal distributions from which this is coming from? Here you have taken an example of k=2 distributions, can we also estimate k, and the k parameter sets? @stackit: I'm not sure there's a straightforward general way to compute the most likely value of k as part of the EM process in this case. The main issue is that we would need to start EM with estimates for each of parameters we want to find, and that entails that we need to know/estimate k before we begin. It is possible, however, to estimate the proportion of points belonging to a group via EM here. Maybe if we overestimate k, the proportion of all but two of the groups would drop to near zero. I haven't experimented with this, so I don't know how well it would work in practice. @AlexRiley Can you say a bit more about the formulas for computing the new mean and standard deviation estimates? @AlexRiley Thanks for the explanation. Why are the new standard deviation estimates calculated using the old guess of the mean? What if the new estimates of the mean are found first? @Lemon GoodDeeds Kaushal - apologies for my late reply to your questions. I've tried to edit the answer to address the points you've raised. I have also made all of the code used in this answer accessible in a notebook here  (which also includes more details explanations of some points I touched upon).
EM is an algorithm for maximizing a likelihood function when some of the variables in your model are unobserved (i.e. when you have latent variables).  
You might fairly ask, if we're just trying to maximize a function, why don't we just use the existing machinery for maximizing a function.  Well, if you try to maximize this by taking derivatives and setting them to zero, you find that in many cases the first-order conditions don't have a solution.  There's a chicken-and-egg problem in that to solve for your model parameters you need to know the distribution of your unobserved data; but the distribution of your unobserved data is a function of your model parameters.  
E-M tries to get around this by iteratively guessing a distribution for the unobserved data, then estimating the model parameters by maximizing something that is a lower bound on the actual likelihood function, and repeating until convergence:
The EM algorithm 
Start with guess for values of your model parameters
E-step:  For each datapoint that has missing values, use your model equation to solve for the distribution of the missing data given your current guess of the model parameters and given the observed data (note that you are solving for a distribution for each missing value, not for the expected value).  Now that we have a distribution for each missing value, we can calculate the expectation of the likelihood function with respect to the unobserved variables.  If our guess for the model parameter was correct, this expected likelihood will be the actual likelihood of our observed data; if the parameters were not correct, it will just be a lower bound.
M-step:  Now that we've got an expected likelihood function with no unobserved variables in it, maximize the function as you would in the fully observed case, to get a new estimate of your model parameters.
Repeat until convergence.
 I do not understand your E-step. Part of the problem is that as I am learning this stuff, I can't find people who use the same terminology. So what do you mean by model equation? I don't know what you mean by solving for a probability distribution?
Here is a straight-forward recipe to understand the Expectation Maximisation algorithm:
1- Read this EM tutorial paper by Do and Batzoglou.
2- You may have question marks in your head, have a look at the explanations on this maths stack exchange page.
3- Look at this code that I wrote in Python that explains the example in the EM tutorial paper of item 1:
Warning : The code may be messy/suboptimal, since I am not a Python developer. But it does the job.
import numpy as np
import math

#### E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* #### 

def get_mn_log_likelihood(obs,probs):
    """ Return the (log)likelihood of obs, given the probs"""
    # Multinomial Distribution Log PMF
    # ln (pdf)      =             multinomial coeff            *   product of probabilities
    # ln[f(x|n, p)] = [ln(n!) - (ln(x1!)+ln(x2!)+...+ln(xk!))] + [x1*ln(p1)+x2*ln(p2)+...+xk*ln(pk)]     

    multinomial_coeff_denom= 0
    prod_probs = 0
    for x in range(0,len(obs)): # loop through state counts in each observation
        multinomial_coeff_denom = multinomial_coeff_denom + math.log(math.factorial(obs[x]))
        prod_probs = prod_probs + obs[x]*math.log(probs[x])

    multinomial_coeff = math.log(math.factorial(sum(obs))) -  multinomial_coeff_denom
    likelihood = multinomial_coeff + prod_probs
    return likelihood

# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T
# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T
# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T
# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T
# 5th:  Coin A, {THHHTHHHTH}, 7H,3T
# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45

# represent the experiments
head_counts = np.array([5,9,8,4,7])
tail_counts = 10-head_counts
experiments = zip(head_counts,tail_counts)

# initialise the pA(heads) and pB(heads)
pA_heads = np.zeros(100); pA_heads[0] = 0.60
pB_heads = np.zeros(100); pB_heads[0] = 0.50

# E-M begins!
delta = 0.001  
j = 0 # iteration counter
improvement = float('inf')
while (improvement>delta):
    expectation_A = np.zeros((5,2), dtype=float) 
    expectation_B = np.zeros((5,2), dtype=float)
    for i in range(0,len(experiments)):
        e = experiments[i] # i'th experiment
        ll_A = get_mn_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) # loglikelihood of e given coin A
        ll_B = get_mn_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) # loglikelihood of e given coin B

        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of A proportional to likelihood of A 
        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of B proportional to likelihood of B                            

        expectation_A[i] = np.dot(weightA, e) 
        expectation_B[i] = np.dot(weightB, e)

    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); 
    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); 

    improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))
    j = j+1

 I find that your program will result in both A and B to 0.66, I also implement it using scala, also find that the result is 0.66, can you help check that ? Using a spreadsheet, I only find your 0.66 results if my initial guesses are equal. Otherwise, I can reproduce the output of the tutorial. @zjffdu, how many iterations does the EM run before returning you 0.66? If you initialise with equal values it may be getting stuck at a local maximum and you will see that the number of iterations is extremely low (since there is no improvement). You can also check out this slide by Andrew Ng and Harvard's course note
Technically the term "EM" is a bit underspecified, but I assume you refer to the Gaussian Mixture Modelling cluster analysis technique, that is an instance of the general EM principle.
Actually, EM cluster analysis is not a classifier. I know that some people consider clustering to be "unsupervised classification", but actually cluster analysis is something quite different.
The key difference, and the big misunderstanding classification people always have with cluster analysis is that: in cluster analaysis, there is no "correct solution". It is a knowledge discovery method, it is actually meant to find something new! This makes evaluation very tricky. It is often evaluated using a known classification as reference, but that is not always appropriate: the classification you have may or may not reflect what is in the data.
Let me give you an example: you have a large data set of customers, including gender data. A method that splits this data set into "male" and "female" is optimal when you compare it with the existing classes. In a "prediction" way of thinking this is good, as for new users you could now predict their gender. In a "knowledge discovery" way of thinking this is actually bad, because you wanted to discover some new structure in the data. A method that would e.g. split the data into elderly people and kids however would score as worse as it can get with respect to the male/female class. However, that would be an excellent clustering result (if the age wasn't given).
Now back to EM. Essentially it assumes that your data is composed of multiple multivariate normal distributions (note that this is a very strong assumption, in particular when you fix the number of clusters!). It then tries to find a local optimal model for this by alternatingly improving the model and the object assignment to the model.
For best results in a classification context, choose the number of clusters larger than the number of classes, or even apply the clustering to single classes only (to find out whether there is some structure within the class!).
Say you want to train a classifier to tell apart "cars", "bikes" and "trucks". There is little use in assuming the data to consist of exactly 3 normal distributions. However, you may assume that there is more than one type of cars (and trucks and bikes). So instead of training a classifier for these three classes, you cluster cars, trucks and bikes into 10 clusters each (or maybe 10 cars, 3 trucks and 3 bikes, whatever), then train a classifier to tell apart these 30 classes, and then merge the class result back to the original classes. You may also discover that there is one cluster that is particularly hard to classify, for example Trikes. They're somewhat cars, and somewhat bikes. Or delivery trucks, that are more like oversized cars than trucks.
 how is EM underspecified? There is more than one version of it. Technically, you can call Lloyd style k-means "EM", too. You need to specify what model you use.
Other answers being good, i will try to provide another perspective and tackle the intuitive part of the question.
EM (Expectation-Maximization) algorithm is a variant of a class of iterative algorithms using duality
Excerpt (emphasis mine):

In mathematics, a duality, generally speaking, translates concepts,
  theorems or mathematical structures into other concepts, theorems or
  structures, in a one-to-one fashion, often (but not always) by means
  of an involution operation: if the dual of A is B, then the dual of B
  is A. Such involutions sometimes have fixed points, so that the dual
  of A is A itself

Usually a dual B of an object A is related to A in some way that preserves some symmetry or compatibility. For example AB = const
Examples of iterative algorithms, employing duality (in the previous sense) are:

Euclidean algorithm for Greatest Common Divisor, and its variants
Gram–Schmidt Vector Basis algorithm and variants
Arithmetic Mean - Geometric Mean Inequality, and its variants
Expectation-Maximization algorithm and its variants (see also here for an information-geometric view)
(.. other similar algorithms..)

In a similar fashion, the EM algorithm can also be seen as two dual maximization steps:

..[EM] is seen as maximizing a joint function of the parameters and of
  the distribution over the unobserved variables.. The E-step maximizes
  this function with respect to the distribution over the unobserved
  variables; the M-step with respect to the parameters..

In an iterative algorithm using duality there is the explicit (or implicit) assumption of an equilibrium (or fixed) point of convergence (for EM this is proved using Jensen's inequality)
So the outline of such algorithms is:

E-like step: Find best solution x with respect to given y being held constant.
M-like step (dual): Find best solution y with respect to x (as computed in previous step) being held constant.
Criterion of Termination/Convergence step: Repeat steps 1, 2 with the updated values of x,y until convergence (or specified number of iterations is reached)

Note that when such an algorithm converges to a (global) optimum, it has found a configuration which is best in both senses (i.e in both the x domain/parameters and the y domain/parameters). However the algorithm can just find a local optimum and not the global optimum.
i would say this is the intuitive description of the outline of the algorithm
For the statistical arguments and applications, other answers have given good explanations (check also references in this answer)

The accepted answer references the Chuong EM Paper, which does a decent job explaining EM. There is also a youtube video that explains the paper in more detail. 
To recap, here is the scenario:
1st:  {H,T,T,T,H,H,T,H,T,H} 5 Heads, 5 Tails; Did coin A or B generate me?
2nd:  {H,H,H,H,T,H,H,H,H,H} 9 Heads, 1 Tails
3rd:  {H,T,H,H,H,H,H,T,H,H} 8 Heads, 2 Tails
4th:  {H,T,H,T,T,T,H,H,T,T} 4 Heads, 6 Tails
5th:  {T,H,H,H,T,H,H,H,T,H} 7 Heads, 3 Tails

Two possible coins, A & B are used to generate these distributions.
A & B have an unknown parameter: their bias towards heads.

We don't know the biases, but we can simply start with a guess: A=60% heads, B=50% heads.

In the case of the first trial's question, intuitively we'd think B generated it since the proportion of heads matches B's bias very well... but that value was just a guess, so we can't be sure.  
With that in mind, I like to think of the EM solution like this:

Each trial of flips gets to 'vote' on which coin it likes the most


This is based on how well each coin fits its distribution  
OR, from the point of view of the coin, there is high expectation of seeing this trial relative to the other coin (based on log likelihoods).

Depending on how much each trial likes each coin, it can update the guess of that coin's parameter (bias).  


The more a trial likes a coin, the more it gets to update the coin's bias to reflect its own!  
Essentially the coin's biases are updated by combining these weighted updates across all trials, a process called (maximazation), which refers to trying to get the best guesses for each coin's bias given a set of trials. 


This may be an oversimplification (or even fundamentally wrong on some levels), but I hope this helps on an intuitive level!

EM is used to maximize the likelihood of a model Q with latent variables Z.
It's an iterative optimization.
theta <- initial guess for hidden parameters
while not converged:
    #e-step
    Q(theta'|theta) = E[log L(theta|Z)]
    #m-step
    theta <- argmax_theta' Q(theta'|theta)

e-step:
given current estimation of Z calculate the expected loglikelihood function 
m-step:
find theta which maximizes this Q
GMM Example:
e-step: estimate label assignments for each datapoint given the current gmm-parameter estimation
m-step: maximize a new theta given the new label assigments
K-means is also an EM algorithm and there is a lot of explaining animations on K-means.

Using the same article by Do and Batzoglou cited in Zhubarb's answer, I implemented EM for that problem in Java. The comments to his answer show that the algorithm gets stuck at a local optimum, which also occurs with my implementation if the parameters thetaA and thetaB are the same.
Below is the standard output of my code, showing the convergence of the parameters.
thetaA = 0.71301, thetaB = 0.58134
thetaA = 0.74529, thetaB = 0.56926
thetaA = 0.76810, thetaB = 0.54954
thetaA = 0.78316, thetaB = 0.53462
thetaA = 0.79106, thetaB = 0.52628
thetaA = 0.79453, thetaB = 0.52239
thetaA = 0.79593, thetaB = 0.52073
thetaA = 0.79647, thetaB = 0.52005
thetaA = 0.79667, thetaB = 0.51977
thetaA = 0.79674, thetaB = 0.51966
thetaA = 0.79677, thetaB = 0.51961
thetaA = 0.79678, thetaB = 0.51960
thetaA = 0.79679, thetaB = 0.51959
Final result:
thetaA = 0.79678, thetaB = 0.51960

Below is my Java implementation of EM to solve the problem in (Do and Batzoglou, 2008). The core part of the implementation is the loop to run EM until the parameters converge.
private Parameters _parameters;

public Parameters run()
{
    while (true)
    {
        expectation();

        Parameters estimatedParameters = maximization();

        if (_parameters.converged(estimatedParameters)) {
            break;
        }

        _parameters = estimatedParameters;
    }

    return _parameters;
}

Below is the entire code.
import java.util.*;

/*****************************************************************************
This class encapsulates the parameters of the problem. For this problem posed
in the article by (Do and Batzoglou, 2008), the parameters are thetaA and
thetaB, the probability of a coin coming up heads for the two coins A and B,
respectively.
*****************************************************************************/
class Parameters
{
    double _thetaA = 0.0; // Probability of heads for coin A.
    double _thetaB = 0.0; // Probability of heads for coin B.

    double _delta = 0.00001;

    public Parameters(double thetaA, double thetaB)
    {
        _thetaA = thetaA;
        _thetaB = thetaB;
    }

    /*************************************************************************
    Returns true if this parameter is close enough to another parameter
    (typically the estimated parameter coming from the maximization step).
    *************************************************************************/
    public boolean converged(Parameters other)
    {
        if (Math.abs(_thetaA - other._thetaA) < _delta &&
            Math.abs(_thetaB - other._thetaB) < _delta)
        {
            return true;
        }

        return false;
    }

    public double getThetaA()
    {
        return _thetaA;
    }

    public double getThetaB()
    {
        return _thetaB;
    }

    public String toString()
    {
        return String.format("thetaA = %.5f, thetaB = %.5f", _thetaA, _thetaB);
    }

}


/*****************************************************************************
This class encapsulates an observation, that is the number of heads
and tails in a trial. The observation can be either (1) one of the
experimental observations, or (2) an estimated observation resulting from
the expectation step.
*****************************************************************************/
class Observation
{
    double _numHeads = 0;
    double _numTails = 0;

    public Observation(String s)
    {
        for (int i = 0; i < s.length(); i++)
        {
            char c = s.charAt(i);

            if (c == 'H')
            {
                _numHeads++;
            }
            else if (c == 'T')
            {
                _numTails++;
            }
            else
            {
                throw new RuntimeException("Unknown character: " + c);
            }
        }
    }

    public Observation(double numHeads, double numTails)
    {
        _numHeads = numHeads;
        _numTails = numTails;
    }

    public double getNumHeads()
    {
        return _numHeads;
    }

    public double getNumTails()
    {
        return _numTails;
    }

    public String toString()
    {
        return String.format("heads: %.1f, tails: %.1f", _numHeads, _numTails);
    }

}

/*****************************************************************************
This class runs expectation-maximization for the problem posed by the article
from (Do and Batzoglou, 2008).
*****************************************************************************/
public class EM
{
    // Current estimated parameters.
    private Parameters _parameters;

    // Observations from the trials. These observations are set once.
    private final List<Observation> _observations;

    // Estimated observations per coin. These observations are the output
    // of the expectation step.
    private List<Observation> _expectedObservationsForCoinA;
    private List<Observation> _expectedObservationsForCoinB;

    private static java.io.PrintStream o = System.out;

    /*************************************************************************
    Principal constructor.
    @param observations The observations from the trial.
    @param parameters The initial guessed parameters.
    *************************************************************************/
    public EM(List<Observation> observations, Parameters parameters)
    {
        _observations = observations;
        _parameters = parameters;
    }

    /*************************************************************************
    Run EM until parameters converge.
    *************************************************************************/
    public Parameters run()
    {

        while (true)
        {
            expectation();

            Parameters estimatedParameters = maximization();

            o.printf("%s\n", estimatedParameters);

            if (_parameters.converged(estimatedParameters)) {
                break;
            }

            _parameters = estimatedParameters;
        }

        return _parameters;

    }

    /*************************************************************************
    Given the observations and current estimated parameters, compute new
    estimated completions (distribution over the classes) and observations.
    *************************************************************************/
    private void expectation()
    {

        _expectedObservationsForCoinA = new ArrayList<Observation>();
        _expectedObservationsForCoinB = new ArrayList<Observation>();

        for (Observation observation : _observations)
        {
            int numHeads = (int)observation.getNumHeads();
            int numTails = (int)observation.getNumTails();

            double probabilityOfObservationForCoinA=
                binomialProbability(10, numHeads, _parameters.getThetaA());

            double probabilityOfObservationForCoinB=
                binomialProbability(10, numHeads, _parameters.getThetaB());

            double normalizer = probabilityOfObservationForCoinA +
                                probabilityOfObservationForCoinB;

            // Compute the completions for coin A and B (i.e. the probability
            // distribution of the two classes, summed to 1.0).

            double completionCoinA = probabilityOfObservationForCoinA /
                                     normalizer;
            double completionCoinB = probabilityOfObservationForCoinB /
                                     normalizer;

            // Compute new expected observations for the two coins.

            Observation expectedObservationForCoinA =
                new Observation(numHeads * completionCoinA,
                                numTails * completionCoinA);

            Observation expectedObservationForCoinB =
                new Observation(numHeads * completionCoinB,
                                numTails * completionCoinB);

            _expectedObservationsForCoinA.add(expectedObservationForCoinA);
            _expectedObservationsForCoinB.add(expectedObservationForCoinB);
        }
    }

    /*************************************************************************
    Given new estimated observations, compute new estimated parameters.
    *************************************************************************/
    private Parameters maximization()
    {

        double sumCoinAHeads = 0.0;
        double sumCoinATails = 0.0;
        double sumCoinBHeads = 0.0;
        double sumCoinBTails = 0.0;

        for (Observation observation : _expectedObservationsForCoinA)
        {
            sumCoinAHeads += observation.getNumHeads();
            sumCoinATails += observation.getNumTails();
        }

        for (Observation observation : _expectedObservationsForCoinB)
        {
            sumCoinBHeads += observation.getNumHeads();
            sumCoinBTails += observation.getNumTails();
        }

        return new Parameters(sumCoinAHeads / (sumCoinAHeads + sumCoinATails),
                              sumCoinBHeads / (sumCoinBHeads + sumCoinBTails));

        //o.printf("parameters: %s\n", _parameters);

    }

    /*************************************************************************
    Since the coin-toss experiment posed in this article is a Bernoulli trial,
    use a binomial probability Pr(X=k; n,p) = (n choose k) * p^k * (1-p)^(n-k).
    *************************************************************************/
    private static double binomialProbability(int n, int k, double p)
    {
        double q = 1.0 - p;
        return nChooseK(n, k) * Math.pow(p, k) * Math.pow(q, n-k);
    }

    private static long nChooseK(int n, int k)
    {
        long numerator = 1;

        for (int i = 0; i < k; i++)
        {
            numerator = numerator * n;
            n--;
        }

        long denominator = factorial(k);

        return (long)(numerator / denominator);
    }

    private static long factorial(int n)
    {
        long result = 1;
        for (; n >0; n--)
        {
            result = result * n;
        }

        return result;
    }

    /*************************************************************************
    Entry point into the program.
    *************************************************************************/
    public static void main(String argv[])
    {
        // Create the observations and initial parameter guess
        // from the (Do and Batzoglou, 2008) article.

        List<Observation> observations = new ArrayList<Observation>();
        observations.add(new Observation("HTTTHHTHTH"));
        observations.add(new Observation("HHHHTHHHHH"));
        observations.add(new Observation("HTHHHHHTHH"));
        observations.add(new Observation("HTHTTTHHTT"));
        observations.add(new Observation("THHHTHHHTH"));

        Parameters initialParameters = new Parameters(0.6, 0.5);

        EM em = new EM(observations, initialParameters);

        Parameters finalParameters = em.run();

        o.printf("Final result:\n%s\n", finalParameters);
    }
}

Why is the F-Measure a harmonic mean and not an arithmetic mean of the Precision and Recall measures? 
When we calculate the F-Measure considering both Precision and Recall, we take the harmonic mean of the two measures instead of a simple arithmetic mean. 
What is the intuitive reason behind taking the harmonic mean and not a simple average?
 The intuition is to balance precision and recall (usually the best measurement, but in some case you want to maximize precision or recall, which is a different story). You cannot get a high f-score if either one is very low. cse.unsw.edu.au/~teachadmin/info/harmonic3.html This is a good resource to understanding HM Fix the link above: di.unipi.it/~bozzo/The%20Harmonic%20Mean.htm or the original @archive.org
Here we already have some elaborate answers but I thought some more information about it would be helpful for some guys who want to delve deeper(especially why F measure).  
According to the theory of measurement the composite measure should satisfy the following 6 definitions: 

Connectedness(two pairs can be ordered) and transitivity(if e1 >= e2 and e2 >= e3 then e1 >= e3)
Independence: two components contribute their effects independently to the effectiveness. 
Thomsen condition: Given that at a constant recall (precision) we find a difference in effectiveness for two values of precision (recall) then this difference cannot be removed or reversed by changing the constant value.
Restricted solvability. 
Each component is essential: Variation in one while leaving the other constant gives a variation in effectiveness.
Archimedean property for each component. It merely ensures that the intervals on a component are comparable.

We can then derive and get the function of the effectiveness: 

And normally we don't use the effectiveness but the much simper F score because:

Now that we have the general formula of F measure: 

where we can place more emphesis on recall or precision by setting beta, because beta is defined as follows: 

If we weight recall more important than precision(all relevant are selected) we can set beta as 2 and we get the F2 measure. And if we do the reverse and weight precision higher than recall(as much selected elements are relevant as possible, for instance in some grammar error correction scenarios like CoNLL) we just set beta as 0.5 and get the F0.5 measure. And obviously we can set beta as 1 to get the mostly used F1 measure(harmonic mean of precision and recall). 
I think to some extent I have already answered why we do not use the arithmetic mean.
References:
1. https://en.wikipedia.org/wiki/F1_score
2. The truth of the F-measure
3. Information retrival

To explain, consider for example, what the average of 30mph and 40mph is? if you drive for 1 hour at each speed, the average speed over the 2 hours is indeed the arithmetic average, 35mph.
However if you drive for the same distance at each speed -- say 10 miles -- then the average speed over 20 miles is the harmonic mean of 30 and 40, about 34.3mph.
The reason is that for the average to be valid, you really need the values to be in the same scaled units. Miles per hour need to be compared over the same number of hours; to compare over the same number of miles you need to average hours per mile instead, which is exactly what the harmonic mean does.
Precision and recall both have true positives in the numerator, and different denominators. To average them it really only makes sense to average their reciprocals, thus the harmonic mean.
 Thanks, that is a good argument on why this is supported from theory; my answer was more on the pragmatic side.
Because it punishes extreme values more.
Consider a trivial method (e.g. always returning class A). There are infinite data elements of class B, and a single element of class A:
Precision: 0.0
Recall:    1.0

When taking the arithmetic mean, it would have 50% correct. Despite being the worst possible outcome! With the harmonic mean, the F1-measure is 0.
Arithmetic mean: 0.5
Harmonic mean:   0.0

In other words, to have a high F1, you need to both have a high precision and recall.
 When the recall is 0.0 the precision has to be greater than 0.0 right? But I get the point in your example. Nicely explained - Thanks. In your example, precision for class A is 0.5 instead of 0 and recall of class A is 1; precision for class B is 0 and recall of class B is 0 as we'll. I assume your balanced class means the true labels are A and B; each applies to 50% of data. Let's make infinite elements of class B, and a single element of class A. It doesn't change the math behind F1. It is not just a heuristic to select more balance. Harmonic mean is there only way that makes sense given the units of these ratios. Mean wouldn't have a meaning in comparison Where does it say "heuristic", and where does your comment differ from my answer? But: F-measure is a heuristic in that it assumes precision and recall are equally important. That is why the beta term needs to be chosen - heuristically, one usually uses beta=1.
The above answers are well explained. This is just for a quick reference to understand the nature of the arithmetic mean and the harmonic mean with plots. As you can see from the plot, consider the X axis and Y axis as precision and recall, and the Z axis as the F1 Score. So, from the plot of the harmonic mean, both the precision and recall should contribute evenly for the F1 score to rise up unlike the Arithmetic mean.
This is for the arithmetic mean.

This is for the Harmonic mean.

 Please use formatting tools to properly edit and format your answer. Image should be displayed here , its not a hyperlink.
The harmonic mean is the equivalent of the arithmetic mean for reciprocals of quantities that should be averaged by the arithmetic mean. More precisely, with the harmonic mean, you transform all your numbers to the "averageable" form (by taking the reciprocal), you take their arithmetic mean and then transform the result back to the original representation (by taking the reciprocal again).
Precision and the recall are "naturally" reciprocals because their numerator is the same and their denominators are different. Fractions are more sensible to average by arithmetic mean when they have the same denominator.
For more intuition, suppose that we keep the number of true positive items constant. Then by taking the harmonic mean of the precision and the recall, you implicitly take the arithmetic mean of the false positives and the false negatives. It basically means that false positives and false negatives are equally important to you when the true positives stay the same. If an algorithm has N more false positive items but N less false negatives (while having the same true positives), the F-measure stays the same.
In other words, the F-measure is suitable when:

mistakes are equally bad, whether they are false positives or false negatives
the number of mistakes is measured relative to the number of true positives
true negatives are uninteresting

Point 1 may or may not be true, there are weighted variants of the F-measure that can be used if this assumption isn't true. Point 2 is quite natural since we can expect the results to scale if we just classify more and more points. The relative numbers should stay the same. 
Point 3 is quite interesting. In many applications negatives are the natural default and it may even be hard or arbitrary to specify what really counts as a true negative. For example a fire alarm is having a true negative event every second, every nanosecond, every time a Planck time has passed etc. Even a piece of rock has these true negative fire-detection events all the time.
Or in a face detection case, most of the time you "correctly don't return" billions of possible areas in the image but this is not interesting. The interesting cases are when you do return a proposed detection or when you should return it.
By contrast the classification accuracy cares equally about true positives and true negatives and is more suitable if the total number of samples (classification events) is well-defined and rather small.
 Very well explained!1D Number Array Clustering 
So let's say I have an array like this:
[1,1,2,3,10,11,13,67,71]

Is there a convenient way to partition the array into something like this?
[[1,1,2,3],[10,11,13],[67,71]]

I looked through similar questions yet most people suggested using k-means to cluster points, like scipy, which is quite confusing to use for a beginner like me. Also I think that k-means is more suitable for two or more dimensional clustering right? Are there any ways to partition an array of N numbers to many partitions/clustering depending on the numbers?
Some people also suggest rigid range partitioning, but it doesn't always render the results as
 expected

Don't use multidimensional clustering algorithms for a one-dimensional problem. A single dimension is much more special than you naively think, because you can actually sort it, which makes things a lot easier.
In fact, it is usually not even called clustering, but e.g. segmentation or natural breaks optimization.
You might want to look at Jenks Natural Breaks Optimization and similar statistical methods. Kernel Density Estimation is also a good method to look at, with a strong statistical background. Local minima in density are be good places to split the data into clusters, with statistical reasons to do so. KDE is maybe the most sound method for clustering 1-dimensional data.
With KDE, it again becomes obvious that 1-dimensional data is much more well behaved. In 1D, you have local minima; but in 2D you may have saddle points and such "maybe" splitting points. See this Wikipedia illustration of a saddle point, as how such a point may or may not be appropriate for splitting clusters.
See this answer for an example how to do this in Python (green markers are the cluster modes; red markers a points where the data is cut; the y axis is a log-likelihood of the density):

 Implementation here: macwright.org/2013/02/18/literate-jenks.html Could you update your answer with why meanshift or dbscan may or may not be good approaches to clustering 1D? See scikit-learn.org/stable/modules/clustering.html Essentially, both are very naive approximations to Kernel Density Estimation. Mean-Shift is a mode-seeking approach for multivariate KDE, and DBSCAN is using the most primitive KDE (box kernel) to define what is dense and what is not. There is 0 benefit to use them on 1-dimensional data. Ckmeans.1d.dp (k-means adapted for dimensional clustering) is worth a look however. See journal.r-project.org/archive/2011-2/… @skoush that is a slower k-means variant that yields the global optimum (in 1d only). But if the SSQ k-means objective doesn't solve your problem it does not matter if you find a 0.1% better (by SSQ) k-means solution than with the faster standard algorithm.
You may look for discretize algorithms. 1D discretization  problem is a lot similar to what you are asking. They decide cut-off points, according to frequency, binning strategy etc.
weka uses following algorithms in its , discretization process.

weka.filters.supervised.attribute.Discretize
uses either Fayyad & Irani's MDL method or Kononeko's MDL criterion
weka.filters.unsupervised.attribute.Discretize
uses simple binning

What is the difference between Gradient Descent and Newton's Gradient Descent? 
I understand what Gradient Descent does. Basically it tries to move towards the local optimal solution by slowly moving down the curve. I am trying to understand what is the actual difference between the plan gradient descent and the newton's method?
From Wikipedia, I read this short line "Newton's method uses curvature information to take a more direct route." What does this intuitively mean?
 curvature relates to how Newton's method uses the fuction's second order derivative. Gradient descent is typically first order. Watch this lecture from start to finish: youtube.com/… Very similar, also with a good answer: math.stackexchange.com/q/1085436/407385
At a local minimum (or maximum) x, the derivative of the target function f vanishes: f'(x) = 0 (assuming sufficient smoothness of f).
Gradient descent tries to find such a minimum x by using information from the first derivative of f: It simply follows the steepest descent from the current point. This is like rolling a ball down the graph of f until it comes to rest (while neglecting inertia).
Newton's method tries to find a point x satisfying f'(x) = 0 by approximating f' with a linear function g and then solving for the root of that function explicitely (this is called Newton's root-finding method). The root of g is not necessarily the root of f', but it is under many circumstances a good guess (the Wikipedia article on Newton's method for root finding has more information on convergence criteria). While approximating f', Newton's method makes use of f'' (the curvature of f). This means it has higher requirements on the smoothness of f, but it also means that (by using more information) it often converges faster.
 I always see mentions of choosing the 'steepest descent'. What does that mean? Is that the most negative number of f'(x)? @Chowza: If your domain is multi-dimensional, e.g. if f maps 2D points to real numbers, then the gradient of f at any point is not a scalar number but a vector. The reason is that the "steepness" of f at that point depends on the direction that you're looking in. It's like standing on a mountain top: If you look north the mountain may drop off very sharply, but to the other sides it may be less steep. Choosing the steepest descent hence means choosing that direction which causes the greatest change in your target function.
Put simply, gradient descent you just take a small step towards where you think the zero is and then recalculate; Newton's method, you go all the way there.
 Is "all the way" true for a non-quadratic function? Yes, for non quadratic functions you are just approximating the first derivative with a line.  This is a bit hand wavey but I think it's fine for intuition. Ok, I agree. All the way to "where you think the zero is" is undoubtedly correct. If the main difference as you say is "small steps" vs "all the way", could you elaborate on how the size of the "small step" is determined? @MrPurple it's not very well defined, small enough that the gradient doesn't change too much (so you don't keep zigzagging) but large enough that you make progress.  A lot of research is around how to optimize this adaptively.  For intuition, think like on the order of .1% of the x value.
Edit 2017: The original link is dead -
 but the way back machine still got it :) https://web.archive.org/web/20151122203025/http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf
this power point the main ideas are explained simply http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf
I hope this help :)
 The link is down
If you simply compare Gradient Descent and Newton's method, the purpose of the two methods are different.
Gradient Descent is used to find(approximate) local maxima or minima (x to make min f(x) or max f(x)). While Newton's method is to find(approximate) the root of a function, i.e. x to make f(x) = 0
In this sense, they are used to solve different problems. However, Newton's method can also be used in the context of optimization (the realm that GD is solving). Because finding maxima or minima can be approached by finding f'(x) = 0 which is exactly Newton's method is used for.
In conclusion, two methods can be used in optimization: 1)GD and 2)find x so f'(x)=0
and Newton's method is just a way to solve that second problem.
How to approach a number guessing game (with a twist) algorithm? 
I am learning programming (Python and algorithms) and was trying to work on a project that I find interesting. I have created a few basic Python scripts, but I’m not sure how to approach a solution to a game I am trying to build.
Here’s how the game will work:
Users will be given items with a value. For example,
Apple = 1
Pears = 2
Oranges  = 3

They will then get a chance to choose any combo of them they like (i.e. 100 apples, 20 pears, and one orange). The only output the computer gets is the total value (in this example, it's currently $143). The computer will try to guess what they have. Which obviously it won’t be able to get correctly the first turn.
         Value    quantity(day1)    value(day1)
Apple      1        100                100
Pears      2         20                 40
Orange     3          1                  3
Total               121                143

The next turn the user can modify their numbers but no more than 5% of the total quantity (or some other percent we may chose. I’ll use 5% for example.). The prices of fruit can change(at random) so the total value may change based on that also (for simplicity I am not changing fruit prices in this example). Using the above example, on day 2 of the game, the user returns a value of $152 and $164 on day 3. Here's an example:
Quantity (day2)   %change (day2)    Value (day2)   Quantity (day3)   %change (day3)   Value(day3)
 104                                 104            106                                106
  21                                  42             23                                 46
   2                                   6              4                                 12
 127               4.96%             152            133               4.72%            164

*(I hope the tables show up right, I had to manually space them so hopefully it's not just doing it on my screen, if it doesn't work let me know and I'll try to upload a screenshot.)
I am trying to see if I can figure out what the quantities are over time (assuming the user will have the patience to keep entering numbers). I know right now my only restriction is the total value cannot be more than 5% so I cannot be within 5% accuracy right now so the user will be entering it forever.
What I have done so far
Here’s my solution so far (not much). Basically, I take all the values and figure out all the possible combinations of them (I am done this part). Then I take all the possible combos and put them in a database as a dictionary (so for example for $143, there could be a dictionary entry {apple:143, Pears:0, Oranges :0}..all the way to {apple:0, Pears:1, Oranges :47}. I do this each time I get a new number so I have a list of all possibilities.
Here’s where I’m stuck. In using the rules above, how can I figure out the best possible solution? I think I’ll need a fitness function that automatically compares the two days data and removes any possibilities that have more than 5% variance of the previous days data.
Questions:
So my question with user changing the total and me having a list of all the probabilities, how should I approach this? What do I need to learn? Is there any algorithms out there or theories that I can use that are applicable? Or, to help me understand my mistake, can you suggest what rules I can add to make this goal feasible (if it's not in its current state. I was thinking adding more fruits and saying they must pick at least 3, etc..)?  Also, I only have a vague understanding of genetic algorithms, but I thought I could use them here, if is there something I can use?
I'm very very eager to learn so any advice or tips would be greatly appreciated (just please don't tell me this game is impossible).
UPDATE: Getting feedback that this is hard to solve. So I thought I'd add another condition to the game that won't interfere with what the player is doing (game stays the same for them) but everyday the value of the fruits change price (randomly). Would that make it easier to solve? Because within a 5% movement and certain fruit value changes, only a few combinations are probable over time.
Day 1, anything is possible and getting a close enough range is almost impossible, but as the prices of fruits change and the user can only choose a 5% change, then shouldn't (over time) the range be narrow and narrow. In the above example, if prices are volatile enough I think I could brute force a solution that gave me a range to guess in, but I'm trying to figure out if there's a more elegant solution or other solutions to keep narrowing this range over time.
UPDATE2: After reading and asking around, I believe this is a hidden Markov/Viterbi problem that tracks the changes in fruit prices as well as total sum (weighting the last data point the heaviest). I'm not sure how to apply the relationship though. I think this is the case and could be wrong but at the least I'm starting to suspect this is a some type of machine learning problem.
Update 3: I am created a test case (with smaller numbers) and a generator to help automate the user generated data and I am trying to create a graph from it to see what's more likely.
Here's the code, along with the total values and comments on what the users actually fruit quantities are.
#!/usr/bin/env python
import itertools

# Fruit price data
fruitPriceDay1 = {'Apple':1, 'Pears':2, 'Oranges':3}
fruitPriceDay2 = {'Apple':2, 'Pears':3, 'Oranges':4}
fruitPriceDay3 = {'Apple':2, 'Pears':4, 'Oranges':5}

# Generate possibilities for testing (warning...will not scale with large numbers)
def possibilityGenerator(target_sum, apple, pears, oranges):
    allDayPossible = {}
    counter = 1
    apple_range = range(0, target_sum + 1, apple)
    pears_range = range(0, target_sum + 1, pears)
    oranges_range = range(0, target_sum + 1, oranges)
    for i, j, k in itertools.product(apple_range, pears_range, oranges_range):
        if i + j + k == target_sum:
            currentPossible = {}

            #print counter
            #print 'Apple', ':', i/apple, ',', 'Pears', ':', j/pears, ',', 'Oranges', ':', k/oranges
            currentPossible['apple'] = i/apple
            currentPossible['pears'] = j/pears
            currentPossible['oranges'] = k/oranges

            #print currentPossible
            allDayPossible[counter] = currentPossible
            counter = counter +1
    return allDayPossible

# Total sum being returned by user for value of fruits
totalSumDay1=26 # Computer does not know this but users quantities are apple: 20, pears 3, oranges 0 at the current prices of the day
totalSumDay2=51 # Computer does not know this but users quantities are apple: 21, pears 3, oranges 0 at the current prices of the day
totalSumDay3=61 # Computer does not know this but users quantities are apple: 20, pears 4, oranges 1 at the current prices of the day
graph = {}
graph['day1'] = possibilityGenerator(totalSumDay1, fruitPriceDay1['Apple'], fruitPriceDay1['Pears'], fruitPriceDay1['Oranges'] )
graph['day2'] = possibilityGenerator(totalSumDay2, fruitPriceDay2['Apple'], fruitPriceDay2['Pears'], fruitPriceDay2['Oranges'] )
graph['day3'] = possibilityGenerator(totalSumDay3, fruitPriceDay3['Apple'], fruitPriceDay3['Pears'], fruitPriceDay3['Oranges'] )

# Sample of dict = 1 : {'oranges': 0, 'apple': 0, 'pears': 0}..70 : {'oranges': 8, 'apple': 26, 'pears': 13}
print graph

 +1 for an excellent question which is clear and well formatted. Thank you so much SpeedBirdNine.  First time someone's said that to me here.  Usually I get the exact opposite :-) You might want to try this on math.stackexchange.com @Mysticial the points don't mean much to me but useful to draw more bright minds into attempting this problem.  I think this problem's solution will benefit everyone in the community so I see it as totally worth it, if we all think of a solution and those reading end up with something useful in process. In the age of big data analytics and cheap cloud computing, I think these types of problems will grow and grow.  Plus selfishly, I'm just interested in creating a program that friends/family will think is 'magic'! stats.stackexchange.com is more relevant than math.
We'll combine graph-theory and probability:
On the 1st day, build a set of all feasible solutions. Lets denote the solutions set as A1={a1(1), a1(2),...,a1(n)}.
On the second day you can again build the solutions set A2.
Now, for each element in A2, you'll need to check if it can be reached from each element of A1 (given x% tolerance). If so - connect A2(n) to A1(m). If it can't be reached from any node in A1(m) - you can delete this node.
Basically we are building a connected directed acyclic graph.
All paths in the graph are equally likely. You can find an exact solution only when there is a single edge from Am to Am+1 (from a node in Am to a node in Am+1).
Sure, some nodes appear in more paths than other nodes. The probability for each node can be directly deduced based on the number of paths that contains this node.
By assigning a weight to each node, which equals to the number of paths that leads to this node, there is no need to keep all history, but only the previous day.
Also, have a look at non-negative-values linear diphantine equations - A question I asked a while ago. The accepted answer is a great way to enumarte all combos in each step.
 There is an extra reduction in the size of the sets possible. After your A1+A2 steps, if you add a next set of possible configurations A3, you can trim the sets A2 and A3 based on the "not reacheable within 5%" criterium, but you can also "cascade" this back to the A1-A2 junction. As a net result, the set A1 can only become smaller. But the set An+1 will "probably" be larger than the set An. But I don't think the aim of the gaim is to only guess the right candidate from the A1 set ... @Lostsoul: If you find my answer unclear - please let me know and I'll try to explain better. @LiorKogan I understand your solution but have been caught by trying to implement it successfully.  I understand your logic and it makes sense, but I'm starting to think since all numbers have an equal probability of being successful, how can it differentiate the correct solution out of so many possibilities.   I ended up looking into to the hidden markov model, which seems correct but it only weights the last successful match(not A1,A2,...). I'm not 100% sure still but I'm starting to think I'll need to use the hidden markov model to assign probabilities to the correct answer , then use a graph to navigate and try to find the best current answer(based on the history of total sums). What do you think? I don't suggest that all numbers are equally likely, but that all paths in the graph are equally likely. Some numbers (nodes) appear in more paths, hence they are more probable. The probability for each node can be calculated by dividing the number of paths (from t0) that goes through this node, by the total number of paths. *** Which part did you find hard to implement?" *** I wouldn't build a HMM here. There is no need to predict anything. We only need to infer our data and assign probabilities to each state.
This problem is impossible to solve.
Let's say that you know exactly for what ratio number of items was increased, not just what is the maximum ratio for this.
A user has N fruits and you have D days of guessing.
In each day you get N new variables and then you have in total D*N variables.
For each day you can generate only two equations. One equation is the sum of  n_item*price and other is based on a known ratio. In total you have at most 2*D equations if they are all independent.
2*D < N*D for all N > 2
 Thanks Ralu, someone on the math site said something simlair so I updated the question to add a new condition(without changing the process for the user).  What if the value of the fruit changes randomly everyday(I would have no control over it, since I could easily put extreme values to isolate possibilities)?  If fruits prices are changing, wouldn't certain possibilities be less likely and over time the possibilities actually decrease to something more accurate? There is no such thing as less an more probable. It is just about possible/impossible. And yes you can probably throw out some solutions if you know that they are integer solutions, but that is all. Think about that user begin whit 1000000, 1000000 and 1000000 and then it can change each value by +/- 50000 each time. So it does not matter if you limit difference for each step. I agree with you and thank you for explaining.  I'm just thinking there's two things I need to solve to get to the answer.  One is to limit the possibilities. If the total value is 5 and the price of apples jumped to $100 then clearly the user has no apples so I can eliminate that and so on..until maybe I have a range. Once I have that range then I think a simple guessing game structure can occur, the point of this question though is not to get the most accurate answer(which would be nice) but realistic how to get the most narrow range.
Disclaimer: I changed my answer dramatically after temporarily deleting my answer and re-reading the question carefully as I misread some critical parts of the question. While still referencing similar topics and algorithms, the answer was greatly improved after I attempted to solve some of the problem in C# myself.
Hollywood version

The problem is a Dynamic constraint satisfaction problem (DCSP), a variation on Constraint satisfaction problems (CSP.)
Use Monte Carlo to find potential solutions for a given day if values and quantity ranges are not tiny. Otherwise, use brute force to find every potential solutions.
Use Constraint Recording (related to DCSP), applied in cascade to previous days to restrict the potential solution set.
Cross your fingers, aim and shoot (Guess), based on probability.
(Optional) Bruce Willis wins.

Original version
First, I would like to state what I see two main problems here:

The sheer number of possible solutions. Knowing only the number of items and the total value, lets say 3 and 143 for example, will yield a lot of possible solutions. Plus, it is not easy to have an algorithm picking valid solution without inevitably trying invalid solutions (total not equal to 143.)
When possible solutions are found for a given day Di, one must find a way to eliminate potential solutions with the added information given by { Di+1 .. Di+n }.

Let's lay down some bases for the upcoming examples:

Lets keep the same item values, the whole game. It can either be random or chosen by the user.
The possible item values is bound to the very limited range of [1-10], where no two items can have the same value.
No item can have a quantity greater than 100. That means: [0-100].

In order to solve this more easily I took the liberty to change one constraint, which makes the algorithm converge faster:

The "total quantity" rule is overridden by this rule: You can add or remove any number of items within the [1-10] range, total, in one day. However, you cannot add or remove the same number of items, total, more than twice. This also gives the game a maximum lifecycle of 20 days.

This rule enables us to rule out solutions more easily. And, with non-tiny ranges, renders Backtracking algorithms still useless, just like your original problem and rules.
In my humble opinion, this rule is not the essence of the game but only a facilitator, enabling the computer to solve the problem.
Problem 1: Finding potential solutions
For starters, problem 1. can be solved using a Monte Carlo algorithm to find a set of potential solutions. The technique is simple: Generate random numbers for item values and quantities (within their respective accepted range). Repeat the process for the required number of items. Verify whether or not the solution is acceptable. That means verifying if items have distinct values and the total is equal to our target total (say, 143.)
While this technique has the advantage of being easy to implement it has some drawbacks:

The user's solution is not guaranteed to appear in our results.
There is a lot of "misses". For instance, it takes more or less 3,000,000 tries to find 1,000 potential solutions given our constraints.
It takes a lot of time: around 4 to 5 seconds on my lazy laptop.

How to get around these drawback? Well...

Limit the range to smaller values and
Find an adequate number of potential solutions so there is a good chance the user's solution appears in your solution set.
Use heuristics to find solutions more easily (more on that later.)

Note that the more you restrict the ranges, the less useful while be the Monte Carlo algorithm is, since there will be few enough valid solutions to iterate on them all in reasonable time. For constraints { 3, [1-10], [0-100] } there is around 741,000,000 valid solutions (not constrained to a target total value.) Monte Carlo is usable there. For { 3, [1-5], [0-10] }, there is only around 80,000. No need to use Monte Carlo; brute force for loops will do just fine.
I believe the problem 1 is what you would call a Constraint satisfaction problem (or CSP.)
Problem 2: Restrict the set of potential solutions
Given the fact that problem 1 is a CSP, I would go ahead and call problem 2, and the problem in general, a Dynamic CSP (or DCSP.)

[DCSPs] are useful when the original formulation of a
  problem is altered in some way, typically because the set of
  constraints to consider evolves because of the environment. DCSPs
  are viewed as a sequence of static CSPs, each one a transformation of
  the previous one in which variables and constraints can be added
  (restriction) or removed (relaxation).

One technique used with CSPs that might be useful to this problem is called Constraint Recording:

With each change in the environment (user entered values for Di+1), find information about the new constraint: What are the possibly "used" quantities for the add-remove constraint.
Apply the constraint to every preceding day in cascade. Rippling effects might significantly reduce possible solutions.

For this to work, you need to get a new set of possible solutions every day; Use either brute force or Monte Carlo. Then, compare solutions of Di to Di-1 and keep only solutions that can succeed to previous days' solutions without violating constraints.
You will probably have to keep an history of what solutions lead to what other solutions (probably in a directed graph.) Constraint recording enables you to remember possible add-remove quantities and rejects solutions based on that.
There is a lot of other steps that could be taken to further improve your solution. Here are some ideas:

Record constraints for item-value combinations found in previous days solutions. Reject other solutions immediately (as item values must not change.) You could even find a smaller solution sets for each existing solution using solution-specific constraints to reject invalid solutions earlier.
Generate some "mutant", full-history, solutions each day in order to "repair" the case where the D1 solution set doesn't contain the user's solution. You could use a genetic algorithm to find a mutant population based on an existing solution set.)
Use heuristics in order find solutions easily (e.g. when a valid solution is found, try and find variations of this solution by substituting quantities around.)
Use behavioral heuristics in order to predict some user actions (e.g. same quantity for every item, extreme patterns, etc.)
Keep making some computations while the user is entering new quantities.

Given all of this, try and figure out a ranking system based on occurrence of solutions and heuristics to determine a candidate solution.
 I will try to tomorrow, but I'm not too good at formal proofs. However, I can safely say that the problem looks like an optimisation problem, which is more often than not NP rather than P. I finally removed the NP-hard assumption (and refactored my answer a great deal), since I initially though the problem was an optimization problem. The problem might still be of NP-something complexity, but I'm not certain.
I wrote a program to play the game.  Of course, I had to automate the human side, but I believe I did it all in such a way that I shouldn't invalidate my approach when played against a real human.
I approached this from a machine learning perspective and treated the problem as a hidden markov model where the total price was the observation.  My solution is to use a particle filter.  This solution is written in Python 2.7 using NumPy and SciPy.
I stated any assumptions I made either explicitly in the comments or implicitly in the code.  I also set some additional constraints for the sake of getting code to run in an automated fashion.  It's not particularly optimized as I tried to err on the side comprehensibility rather than speed.
Each iteration outputs the current true quantities and the guess.  I just pipe the output to a file so I can review it easily.  An interesting extension would be to plot the output on a graph either 2D (for 2 fruits) or 3D (for 3 fruits).  Then you would be able to see the particle filter hone in on the solution.
Update:
Edited the code to include updated parameters after tweaking.  Included plotting calls using matplotlib (via pylab).  Plotting works on Linux-Gnome, your mileage may vary.  Defaulted NUM_FRUITS to 2 for plotting support.  Just comment out all the pylab calls to remove plotting and be able to change NUM_FRUITS to anything.
Does a good job estimating the current fxn represented by UnknownQuantities X Prices = TotalPrice.  In 2D (2 Fruits) this is a line, in 3D (3 Fruits) it'd be a plane.  Seems to be too little data for the particle filter to reliably hone in on the correct quantities.  Need a little more smarts on top of the particle filter to really bring together the historical information.  You could try converting the particle filter to 2nd- or 3rd-order.
Update 2:
I've been playing around with my code, a lot.  I tried a bunch of things and now present the final program that I'll be making (starting to burn out on this idea).
Changes:
The particles now use floating points rather than integers.  Not sure if this had any meaningful effect, but it is a more general solution.  Rounding to integers is done only when making a guess.
Plotting shows true quantities as green square and current guess as red square.  Currently believed particles shown as blue dots (sized by how much we believe them).  This makes it really easy to see how well the algorithm is working.  (Plotting also tested and working on Win 7 64-bit).
Added parameters for turning off/on quantity changing and price changing.  Of course, both 'off' is not interesting.  
It does a pretty dang good job, but, as has been noted, it's a really tough problem, so getting the exact answer is hard.  Turning off CHANGE_QUANTITIES produces the simplest case.  You can get an appreciation for the difficulty of the problem by running with 2 fruits with CHANGE_QUANTITIES off.  See how quickly it hones in on the correct answer then see how harder it is as you increase the number of fruit.
You can also get a perspective on the difficulty by keeping CHANGE_QUANTITIES on, but adjusting the MAX_QUANTITY_CHANGE from very small values (.001) to "large" values (.05).
One situation where it struggles is if on dimension (one fruit quantity) gets close to zero.  Because it's using an average of particles to guess it will always skew away from a hard boundary like zero.
In general this makes a great particle filter tutorial.

from __future__ import division
import random
import numpy
import scipy.stats
import pylab

# Assume Guesser knows prices and total
# Guesser must determine the quantities

# All of pylab is just for graphing, comment out if undesired
#   Graphing only graphs first 2 FRUITS (first 2 dimensions)

NUM_FRUITS = 3
MAX_QUANTITY_CHANGE = .01 # Maximum percentage change that total quantity of fruit can change per iteration
MAX_QUANTITY = 100 # Bound for the sake of instantiating variables
MIN_QUANTITY_TOTAL = 10 # Prevent degenerate conditions where quantities all hit 0
MAX_FRUIT_PRICE = 1000 # Bound for the sake of instantiating variables
NUM_PARTICLES = 5000
NEW_PARTICLES = 500 # Num new particles to introduce each iteration after guessing
NUM_ITERATIONS = 20 # Max iterations to run
CHANGE_QUANTITIES = True
CHANGE_PRICES = True

'''
  Change individual fruit quantities for a random amount of time
  Never exceed changing fruit quantity by more than MAX_QUANTITY_CHANGE
'''
def updateQuantities(quantities):
  old_total = max(sum(quantities), MIN_QUANTITY_TOTAL)
  new_total = old_total
  max_change = int(old_total * MAX_QUANTITY_CHANGE)

  while random.random() > .005: # Stop Randomly    
    change_index = random.randint(0, len(quantities)-1)
    change_val = random.randint(-1*max_change,max_change)

    if quantities[change_index] + change_val >= 0: # Prevent negative quantities
      quantities[change_index] += change_val
      new_total += change_val

      if abs((new_total / old_total) - 1) > MAX_QUANTITY_CHANGE:
        quantities[change_index] -= change_val # Reverse the change

def totalPrice(prices, quantities):
  return sum(prices*quantities)

def sampleParticleSet(particles, fruit_prices, current_total, num_to_sample):
  # Assign weight to each particle using observation (observation is current_total)
  # Weight is the probability of that particle (guess) given the current observation
  # Determined by looking up the distance from the hyperplane (line, plane, hyperplane) in a
  #   probability density fxn for a normal distribution centered at 0 
  variance = 2
  distances_to_current_hyperplane = [abs(numpy.dot(particle, fruit_prices)-current_total)/numpy.linalg.norm(fruit_prices) for particle in particles]
  weights = numpy.array([scipy.stats.norm.pdf(distances_to_current_hyperplane[p], 0, variance) for p in range(0,NUM_PARTICLES)])

  weight_sum = sum(weights) # No need to normalize, as relative weights are fine, so just sample un-normalized

  # Create new particle set weighted by weights
  belief_particles = []
  belief_weights = []
  for p in range(0, num_to_sample):
    sample = random.uniform(0, weight_sum)
    # sum across weights until we exceed our sample, the weight we just summed is the index of the particle we'll use
    p_sum = 0
    p_i = -1
    while p_sum < sample:
      p_i += 1
      p_sum += weights[p_i]
    belief_particles.append(particles[p_i])
    belief_weights.append(weights[p_i])

  return belief_particles, numpy.array(belief_weights)

'''
  Generates new particles around the equation of the current prices and total (better particle generation than uniformly random)
'''
def generateNewParticles(current_total, fruit_prices, num_to_generate):
  new_particles = []
  max_values = [int(current_total/fruit_prices[n]) for n in range(0,NUM_FRUITS)]
  for p in range(0, num_to_generate):
    new_particle = numpy.array([random.uniform(1,max_values[n]) for n in range(0,NUM_FRUITS)])
    new_particle[-1] = (current_total - sum([new_particle[i]*fruit_prices[i] for i in range(0, NUM_FRUITS-1)])) / fruit_prices[-1]
    new_particles.append(new_particle)
  return new_particles


# Initialize our data structures:
# Represents users first round of quantity selection
fruit_prices = numpy.array([random.randint(1,MAX_FRUIT_PRICE) for n in range(0,NUM_FRUITS)])
fruit_quantities = numpy.array([random.randint(1,MAX_QUANTITY) for n in range(0,NUM_FRUITS)])
current_total = totalPrice(fruit_prices, fruit_quantities)
success = False

particles = generateNewParticles(current_total, fruit_prices, NUM_PARTICLES) #[numpy.array([random.randint(1,MAX_QUANTITY) for n in range(0,NUM_FRUITS)]) for p in range(0,NUM_PARTICLES)]
guess = numpy.average(particles, axis=0)
guess = numpy.array([int(round(guess[n])) for n in range(0,NUM_FRUITS)])

print "Truth:", str(fruit_quantities)
print "Guess:", str(guess)

pylab.ion()
pylab.draw()
pylab.scatter([p[0] for p in particles], [p[1] for p in particles])
pylab.scatter([fruit_quantities[0]], [fruit_quantities[1]], s=150, c='g', marker='s')
pylab.scatter([guess[0]], [guess[1]], s=150, c='r', marker='s')
pylab.xlim(0, MAX_QUANTITY)
pylab.ylim(0, MAX_QUANTITY)
pylab.draw()

if not (guess == fruit_quantities).all():
  for i in range(0,NUM_ITERATIONS):
    print "------------------------", i

    if CHANGE_PRICES:
      fruit_prices = numpy.array([random.randint(1,MAX_FRUIT_PRICE) for n in range(0,NUM_FRUITS)])

    if CHANGE_QUANTITIES:
      updateQuantities(fruit_quantities)
      map(updateQuantities, particles) # Particle Filter Prediction

    print "Truth:", str(fruit_quantities)
    current_total = totalPrice(fruit_prices, fruit_quantities)

    # Guesser's Turn - Particle Filter:
    # Prediction done above if CHANGE_QUANTITIES is True

    # Update
    belief_particles, belief_weights = sampleParticleSet(particles, fruit_prices, current_total, NUM_PARTICLES-NEW_PARTICLES)
    new_particles = generateNewParticles(current_total, fruit_prices, NEW_PARTICLES)

    # Make a guess:
    guess = numpy.average(belief_particles, axis=0, weights=belief_weights) # Could optimize here by removing outliers or try using median
    guess = numpy.array([int(round(guess[n])) for n in range(0,NUM_FRUITS)]) # convert to integers
    print "Guess:", str(guess)

    pylab.cla()
    #pylab.scatter([p[0] for p in new_particles], [p[1] for p in new_particles], c='y') # Plot new particles
    pylab.scatter([p[0] for p in belief_particles], [p[1] for p in belief_particles], s=belief_weights*50) # Plot current particles
    pylab.scatter([fruit_quantities[0]], [fruit_quantities[1]], s=150, c='g', marker='s') # Plot truth
    pylab.scatter([guess[0]], [guess[1]], s=150, c='r', marker='s') # Plot current guess
    pylab.xlim(0, MAX_QUANTITY)
    pylab.ylim(0, MAX_QUANTITY)
    pylab.draw()

    if (guess == fruit_quantities).all():
      success = True
      break

    # Attach new particles to existing particles for next run:
    belief_particles.extend(new_particles)
    particles = belief_particles
else:
  success = True

if success:
  print "Correct Quantities guessed"
else:
  print "Unable to get correct answer within", NUM_ITERATIONS, "iterations"

pylab.ioff()
pylab.show()

 wow..I was just going to write an answer to my own question saying that the answers were good but I think the solution is a hidden markov or viterbi algorithm.  I got a message saying a new answer had been posted and I refreshed to this.  Great answer.  I'll do some tests and let you know how it goes..Thanks Kyle it seems interesting.  I get your logic but there's a few questions I had.  It seems like its randomly guessing.  Is there a way to include  not only the past sum but all of the past sums(with the last one more heavily weighted).  It seems like every answer is close to the last answer only but looking a few sums back the result suggested doesn't seem related. It's only representing a first-order hidden markov model, so it's only caring about one step.  An improvement would be to turn it into a 2nd or 3rd order.  Right now I'm working on tuning the parameters for better results.  In theory, a well-tuned 1st-order HMM should be fine, because the particles "represent" a history of where they came from.  Hopefully I'll have a tuned update soon which works better. also my question is a bit different..but I am more trying to learn the algo for this.  My question basically says based on the sum I compute a list of every possibility and wanted an algo to try to figure out which possibility was most likely.  No worries though I think your logic applies, but I think somehow it needs to use the hidden markov or viterbi algorithm to track the changes in fruit prices as well as change in sum and based on that relation provide a guess. @Kyle..Thanks so much.  I was asking on other boards questions of implementing and after reading your code, it did make things alot more clear. Getting it to go more than one order I think will be interesting, as my end goal is to give the user hundreds of options from a fictional store and let them choose any quantity(I will try to do this on hadoop and scale over my 3 machines at home, but the quicker it can figure it out the better). Thanks so much.
For your initial rules:
From my school years, I would say that if we make an abstraction of the 5% changes, we have everyday an equation with three unknown values (sorry I don't know the maths vocabulary in English), which are the same values as previous day.
At day 3, you have three equations, three unknown values, and the solution should be direct.
I guess the 5% change each day may be forgotten if the values of the three elements are different enough, because, as you said, we will use approximations and round the numbers.
For your adapted rules:
Too many unknowns - and changing - values in this case, so there is no direct solution I know of. I would trust Lior on this; his approach looks fine! (If you have a limited range for prices and quantities.)
Mixing categorial and continuous data in Naive Bayes classifier using scikit-learn 
I'm using scikit-learn in Python to develop a classification algorithm to predict the gender of certain customers. Amongst others, I want to use the Naive Bayes classifier but my problem is that I have a mix of categorical data (ex: "Registered online", "Accepts email notifications" etc) and continuous data (ex: "Age", "Length of membership" etc). I haven't used scikit much before but I suppose that that Gaussian Naive Bayes is suitable for continuous data and that Bernoulli Naive Bayes can be used for categorical data. However, since I want to have both categorical and continuous data in my model, I don't really know how to handle this. Any ideas would be much appreciated!
 Hi, can you tell which solution worked best for you?
You have at least two options:

Transform all your data into a categorical representation by computing percentiles for each continuous variables and then binning the continuous variables using the percentiles as bin boundaries. For instance for the height of a person create the following bins: "very small", "small", "regular", "big", "very big" ensuring that each bin contains approximately 20% of the population of your training set. We don't have any utility to perform this automatically in scikit-learn but it should not be too complicated to do it yourself. Then fit a unique multinomial NB on those categorical representation of your data.
Independently fit a gaussian NB model on the continuous part of the data and a multinomial NB model on the categorical part. Then transform all the dataset by taking the class assignment probabilities (with predict_proba method) as new features: np.hstack((multinomial_probas, gaussian_probas)) and then refit a new model (e.g. a new gaussian NB) on the new features.

 @ogrisel: Am I right in believing that the second method might miss correlations between the continuous and categorical data? For example, suppose young people who register online are typically male, but young people who do not register online are typically female. But further suppose for the sake of concreteness that the gaussian NB model predicts young people (without knowledge of the categorical data) are generally male. Since only this probability is being passed on to the second-stage gaussian NB, it will miss the correlation. @unutbu: Naive Bayes classifiers assumes independence of the features given the class.  The first method listed above will learn P(age|gender) and P(registration_type|gender) independently.  The correlation between age and registration_type will not be captured for a given gender. @ogrisel can we use one-hot-encoding to convert the categorical variables to  values between 0 and n-1 for n classes and keep the continuous variables as they are for GaussianNB() ? based on this post: dataaspirant.com/2017/02/20/… @jai, No! First, one-hot encoding is not the same as converting to values between 0 and n-1.  Second, converting categorical variables to values between 0 and n-1 and then treating them as continuous variables makes no sense.  Third, one-hot categorical variables are so non-Gaussian, that treating them as Gaussian (which GaussianNB assumes) does not, in my experience, produce good results. @ogrisel, I thought that predict_proba is to be used for predicting probabilities on the 'test' data. E.g. I create 2 separate classifiers on the train data, and I can then use this to predict the probability for my remaining test data. If I then train another Gaussian model on the predict_proba result from the test data, doesn't that leave with nothing to test on? Am I looking at this correctly? Cheers
The simple answer: multiply result!! it's the same.
Naive Bayes based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features - meaning you calculate the Bayes probability dependent on a specific feature without holding the others - which means that the algorithm multiply each probability from one feature with the probability from the second feature (and we totally ignore the denominator - since it is just a normalizer).
so the right answer is:

calculate the probability from the categorical variables.
calculate the probability from the continuous variables.
multiply 1. and 2. 

 Gaussian NB gives a density estimate for the prior. I'm not sure about what you meant for the second part. @Davis, I'm not sure what you meant, but the Gaussian NB means that the the likelihood of the features is assumed to be Gaussian and this how the P(x|y) is calculated. I mean there isn't Pr(x_i | y) anymore, but this prior is replaced Norm(mu_i, sig_i) which is a density estimate because the probability of Pr(X_i = x | y) is zero as the RV X_i is continuous. I think your question is not related to the topic but you can get your answer from: stats.stackexchange.com/questions/26624/… Yaron ,when you say calculate the probabilities, is this on the test or train data, and what function would you use to do this? Are you using predict-proba on the train data as well as doing the fit on the train data? I'm struggling to figure out what I should be multiplying... Cheers
Hope I'm not too late. I recently wrote a library called Mixed Naive Bayes, written in NumPy. It can assume a mix of Gaussian and categorical (multinoulli) distributions on the training data features.
https://github.com/remykarem/mixed-naive-bayes
The  library is written such that the APIs are similar to scikit-learn's.
In the example below, let's assume that the first 2 features are from a categorical distribution and the last 2 are Gaussian. In the fit() method, just specify categorical_features=[0,1], indicating that Columns 0 and 1 are to follow categorical distribution.
from mixed_naive_bayes import MixedNB
X = [[0, 0, 180.9, 75.0],
     [1, 1, 165.2, 61.5],
     [2, 1, 166.3, 60.3],
     [1, 1, 173.0, 68.2],
     [0, 2, 178.4, 71.0]]
y = [0, 0, 1, 1, 0]
clf = MixedNB(categorical_features=[0,1])
clf.fit(X,y)
clf.predict(X)

Pip installable via pip install mixed-naive-bayes. More information on the usage in the README.md file. Pull requests are greatly appreciated :)
Decision tree vs. Naive Bayes classifier [closed] 






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 4 years ago.




I am doing some research about different data mining techniques and came across something that I could not figure out.
If any one have any idea that would be great.
In which cases is it better to use a Decision tree and other cases a Naive Bayes classifier?
Why use one of them in certain cases? And the other in different cases?
(By looking at its functionality, not at the algorithm)
Anyone have some explanations or references about this?
 This question appears to be off-topic because it is about statistics I'm voting to close this question as off-topic because it belongs more to datascience.stackexchange.com
Decision Trees are very flexible, easy to understand, and easy to debug.  They will work with classification problems and regression problems.  So if you are trying to predict a categorical value like (red, green, up, down) or if you are trying to predict a continuous value like 2.9, 3.4 etc Decision Trees will handle both problems.  Probably one of the coolest things about Decision Trees is they only need a table of data and they will build a classifier directly from that data without needing any up front design work to take place.  To some degree properties that don't matter won't be chosen as splits and will get eventually pruned so it's very tolerant of nonsense.  To start it's set it and forget it.
However, the downside.  Simple decision trees tend to over fit the training data more so that other techniques which means you generally have to do tree pruning and tune the pruning procedures.  You didn't have any upfront design cost, but you'll pay that back on tuning the trees performance.  Also simple decision trees divide the data into squares so building clusters around things means it has to split a lot to encompass clusters of data.  Splitting a lot leads to complex trees and raises probability you are overfitting.  Tall trees get pruned back so while you can build a cluster around some feature in the data it might not survive the pruning process.  There are other techniques like surrogate splits which let you split along several variables at once creating splits in the space that aren't either horizontal or perpendicular ( 0 < slope < infinity ).  Cool, but your tree starts to become harder to understand, and its complex to implement these algorithms.  Other techniques like boosting and random forest decision trees can perform quite well, and some feel these techniques are essential to get the best performance out of decision trees.  Again this adds more things to understand and use to tune the tree and hence more things to implement.  In the end the more we add to the algorithm the taller the barrier to using it.
Naive Bayes requires you build a classification by hand.  There's not way to just toss a bunch of tabular data at it and have it pick the best features it will use to classify.  Picking which features matter is up to you.  Decisions trees will pick the best features for you from tabular data.  If there were a way for Naive Bayes to pick features you'd be getting close to using the same techniques that make decision trees work like that.  Give this fact that means you may need to combine Naive Bayes with other statistical techniques to help guide you towards what features best classify and that could be using decision trees.  Naive bayes will answer as a continuous classifier.  There are techniques to adapt it to categorical prediction however they will answer in terms of probabilities like (A 90%, B 5%, C 2.5% D 2.5%)  Bayes can perform quite well, and it doesn't over fit nearly as much so there is no need to prune or process the network.  That makes them simpler algorithms to implement.  However, they are harder to debug and understand because it's all probabilities getting multiplied 1000's of times so you have to be careful to test it's doing what you expect.  Naive bayes does quite well when the training data doesn't contain all possibilities so it can be very good with low amounts of data.  Decision trees work better with lots of data compared to Naive Bayes.
Naive Bayes is used a lot in robotics and computer vision, and does quite well with those tasks.  Decision trees perform very poorly in those situations.  Teaching a decision tree to recognize poker hands by looking a millions of poker hands does very poorly because royal flushes and quads occurs so little it often gets pruned out.  If it's pruned out of the resulting tree it will misclassify those important hands (recall tall trees discussion from above).  Now just think if you are trying to diagnose cancer using this.  Cancer doesn't occur in the population in large amounts, and it will get pruned out more likely.  Good news is this can be handled by using weights so we weight a winning hand or having cancer as higher than a losing hand or not having cancer and that boosts it up the tree so it won't get pruned out.  Again this is the part of tuning the resulting tree to the situation that I discussed earlier.
Decision trees are neat because they tell you what inputs are the best predicators of the outputs so often decision trees can guide you to find if there is a statistical relationship between a given input to the output and how strong that relationship is.  Often the resulting decision tree is less important than relationships it describes.  So decision trees can be used a research tool as you learn about your data so you can build other classifiers.
If you are dicing between using decision trees vs naive bayes to solve a problem often times it best to test each one.  Build a decision tree and build a naive bayes classifier then have a shoot out using the training and validation data you have.  Which ever performs best will more likely perform better in the field.  And it's always a good idea to cast each of those against K-nearest neighbor (KNN) predictors because k-nearest has been shown to out perform both of them in some situations, and KNN is a simple algorithm to implement and use.  If KNN performs better than the other two go with it.
Some sources:
The manual on CART based decision trees.  This books covers the CART algorithm, but also discusses decision trees, weights, missing values, surrogate splits, boosting, etc.
http://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418
A gentler intro to CART
https://www.youtube.com/watch?v=p17C9q2M00Q
Comparison of algorithms - notice that KNN, Decision Trees, C4.5, and SVM do quite well on most of the tests.
http://www4.ncsu.edu/~arezaei2/paper/JCIT4-184028_Camera%20Ready.pdf
Another comparison of algorithms - Boosted Decision Trees and random top the list with KNN in the middle:
http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf
Another good run down of various techniques:
http://www.quora.com/What-are-the-advantages-of-different-classification-algorithms
 Thank you for taking the time to write this. Clear and straight to the point. Readers should be wary --- some of this is not correct. For instance, it is totally, trivially, possible to build a Naive Bayes model from tabular data. I did so last week, in about 20 minutes, and have done so dozens of times before that. KNN does not out perform all decision tree models, in general, or even Naive Bayes in general, though it may for specific datasets. However, the post is useful because lazy students have taken it as fact, and even plagiarized it. They were easily caught. +1. Allow me to clarify, I'm not saying it is impossible to build a Naive Bayes (NB) model from tabular data, but NB doesn't include an algorithm that feed it tabular data and it will build a Naive Bayes model from it.   When you say you built it you designed a Naive Bayes model from looking at tabular data vs. allowing the computer to build that Naive Bayes model for you from tabular data.  KNN doesn't always win, but in some cases it has been shown to outperform both models.  I suppose I should soften up the language so there is no confusion there. Your answer makes perfectly sense. But can you give some references to this please? I've added some sources.  Most of what I described about Naive Bayes can be found in any ML text book.  Most of what I claimed about effectiveness of these techniques on data sets can be seen in the comparison paper attached.R Random Forests Variable Importance 
I am trying to use the random forests package for classification in R.
The Variable Importance Measures listed are:

mean raw importance score of variable x for class 0
mean raw importance score of variable x for class 1
MeanDecreaseAccuracy
MeanDecreaseGini

Now I know what these "mean" as in I know their definitions.  What I want to know is how to use them.
What I really want to know is what these values mean in only the context of how accurate they are, what is a good value, what is a bad value, what are the maximums and minimums, etc.
If a variable has a high MeanDecreaseAccuracy or MeanDecreaseGini does that mean it is important or unimportant?  Also any information on raw scores could be useful too.
I want to know everything there is to know about these numbers that is relevant to the application of them.  
An explanation that uses the words 'error', 'summation', or 'permutated' would be less helpful then a simpler explanation that didn't involve any discussion of how random forests works.
Like if I wanted someone to explain to me how to use a radio, I wouldn't expect the explanation to involve how a radio converts radio waves into sound.


An explanation that uses the words 'error', 'summation', or 'permutated'
  would be less helpful then a simpler explanation that didn't involve any
  discussion of how random forests works.
Like if I wanted someone to explain to me how to use a radio, I wouldn't
  expect the explanation to involve how a radio converts radio waves into sound.

How would you explain what the numbers in WKRP 100.5 FM "mean" without going into the pesky technical details of wave frequencies?  Frankly parameters and related performance issues with Random Forests are difficult to get your head around even if you understand some technical terms.
Here's my shot at some answers:

-mean raw importance score of variable x for class 0
-mean raw importance score of variable x for class 1

Simplifying from the Random Forest web page, raw importance score measures how much more helpful than random a particular predictor variable is in successfully classifying data.

-MeanDecreaseAccuracy

I think this is only in the R module, and I believe it measures how much inclusion of this predictor in the model reduces classification error.

-MeanDecreaseGini

Gini is defined as "inequity" when used in describing a society's distribution of income, or a measure of "node impurity" in tree-based classification.  A low Gini (i.e. higher descrease in Gini) means that a particular predictor variable plays a greater role in partitioning the data into the defined classes.  It's a hard one to describe without talking about the fact that data in classification trees are split at individual nodes based on values of predictors.  I'm not so clear on how this translates into better performance.
 Please, include the link to the Gini definition that is actually used for node splitting: en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
For your immediate concern: higher values mean the variables are more important.  This should be true for all the measures you mention.
Random forests give you pretty complex models, so it can be tricky to interpret the importance measures.  If you want to easily understand what your variables are doing, don't use RFs.  Use linear models or a (non-ensemble) decision tree instead.
You said:

An explanation that uses the words
  'error', 'summation', or 'permutated'
  would be less helpful then a simpler
  explanation that didn't involve any
  discussion of how random forests
  works.

It's going to be awfully tough to explain much more than the above unless you dig in and learn what about random forests.  I assume you're complaining about either the manual, or the section from Breiman's manual:
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp
To figure out how important a variable is, they fill it with random junk ("permute" it), then see how much predictive accuracy decreases.  MeanDecreaseAccuracy and MeanDecreaseGini work this way.  I'm not sure what the raw importance scores are.

Interpretability is kinda tough with Random Forests. While RF is an extremely robust classifier it makes its predictions democratically. By this I mean you build hundreds or thousands of trees by taking a random subset of your variables and a random subset of your data and build a tree. Then make a prediction for all the non-selected data and save the prediction. Its robust because it deals well with the vagaries of your data set, (ie it smooths over randomly high/low values, fortuitous plots/samples, measuring the same thing 4 different ways, etc). However if you have some highly correlated variables, both may seem important as they are not both always included in each model. 
One potential approach with random forests may be to help whittle down your predictors then switch to regular CART or try the PARTY package for inference based tree models. However then you must be wary about data mining issues, and making inferences about parameters.
Calculate AUC in R? 
Given a vector of scores and a vector of actual class labels, how do you calculate a single-number AUC metric for a binary classifier in the R language or in simple English? 
Page 9 of "AUC: a Better Measure..." seems to require knowing the class labels, and here is an example in MATLAB where I don't understand 
R(Actual == 1))

Because R (not to be confused with the R language) is defined a vector but used as a function?
 For anyone else who doesn't know, apparently AUC is the "Area Under the receiver operating characteristic Curve"
As mentioned by others, you can compute the AUC using the ROCR package.  With the ROCR package you can also plot the ROC curve, lift curve and other model selection measures. 
You can compute the AUC directly without using any package by using the fact that the AUC is equal to the probability that a true positive is scored greater than a true negative.
For example, if pos.scores is a vector containing a score of the positive examples, and neg.scores is a vector containing the negative examples then the AUC is approximated by:
> mean(sample(pos.scores,1000,replace=T) > sample(neg.scores,1000,replace=T))
[1] 0.7261

will give an approximation of the AUC.  You can also estimate the variance of the AUC by bootstrapping:
> aucs = replicate(1000,mean(sample(pos.scores,1000,replace=T) > sample(neg.scores,1000,replace=T)))

 For my test data set your replicated value is very similar to @jonw's (is 0.8504, yours 0.850591) except I don't need to install pROC.  Thank you @Andrew @eric This is a terrible answer. You do NOT estimate the variance of the AUC - you only estimate the variance of the resampling process. To convince yourself, try changing the sample size in sample... divide it by 10, your variance is multiplied by 10. Multiply it by 10 and your variance is divided by 10. This is certainly not the desired behaviour to compute the variance of the AUC. In addition the answer should note that the estimate is as good as the number of replicates. Go to infinity and you get the actual AUC. Agree with @Calimo, that is not a bootstrap.  To bootstrap you have to resample N data points with replacement M times, where N is the total size of the original data set and M can be whatever (usually a couple hundred or more).  N is not arbitrary.  If N is not set to the full data set size you'll get biased statistics. I'm a bit unclear on the base R method shown. Can it be calculated purely from the confusion matrix? In the context of a given confusion matrix, what would pos.scores and neg.scores be?
The ROCR package will calculate the AUC among other statistics:
auc.tmp <- performance(pred,"auc"); auc <- as.numeric(auc.tmp@y.values)

 I've used ROCR for plotting performance, but I don't see how it calculates a "single-number AUC metric" (from the original question). auc.tmp <- performance(pred,"auc"); auc <- as.numeric(auc.tmp@y.values)
With the package pROC you can use the function auc() like this example from the help page:
> data(aSAH)
> 
> # Syntax (response, predictor):
> auc(aSAH$outcome, aSAH$s100b)
Area under the curve: 0.7314


Without any additional packages:
true_Y = c(1,1,1,1,2,1,2,1,2,2)
probs = c(1,0.999,0.999,0.973,0.568,0.421,0.382,0.377,0.146,0.11)

getROC_AUC = function(probs, true_Y){
    probsSort = sort(probs, decreasing = TRUE, index.return = TRUE)
    val = unlist(probsSort$x)
    idx = unlist(probsSort$ix)  

    roc_y = true_Y[idx];
    stack_x = cumsum(roc_y == 2)/sum(roc_y == 2)
    stack_y = cumsum(roc_y == 1)/sum(roc_y == 1)    

    auc = sum((stack_x[2:length(roc_y)]-stack_x[1:length(roc_y)-1])*stack_y[2:length(roc_y)])
    return(list(stack_x=stack_x, stack_y=stack_y, auc=auc))
}

aList = getROC_AUC(probs, true_Y) 

stack_x = unlist(aList$stack_x)
stack_y = unlist(aList$stack_y)
auc = unlist(aList$auc)

plot(stack_x, stack_y, type = "l", col = "blue", xlab = "False Positive Rate", ylab = "True Positive Rate", main = "ROC")
axis(1, seq(0.0,1.0,0.1))
axis(2, seq(0.0,1.0,0.1))
abline(h=seq(0.0,1.0,0.1), v=seq(0.0,1.0,0.1), col="gray", lty=3)
legend(0.7, 0.3, sprintf("%3.3f",auc), lty=c(1,1), lwd=c(2.5,2.5), col="blue", title = "AUC")


 If you copy-paste this code and receive Error in plot.window(...) : need finite 'xlim' values, it's probably because your labels are 0-1, while @AGS is using labels 1-2. It doesn't give the true AUC if two observations have the same probability and the order of the observation is not random. Otherwise nice and fast code. Do not know why this solution does not work on my data, my probs are not normalized to be within [0,1]
I found some of the solutions here to be slow and/or confusing (and some of them don't handle ties correctly) so I wrote my own data.table based function auc_roc() in my R package mltools.
library(data.table)
library(mltools)

preds <- c(.1, .3, .3, .9)
actuals <- c(0, 0, 1, 1)

auc_roc(preds, actuals)  # 0.875

auc_roc(preds, actuals, returnDT=TRUE)
   Pred CountFalse CountTrue CumulativeFPR CumulativeTPR AdditionalArea CumulativeArea
1:  0.9          0         1           0.0           0.5          0.000          0.000
2:  0.3          1         1           0.5           1.0          0.375          0.375
3:  0.1          1         0           1.0           1.0          0.500          0.875

 This solution is much much faster than auc() method in pROC package!  auc() method in pROC package is pretty slow if one has to calculate auc scores for multi-class or multiple output regression problem.
You can learn more about AUROC in this blog post by Miron Kursa:
https://mbq.me/blog/augh-roc/
He provides a fast function for AUROC:
# By Miron Kursa https://mbq.me
auroc <- function(score, bool) {
  n1 <- sum(!bool)
  n2 <- sum(bool)
  U  <- sum(rank(score)[!bool]) - n1 * (n1 + 1) / 2
  return(1 - U / n1 / n2)
}

Let's test it:
set.seed(42)
score <- rnorm(1e3)
bool  <- sample(c(TRUE, FALSE), 1e3, replace = TRUE)

pROC::auc(bool, score)
mltools::auc_roc(score, bool)
ROCR::performance(ROCR::prediction(score, bool), "auc")@y.values[[1]]
auroc(score, bool)

0.51371668847094
0.51371668847094
0.51371668847094
0.51371668847094

auroc() is 100 times faster than pROC::auc() and computeAUC().
auroc() is 10 times faster than mltools::auc_roc() and ROCR::performance().
print(microbenchmark(
  pROC::auc(bool, score),
  computeAUC(score[bool], score[!bool]),
  mltools::auc_roc(score, bool),
  ROCR::performance(ROCR::prediction(score, bool), "auc")@y.values,
  auroc(score, bool)
))

Unit: microseconds
                                                             expr       min
                                           pROC::auc(bool, score) 21000.146
                            computeAUC(score[bool], score[!bool]) 11878.605
                                    mltools::auc_roc(score, bool)  5750.651
 ROCR::performance(ROCR::prediction(score, bool), "auc")@y.values  2899.573
                                               auroc(score, bool)   236.531
         lq       mean     median        uq        max neval  cld
 22005.3350 23738.3447 22206.5730 22710.853  32628.347   100    d
 12323.0305 16173.0645 12378.5540 12624.981 233701.511   100   c 
  6186.0245  6495.5158  6325.3955  6573.993  14698.244   100  b  
  3019.6310  3300.1961  3068.0240  3237.534  11995.667   100 ab  
   245.4755   253.1109   251.8505   257.578    300.506   100 a   

 For larger sample sizes, bigstatsr::AUC() is even faster (implemented in C++). Disclaimer: I'm the author.
Combining code from ISL 9.6.3 ROC Curves, along with @J. Won.'s answer to this question and a few more places, the following plots the ROC curve and prints the AUC in the bottom right on the plot.
Below probs is a numeric vector of predicted probabilities for binary classification and test$label contains the true labels of the test data.
require(ROCR)
require(pROC)

rocplot <- function(pred, truth, ...) {
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
  area <- auc(truth, pred)
  area <- format(round(area, 4), nsmall = 4)
  text(x=0.8, y=0.1, labels = paste("AUC =", area))

  # the reference x=y line
  segments(x0=0, y0=0, x1=1, y1=1, col="gray", lty=2)
}

rocplot(probs, test$label, col="blue")

This gives a plot like this:


Along the lines of erik's response, you should also be able to calculate the ROC directly by comparing all possible pairs of values from pos.scores and neg.scores:
score.pairs <- merge(pos.scores, neg.scores)
names(score.pairs) <- c("pos.score", "neg.score")
sum(score.pairs$pos.score > score.pairs$neg.score) / nrow(score.pairs)

Certainly less efficient than the sample approach or the pROC::auc, but more stable than the former and requiring less installation than the latter.
Related: when I tried this it gave similar results to pROC's value, but not exactly the same (off by 0.02 or so); the result was closer to the sample approach with very high N. If anyone has ideas why that might be I'd be interested.
 One source of inaccuracy is dealing with ties. Technically you should take the probability that the positive case score is strictly greater than the negative score + 1/2 * prob they are equal. If all scores are unique this won't be a problem.
I usually use the function ROC from the DiagnosisMed package. I like the graph it produces. AUC is returned along with it's confidence interval and it is also mentioned on the graph.
ROC(classLabels,scores,Full=TRUE)

 As of July 20, 2016 this link cran.r-project.org/web/packages/DiagnosisMed/index.html says Package ‘DiagnosisMed’ was removed from the CRAN repository.
Currently top voted answer is incorrect, because it disregards ties. When positive and negative scores are equal, then AUC should be 0.5. Below is corrected example.
computeAUC <- function(pos.scores, neg.scores, n_sample=100000) {
  # Args:
  #   pos.scores: scores of positive observations
  #   neg.scores: scores of negative observations
  #   n_samples : number of samples to approximate AUC

  pos.sample <- sample(pos.scores, n_sample, replace=T)
  neg.sample <- sample(neg.scores, n_sample, replace=T)
  mean(1.0*(pos.sample > neg.sample) + 0.5*(pos.sample==neg.sample))
}

How to calculate the regularization parameter in linear regression 
When we have a high degree linear polynomial that is used to fit a set of points in a linear regression setup, to prevent overfitting, we use regularization, and we include a lambda parameter in the cost function. This lambda is then used to update the theta parameters in the gradient descent algorithm.
My question is how do we calculate this lambda regularization parameter?

The regularization parameter (lambda) is an input to your model so what you probably want to know is how do you select the value of lambda.  The regularization parameter reduces overfitting, which reduces the variance of your estimated regression parameters; however, it does this at the expense of adding bias to your estimate.  Increasing lambda results in less overfitting but also greater bias.  So the real question is "How much bias are you willing to tolerate in your estimate?"
One approach you can take is to randomly subsample your data a number of times and look at the variation in your estimate.  Then repeat the process for a slightly larger value of lambda to see how it affects the variability of your estimate.  Keep in mind that whatever value of lambda you decide is appropriate for your subsampled data, you can likely use a smaller value to achieve comparable regularization on the full data set.
 Does adding a regularization parameter reduce the variance of the parameters? does that mean they all will be almost equal in magnitude? Is that the variance in their values you refer to? Yes, it reduces the variance of the parameters.  Let's assume that you have K parameters (a_1,a_2,...,a_K) in your linear model and your sample size is N. Given a particular sample of size N, you will compute the values a_1 through a_k.  If you were to take another random sample of size N, it would result in a different set of coefficients (a).  If your sample size is small, then a particular coefficient (e.g., a_1) can vary greatly between samples (high variance).  Regularization reduces this variance.  It doesn't mean that all the coefficients (a_1 ... a_k) will be nearly equal.
CLOSED FORM (TIKHONOV) VERSUS GRADIENT DESCENT
Hi! nice explanations for the intuitive and top-notch mathematical approaches there. I just wanted to add some specificities that, where not "problem-solving", may definitely help to speed up and give some consistency to the process of finding a good regularization hyperparameter.
I assume that you are talking about the L2 (a.k. "weight decay") regularization, linearly weighted by the lambda term, and that you are optimizing the weights of your model either with the closed-form Tikhonov equation (highly recommended for low-dimensional  linear regression models), or with some variant of gradient descent with backpropagation. And that in this context, you want to choose the value for lambda that provides best generalization ability.

CLOSED FORM (TIKHONOV)
If you are able to go the Tikhonov way with your model (Andrew Ng says under 10k dimensions, but this suggestion is at least 5 years old) Wikipedia - determination of the Tikhonov factor offers an interesting closed-form solution, which has been proved to provide the optimal value. But this solution probably raises some kind of implementation issues (time complexity/numerical stability) I'm not aware of, because there is no mainstream algorithm to perform it. This 2016 paper looks very promising though and may be worth a try if you really have to optimize your linear model to its best.

For a quicker prototype implementation, this 2015 Python package seems to deal with it iteratively, you could let it optimize and then extract the final value for the lambda:


In this new innovative method, we have derived an iterative approach to solving the general Tikhonov regularization problem, which converges to the noiseless solution, does not depend strongly on the choice of lambda, and yet still avoids the inversion problem.

And from the GitHub README of the project:
InverseProblem.invert(A, be, k, l) #this will invert your A matrix, where be is noisy be, k is the no. of iterations, and lambda is your dampening effect (best set to 1)

GRADIENT DESCENT
All links of this part are from Michael Nielsen's amazing online book "Neural Networks and Deep Learning", recommended lecture!
For this approach it seems to be even less to be said: the cost function is usually non-convex, the optimization is performed numerically and the performance of the model is measured by some form of cross validation (see Overfitting and Regularization and why does regularization help reduce overfitting if you haven't had enough of that). But even when cross-validating, Nielsen suggests something: you may want to take a look at this detailed explanation  on how does the L2 regularization provide a weight decaying effect, but the summary is that it is inversely proportional to the number of samples n, so when calculating the gradient descent equation with the L2 term,

just use backpropagation, as usual, and then add (λ/n)*w to the partial derivative of all the weight terms.

And his conclusion is that, when wanting a similar regularization effect with a different number of samples, lambda has to be changed proportionally:  

we need to modify the regularization parameter. The reason is because the size n of the training set has changed from n=1000 to n=50000, and this changes the weight decay factor 1−learning_rate*(λ/n). If we continued to use λ=0.1 that would mean much less weight decay, and thus much less of a regularization effect. We compensate by changing to λ=5.0.

This is only useful when applying the same model to different amounts of the same data, but I think it opens up the door for some intuition on how it should work, and, more importantly, speed up the hyperparametrization process by allowing you to finetune lambda in smaller subsets and then scale up.
For choosing the exact values, he suggests in his conclusions on how to choose a neural network's hyperparameters the purely empirical approach: start with 1 and then progressively multiply&divide by 10 until you find the proper order of magnitude, and then do a local search within that region. In the comments of this SE related question, the user Brian Borchers suggests also a very well known method that may be useful for that local search:

Take small subsets of the training and validation sets (to be able to make many of them in a reasonable amount of time)
Starting with λ=0 and increasing by small amounts within some region, perform a quick training&validation of the model and plot both loss functions
You will observe three things:


The CV loss function will be consistently higher than the training one, since your model is optimized for the training data exclusively (EDIT: After some time I've seen a MNIST case where adding L2 helped the CV loss decrease faster than the training one until convergence. Probably due to the ridiculous consistency of the data and a suboptimal hyperparametrization though).
The training loss function will have its minimum for λ=0, and then increase with the regularization, since preventing the model from optimally fitting the training data is exactly what regularization does.
The CV loss function will start high at λ=0, then decrease, and then start increasing again at some point (EDIT: this assuming that the setup is able to overfit for λ=0, i.e. the model has enough power and no other regularization means are heavily applied).

The optimal value for λ will be probably somewhere around the minimum of the CV loss function, it also may depend a little on how does the training loss function look like. See the picture for a possible (but not the only one) representation of this: instead of "model complexity" you should interpret the x axis as λ being zero at the right and increasing towards the left.


Hope this helps! Cheers,
Andres

The cross validation described above is a method used often in Machine Learning. However, choosing a reliable and safe regularization parameter is still a very hot topic of research in mathematics. 
If you need some ideas (and have access to a decent university library) you can have a look at this paper:
http://www.sciencedirect.com/science/article/pii/S0378475411000607
 And if you don't have access to a decent university library, it seems to be available here. @Gschneider Thank you for liberating knowledge and education. Ha, the websites URL of the post ... Should rather be called ScienceIndirect.What information can we access from the client? [closed] 






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                
Closed 2 years ago.




I'm trying to compile a list of information that is accessible via javascript such as:

Geo-location
IP address
Browser software
Exit location
Entrance location

I understand that a user can alter any of this information and that it's reliability is purely trust related, but I am still interested in what other information can be mined from the client.

Don't forget about

Screen Size
Allowed Cookies
Allowed Java
Mobile or Desktop
Language

And here is useful link with data-mining demo:  
http://javascriptsource.com/user-details/browser-properties.html
 Thank's this is some useful information, notice some of the information it get's about me is a bit off like my max screen resolution being less than my current resolution but it's close enough. You probably can't get precise informations using this method. client operating system
Here is most of the information:
var info={

    timeOpened:new Date(),
    timezone:(new Date()).getTimezoneOffset()/60,

    pageon(){return window.location.pathname},
    referrer(){return document.referrer},
    previousSites(){return history.length},

    browserName(){return navigator.appName},
    browserEngine(){return navigator.product},
    browserVersion1a(){return navigator.appVersion},
    browserVersion1b(){return navigator.userAgent},
    browserLanguage(){return navigator.language},
    browserOnline(){return navigator.onLine},
    browserPlatform(){return navigator.platform},
    javaEnabled(){return navigator.javaEnabled()},
    dataCookiesEnabled(){return navigator.cookieEnabled},
    dataCookies1(){return document.cookie},
    dataCookies2(){return decodeURIComponent(document.cookie.split(";"))},
    dataStorage(){return localStorage},

    sizeScreenW(){return screen.width},
    sizeScreenH(){return screen.height},
    sizeDocW(){return document.width},
    sizeDocH(){return document.height},
    sizeInW(){return innerWidth},
    sizeInH(){return innerHeight},
    sizeAvailW(){return screen.availWidth},
    sizeAvailH(){return screen.availHeight},
    scrColorDepth(){return screen.colorDepth},
    scrPixelDepth(){return screen.pixelDepth},


    latitude(){return position.coords.latitude},
    longitude(){return position.coords.longitude},
    accuracy(){return position.coords.accuracy},
    altitude(){return position.coords.altitude},
    altitudeAccuracy(){return position.coords.altitudeAccuracy},
    heading(){return position.coords.heading},
    speed(){return position.coords.speed},
    timestamp(){return position.timestamp},


    };

 I heard that's possible to get battery info from mobile device, and some website, such hotel reservation websites, increase their prices when your battery is near empty. battery level is only accessible on some browsers, iOS 9 it's not available @BelowtheRadar a reference would be nice
visitor.js is a javascript library that provides information about the client.
Including:

Continent, Country and city
Date of last visit
Referring website or search engine (including search term)
Time spent on the website
Browser and operating system
IP Address
Language
Browser
OS
Screen size

And more.
http://www.visitorjs.com/
Visitorjs might be very handy, it's not free however.
How many principal components to take? 
I know that principal component analysis does a SVD on a matrix and then generates an eigen value matrix. To select the principal components we have to take only the first few eigen values. Now, how do we decide on the number of eigen values that we should take from the eigen value matrix?

To decide how many eigenvalues/eigenvectors to keep, you should consider your reason for doing PCA in the first place.  Are you doing it for reducing storage requirements, to reduce dimensionality for a classification algorithm, or for some other reason?  If you don't have any strict constraints, I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order).  If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues).

There is no correct answer, it is somewhere between 1 and n.
Think of a principal component as a street in a town you have never visited before. How many streets should you take to get to know the town?
Well, you should obviously visit the main street (the first component), and maybe some of the other big streets too. Do you need to visit every street to know the town well enough? Probably not.
To know the town perfectly, you should visit all of the streets. But what if you could visit, say 10 out of the 50 streets, and have a 95% understanding of the town? Is that good enough? 
Basically, you should select enough components to explain enough of the variance that you are comfortable with. 

As others said, it doesn't hurt to plot the explained variance.
If you use PCA as a preprocessing step for a supervised learning task, you should cross validate the whole data processing pipeline and treat the number of PCA dimension as an hyperparameter to select using a grid search on the final supervised score (e.g. F1 score for classification or RMSE for regression).
If cross-validated grid search on the whole dataset is too costly try on a 2 sub samples, e.g. one with 1% of the data and the second with 10% and see if you come up with the same optimal value for the PCA dimensions.

There are a number of heuristics use for that.
E.g. taking the first k eigenvectors that capture at least 85% of the total variance.
However, for high dimensionality, these heuristics usually are not very good.
 Thanks. Just a small doubt. The eigen vectors would be arranged in decreasing order right? Do you mean first k eigenvalues that capture 85% of the total sum of the eigenvalues? Yes, the eigenvalues correspond to the relative variance. But it is questionable whether high variance = high importance. It makes sense in low dimensions, e.g. physical x,y,z. But when the dimensions have different meanings and scales, it doesn't really make sense anymore.
Depending on your situation, it may be interesting to define the maximal allowed relative error by projecting your data on ndim dimensions. 
I will illustrate this with a small matlab example. Just skip the code if you are not interested in it.
I will first generate a random matrix of n samples (rows) and p features containing exactly 100 non zero principal components.
n = 200;
p = 119;
data = zeros(n, p);
for i = 1:100
  data = data + rand(n, 1)*rand(1, p);
end

The image will look similar to:

For this sample image, one can calculate the relative error made by projecting your input data to ndim dimensions as follows:
[coeff,score] = pca(data,'Economy',true);

relativeError = zeros(p, 1);
for ndim=1:p
    reconstructed = repmat(mean(data,1),n,1) + score(:,1:ndim)*coeff(:,1:ndim)';
    residuals = data - reconstructed;
    relativeError(ndim) = max(max(residuals./data));
end

Plotting the relative error in function of the number of dimensions (principal components) results in the following graph:

Based on this graph, you can decide how many principal components you need to take into account. In this theoretical image taking 100 components result in an exact image representation. So, taking more than 100 elements is useless. If you want for example maximum 5% error, you should take about 40 principal components.
Disclaimer: The obtained values are only valid for my artificial data. So, do not use the proposed values blindly in your situation, but perform the same analysis and make a trade off between the error you make and the number of components you need.
Code reference

Iterative algorithm is based on the source code of pcares
A StackOverflow post about pcares


I highly recommend the following paper by Gavish and Donoho: The Optimal Hard Threshold for Singular Values is 4/sqrt(3). 
I posted a longer summary of this on CrossValidated (stats.stackexchange.com). Briefly, they obtain an optimal procedure in the limit of very large matrices. The procedure is very simple, does not require any hand-tuned parameters, and seems to work very well in practice.
They have a nice code supplement here: https://purl.stanford.edu/vg705qn9070
How do I extract keywords used in text? [closed] 






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    


Closed 4 years ago.










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





How do I data mine a pile of text to get keywords by usage? ("Jacob Smith" or "fence")
And is there a software to do this already? even semi-automatically, and if it can filter out simple words like "the", "and", "or", then I could get to the topics quicker.

The general algorithm is going to go like this:

- Obtain Text
- Strip punctuation, special characters, etc.
- Strip "simple" words
- Split on Spaces
- Loop Over Split Text
    - Add word to Array/HashTable/Etc if it doesn't exist;
       if it does, increment counter for that word

The end result is a frequency count of all words in the text.  You can then take these values and divide by the total number of words to get a percentage of frequency.  Any further processing is up to you.
You're also going to want to look into Stemming. Stemming is used to reduce words to their root.  For example going => go, cars => car, etc.
An algorithm like this is going to be common in spam filters, keyword indexing and the like.
 can you give some source of -> Strip "simple" words list. I am unable to find them In natural language, "simple" words are most often referred to as stop words. en.wikipedia.org/wiki/Stop_words. There are plenty of stopwords.txt files around... eg. ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words A list of English stop words: xpo6.com/list-of-english-stop-words
This is an open question in NLP, so there is no simple answer.
My recommendation for quick-and-dirty "works-for-me" is topia.termextract.
Yahoo has a keyword extraction service (http://developer.yahoo.com/search/content/V1/termExtraction.html) which is low recall but high precision. In other words, it gives you a small number of high quality terms, but misses many of the terms in your documents.
In Python, there is topia.termextract (http://pypi.python.org/pypi/topia.termextract/). It is relatively noisy, and proposes many bogus keywords, but it simple to use.
Termine (http://www.nactem.ac.uk/software/termine/) is a UK webservice that also is relatively noisy, and proposes many bogus keywords. However, it appears to me to be slightly more accurate than topia.termextract. YMMV.
One way to denoise results with too many keywords (e.g. topia.termextract and termine) is to create a vocabulary of terms that occur frequently, and then throw out proposed terms that are not in the vocabulary. In other words, do two passes over your corpus: The first pass, count the frequency of each keywords. In the second pass, discard the keywords that are too rare.
If you want to write your own, perhaps the best introduction is written by Park, who is now at IBM:

"Automatic glossary extraction: beyond terminology identification" available at http://portal.acm.org/citation.cfm?id=1072370
"Glossary extraction and utilization in the information search and delivery system for IBM technical support"

Here are some more references, if you want to learn more:

http://en.wikipedia.org/wiki/Terminology_extraction
"CorePhrase: Keyphrase Extraction for Document Clustering"
Liu et al 2009 from NAACL HLT
"Automatic Identification of Non-compositional Phrases"
"Data Mining Meets Collocations Discovery"
As well as a host of other references you can dig up on the subject.


There is also a service called Alchemy that can do term-extraction, concept tagging, sentiment analysis and so on.
It's valid, I tested it but I don't know they commercial policies (if any).
They provide APIs for any sort of language (pretty much).
I read somewhere (sorry I don't remember where anymore) that the output given by Alchemy is less noisy compared to those proposed by Joseph.
 Alchemy looks to be very effective. But is there an offline equivalent? Coz, when dealing with lotsa data, a cloud based solution seems to be less efficient. There is a Python Toolkit which looks very promising (from a couple of empirical evaluations I did time ago): nltk.org
You did not specify a technology you're working with, so I guess a shell script is also a possibility.
I've always been impressed by the word frequency analysis example in the Advanced Bash-Scripting Guide (12-11)
The following for example fetches a book from project Gutenburg and writes out a word frequency analysis 'report':
wget http://www.gutenberg.org/files/20417/20417-8.txt -q -O- | 
sed -e 's/\.//g'  -e 's/\,//g' -e 's/ /\
/g' | tr 'A-Z' 'a-z' | sort | uniq -c | sort -nr > output.txt

Should be extendable to exclude words from a 'common' list (the, and, a...) etc.

I personally recommend Maui (http://code.google.com/p/maui-indexer/): it relies on KeA but extends it in a variety of ways. It is trainable and can use RDF formatted terminologies. 

I've used NTLK to recognize named entities before with some success.  It is especially good at recognizing people's and organization's names.
Can anyone give a real life example of supervised learning and unsupervised learning? [closed] 






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                
Closed last year.




I recently studied about supervised learning and unsupervised learning. From theory, I know that supervised means getting the information from labeled datasets and unsupervised means clustering the data without any labels given.
But, the problem is I always get confused to identify whether the given example is supervised learning or unsupervised learning during my studies.
Can anyone please give a real life example?

Supervised learning:

You get a bunch of photos with information about what is on them and then you train a model to recognize new photos.
You have a bunch of molecules and information about which are drugs and you train a model to answer whether a new molecule is also a drug.


Unsupervised learning:

You have a bunch of photos of 6 people but without information about who is on which one and you want to divide this dataset into 6 piles, each with the photos of one individual.
You have molecules, part of them are drugs and part are not but you do not know which are which and you want the algorithm to discover the drugs.

 thanks...that means in unsupervised learning, we cluster data into knowledge groups without having any external knowledge or labels?...right? Clustering is the part of unsupervised learning but not the only one. The only distinction between supervised and unsupervised learning is the access to labels (supervised) or lack of it (unsupervised).
Supervised Learning: 

is like learning with a teacher
training dataset is like a teacher
the training dataset is used to train the machine


Example:
Classification: Machine is trained to classify something into some class. 

classifying whether a patient has disease or not
classifying whether an email is spam or not

Regression: Machine is trained to predict some value like price, weight or height.

predicting house/property price
predicting stock market price


Unsupervised Learning: 

is like learning without a teacher
the machine learns through observation & find structures in data


Example:
Clustering: A clustering problem is where you want to discover the inherent groupings in the data

such as grouping customers by purchasing behavior

Association: An association rule learning problem is where you want to discover rules that describe large portions of your data

such as people that buy X also tend to buy Y


Read more: Supervised and Unsupervised Machine Learning Algorithms

Supervised Learning
This is simple and you would have done it a number of times, for example:

Cortana or any speech automated system in your mobile phone trains your voice and then starts working based on this training.
Based on various features (past record of head-to-head, pitch, toss, player-vs-player) WASP predicts the winning % of both teams.
Train your handwriting to OCR system and once trained, it will be able to convert your hand-writing images into text (till some accuracy obviously)
Based on some prior knowledge (when its sunny, temperature is higher; when its cloudy, humidity is higher, etc.) weather apps predict the parameters for a given time.
Based on past information about spams, filtering out a new incoming email into Inbox (normal) or Junk folder (Spam)
Biometric attendance or ATM etc systems where you train the machine after couple of inputs (of your biometric identity - be it thumb or iris or ear-lobe, etc.), machine can validate your future input and identify you.

Unsupervised Learning

A friend invites you to his party where you meet totally strangers. Now you will classify them using unsupervised learning (no prior knowledge) and this classification can be on the basis of gender, age group, dressing, educational qualification or whatever way you would like. Why this learning is different from Supervised Learning? Since you didn't use any past/prior knowledge about people and classified them "on-the-go".
NASA discovers new heavenly bodies and finds them different from
previously known astronomical objects - stars, planets, asteroids,
blackholes etc. (i.e. it has no knowledge about these new bodies)
and classifies them the way it would like to (distance from Milky way, intensity, gravitational force, red/blue shift or whatever)
Let's suppose you have never seen a Cricket match before and by chance watch a video on internet, now you can classify players on the basis of different criterion: Players wearing same sort of kits are in one class, Players of one style are in one class (batsmen, bowler, fielders), or on the basis of playing hand (RH vs LH) or whatever way you would observe [and classify] it.
We are conducting a survey of 500 questions about predicting the IQ level of students in a college. Since this questionnaire is too big, so after 100 students, administration decides to trim the questionnaire down to fewer questions and for it we use some statistical procedure like PCA to trim it down.

I hope these couple of examples explain the difference in detail.

Supervised Learning
Supervised learning is fairly common in classification problems because the goal is often to get the computer to learn a classification system that we have created. Digit recognition, once again, is a common example of classification learning. More generally, classification learning is appropriate for any problem where deducing a classification is useful and the classification is easy to determine. In some cases, it might not even be necessary to give pre-determined classifications to every instance of a problem if the agent can work out the classifications for itself. This would be an example of unsupervised learning in a classification context. 
Supervised learning is the most common technique for training neural networks and decision trees. Both of these techniques are highly dependent on the information given by the pre-determined classifications. In the case of neural networks, the classification is used to determine the error of the network and then adjust the network to minimize it, and in decision trees, the classifications are used to determine what attributes provide the most information that can be used to solve the classification puzzle. We'll look at both of these in more detail, but for now, it should be sufficient to know that both of these examples thrive on having some "supervision" in the form of pre-determined classifications. 
Speech recognition using hidden Markov models and Bayesian networks relies on some elements of supervision as well in order to adjust parameters to, as usual, minimize the error on the given inputs. 
Notice something important here: in the classification problem, the goal of the learning algorithm is to minimize the error with respect to the given inputs. These inputs, often called the "training set", are the examples from which the agent tries to learn. But learning the training set well is not necessarily the best thing to do. For instance, if I tried to teach you exclusive-or, but only showed you combinations consisting of one true and one false, but never both false or both true, you might learn the rule that the answer is always true. Similarly, with machine learning algorithms, a common problem is over-fitting the data and essentially memorizing the training set rather than learning a more general classification technique. 
Unsupervised Learning
Unsupervised learning seems much harder: the goal is to have the computer learn how to do something that we don't tell it how to do! There are actually two approaches to unsupervised learning. The first approach is to teach the agent not by giving explicit categorizations, but by using some sort of reward system to indicate success. Note that this type of training will generally fit into the decision problem framework because the goal is not to produce a classification but to make decisions that maximize rewards. This approach nicely generalizes to the real world, where agents might be rewarded for doing certain actions and punished for doing others. 
Often, a form of reinforcement learning can be used for unsupervised learning, where the agent bases its actions on the previous rewards and punishments without necessarily even learning any information about the exact ways that its actions affect the world. In a way, all of this information is unnecessary because by learning a reward function, the agent simply knows what to do without any processing because it knows the exact reward it expects to achieve for each action it could take. This can be extremely beneficial in cases where calculating every possibility is very time consuming (even if all of the transition probabilities between world states were known). On the other hand, it can be very time consuming to learn by, essentially, trial and error.
But this kind of learning can be powerful because it assumes no pre-discovered classification of examples. In some cases, for example, our classifications may not be the best possible. One striking exmaple is that the conventional wisdom about the game of backgammon was turned on its head when a series of computer programs (neuro-gammon and TD-gammon) that learned through unsupervised learning became stronger than the best human chess players merely by playing themselves over and over. These programs discovered some principles that surprised the backgammon experts and performed better than backgammon programs trained on pre-classified examples. 
A second type of unsupervised learning is called clustering. In this type of learning, the goal is not to maximize a utility function, but simply to find similarities in the training data. The assumption is often that the clusters discovered will match reasonably well with an intuitive classification. For instance, clustering individuals based on demographics might result in a clustering of the wealthy in one group and the poor in another. 

Supervised Learning has input and correct output. For example: We have the data if a person liked the movie or not. On the basis of interviewing people and gathering their response if they liked the movie or not, we are going to predict if the movie is going to be hit or not. 

Let's look at the picture in the link above. I have visited the restaurants marked by red circle. The restaurants which I have not visited is marked by blue circle.
Now, If I have two restaurants to choose from, A and B, marked by green colour, which one will I choose?
Simple. We can classify the given data linearly into two parts. That means, we can draw a line segregating red and blue circle. Look at the picture in the link below:

Now, we can say with some confidence that chances of my visiting B is more than A. This is a case of supervised learning.
Unsupervised learning has inputs. Let's suppose we have a taxi driver who has an option of accepting or rejecting the bookings. We have plotted his accepted booking location on map with blue circle and is shown below:

Now, Taxi driver has got two bookings A and B; Which one he will accept? If we observe the plot, we can see that his accepted booking shows a cluster at lower left corner. That can be shown in the picture below:


Supervised Learning: In Simple Terms, you have certain inputs and expect some outputs. For example, you have a data of stock market which is of previous data and to get results of the present input for the next few years by giving some instructions it can give you needed output.
Unsupervised Learning: You have parameters like colour, type, size of something and you want a program to predict that whether it is a fruit, plant, animal or whatever it is, this is where Supervised comes in. It gives you output by taking some inputs.
 Main difference is that Supervised learning is labeled for training samples.Comparing R to Matlab for Data Mining 
Instead of starting to code in Matlab, I recently started learning R, mainly because it is open-source. I am currently working in data mining and machine learning field. I found many machine learning algorithms implemented in R, and I am still exploring different packages implemented in R.
I have quick question: how do you compare R to Matlab for data mining application, its popularity, pros and cons, industry and academic acceptance etc.? Which one would you choose and why?
I went through various comparisons for Matlab vs R against various metrics but I am specifically interested to get answer for its applicability in Data Mining and ML. 
Since both language are pretty new for me I was just wondering if R would be a good choice or not.
I appreciate any kind of suggestions.
 See: stackoverflow.com/questions/3125527/…
For the past three years or so, i have used R daily, and the largest portion of that daily use is spent on Machine Learning/Data Mining problems.
I was an exclusive Matlab user while in University; at the time i thought it was
an excellent set of tools/platform. I am sure it is today as well.
The Neural Network Toolbox, the Optimization Toolbox, Statistics Toolbox, 
and Curve Fitting Toolbox are each highly desirable (if not essential) 
for someone using MATLAB for ML/Data Mining work, yet they are all separate from 
the base MATLAB environment--in other words, they have to be purchased separately.
My Top 5 list for Learning ML/Data Mining in R:


Mining Association Rules in R

This refers to a couple things: First, a group of R Package that all begin arules (available from CRAN); you can find the complete list (arules, aruluesViz, etc.) on the Project Homepage. Second, all of these packages are based on a data-mining technique known as Market-Basked Analysis and alternatively as Association Rules. In many respects, this family of algorithms is the essence of data-mining--exhaustively traverse large transaction databases and find above-average associations or correlations among the fields (variables or features) in those databases. In practice, you connect them to a data source and let them run overnight. The central R Package in the set mentioned above is called arules; On the CRAN Package page for arules, you will find links to a couple of excellent secondary sources (vignettes in R's lexicon) on the arules package and on Association Rules technique in general.

The standard reference, The Elements of Statistical
Learning by Hastie et al.

The most current edition of this book is available in digital form for free. Likewise, at the book's website (linked to just above) are all data sets used in ESL, available for free download. (As an aside, i have the free digital version; i also purchased the hardback version from BN.com; all of the color plots in the digital version are reproduced in the hardbound version.) ESL contains thorough introductions to at least one exemplar from most of the major
ML rubrics--e.g., neural metworks, SVM, KNN; unsupervised
techniques (LDA, PCA, MDS, SOM, clustering), numerous flavors of regression, CART, 
Bayesian techniques, as well as model aggregation techniques (Boosting, Bagging) 
and model tuning (regularization). Finally, get the R Package that accompanies the book from CRAN (which will save the trouble of having to download the enter the datasets).



CRAN Task View: Machine Learning

The +3,500 Packages available
for R are divided up by domain into about 30 package families or 'Task Views'. Machine Learning
is one of these families. The Machine Learning Task View contains about 50 or so
Packages. Some of these Packages are part of the core distribution, including e1071
(a sprawling ML package that includes working code for quite a few of 
the usual ML categories.)


Revolution Analytics Blog

With particular focus on the posts tagged with Predictive Analytics 


ML in R tutorial comprised of slide deck and R code by Josh Reich

A thorough study of the code would, by itself, be an excellent introduction to ML in R.

And one final resource that i think is excellent, but didn't make in the top 5:

A Guide to Getting Stared in Machine Learning [in R]

posted at the blog A Beautiful WWW
 There's no question that MATLAB is not cheap, at least for most people.  In my work, I try to avoid using the add-on toolboxes from the MathWorks.  As you say, they present an additional cost, but they also limit portability.  One of the great things about MATLAB is what comes in the base product: no special libraries are needed for image loading, for instance, so my code will run on anyone's MATLAB. Oh, my other point was going to be that there is a great deal of statistics and data mining software available free, from the sizable on-line MATLAB community.  See MATLAB Central's File Exchange, for example.  Universities are another good source of MATLAB code. Thanks for the answer. I really appreciate it. I will take a look at the book you mentioned. In addition people wiritng the book not only have had their methods implemented in R but are contributers themselves! @Predictor, the great thing about R is that it will always run on every machine, no matter how many extension packages you install.
Please look at the CRAN Task Views and in particular at the CRAN Task View on Machine Learning and Statistical Learning which summarises this nicely.

Both Matlab and R are good if you are doing matrix-heavy operations. Because they can use highly optimized low-level code (BLAS libraries and such) for this.
However, there is more to data-mining than just crunching matrixes. A lot of people totally neglect the whole data organization aspect of data mining (as opposed to say, plain machine learning).
And once you get to data organization, R and Matlab are a pain. Try implementing an R*-tree in R or matlab to take an O(n^2) algorithm down to O(n log n) runtime. First of all, it totally goes against the way R and Matlab are designed (use bulk math operations wherever possible), secondly it will kill your performance. Interpreted R code for example seems to run at around 50% of the speed of the C code (try R built-in k-means vs. flexclus k-means); and the BLAS libraries are optimized to an insane level, exploiting cache sizes, data alignment, advanced CPU features. If you are adventurous, try implementing a manual matrix multiplication in R or Matlab, and benchmark it against the native one.
Don't get me wrong. There is a lot of stuff where R and matlab are just elegant and excellent for prototyping. You can solve a lot of things in just 10 lines of code, and get a decent performance out of it. Writing the same thing by hand would be hundreds of lines, and probably 10x slower. But sometimes you can optimize by a level of complexity, which for large data sets does beat the optimized matrix operations of R and matlab.
If you want to scale up to "Hadoop size" on the long run, you will have to think about data layout and organization, too, unless all you need is a linear scan over the data. But then, you could just be sampling, too!

Yesterday I found two new books about Data mining. These series of books entitled by ‘Data Mining’ address the need by presenting in-depth description of novel mining algorithms and many useful applications. In addition to understanding each section deeply, the two books present useful hints and strategies to solving problems in the following chapters.The progress of data mining technology and large public popularity establish a need for a comprehensive text on the subject. Books are: “New Fundamental Technologies in Data Mining” here http://www.intechopen.com/books/show/title/new-fundamental-technologies-in-data-mining  &  “Knowledge-Oriented Applications in Data Mining” here http://www.intechopen.com/books/show/title/knowledge-oriented-applications-in-data-mining  These are open access books so you can download it for free or just read on online reading platform like I do. Cheers!

We should not forget the origin sources for these two software: scientific computation and also signal processing leads to Matlab but statistics leads to R.
I used matlab a lot in University since we have one installed on Unix and open to all students. However, the price for Matlab is too high especially compared to free R. If your major focus is not on matrix computation and signal processing, R should work well for your needs.

I think it also depends in which field of study you are. I know of people in coastal research that use a lot of Matlab. Using R in this group would make your life more difficult. If a colleague has solved a problem, you can't use it because he fixed it using Matlab. 

I would also look at the capabilities of each when you are dealing with large amounts of data.  I know that R can have problems with this, and might be restrictive if you are used to an iterative data mining process.  For example looking at multiple models concurrently.  I don't know if MATLAB has a data limitation.

I admit to favoring MATLAB for data mining problems, and I give some of my reasoning here:
Why MATLAB for Data Mining?
I will admit to only a passing familiarity with R/S-Plus, but I'll make the following observations:

R definitely has more of a statistical focus than MATLAB.  I prefer building my own tools in MATLAB, so that I know exactly what they're doing, and I can customize them, but this is more of a necessity in MATLAB than it would be in R.
Code for new statistical techniques (spatial statistics, robust statistics, etc.) often appears early in S-Plus (I assume that this carries over to R, at least some).
Some years ago, I found the commercial version of R, S-Plus to have an extremely limited capacity for data.  I cannot say what the state of R/S-Plus is today, but you may want to check if your data will fit into such tools comfortably.

 S-Plus is not "commercial version of R". Typically new statistical techniques were written in R, and then ported to S-Plus. Marek, can you comment on the data capacity of R? R has a number of ways to deal with data and different data structures. The main methods are in-memory but one change read lines/chunks, work directly with database interfaces, a number of different file types and HPC structures for "big" data. That depends a great deal on what kind of "columns", what you are doing with the data, your hardware, etc... To give a extremely rough answer I have not had problems working with 1e6 - 1e7 cases with 10's to 100+ variables...but I'm not sure how meaningful that answer is. R is free, give it a try. Being familar with matlab, it should be okay. There is even a matlab emulation package for some of the more commnly used syntax.Kmeans without knowing the number of clusters? [duplicate] 






This question already has answers here:
                        
                    



How do I determine k when using k-means clustering?

                                (18 answers)
                            

Closed 3 years ago.



I am attempting to apply k-means on a set of high-dimensional data points (about 50 dimensions) and was wondering if there are any implementations that find the optimal number of clusters. 
I remember reading somewhere that the way an algorithm generally does this is such that the inter-cluster distance is maximized and intra-cluster distance is minimized but I don't remember where I saw that. It would be great if someone can point me to any resources that discuss this. I am using SciPy for k-means currently but any related library would be fine as well.
If there are alternate ways of achieving the same or a better algorithm, please let me know.
 This might be more appropriate for the Theoretical Computer Science Stack Exchange, since it not a question about implementation so much as theory. ...and stackoverflow.com/questions/6353537/k-means-algorithm and stackoverflow.com/questions/6212690/… This question gets asked quite a lot I've answered a similar Q with half a dozen methods (using R) over here: stackoverflow.com/a/15376462/1036500 Maybe you should find cluster centers with subtractive clustering ? Basic concept of this algorithm was presented in: link it is for matlab but should be good enough.
One approach is cross-validation. 
In essence, you pick a subset of your data and cluster it into k clusters, and you ask how well it clusters, compared with the rest of the data: Are you assigning data points to the same cluster memberships, or are they falling into different clusters? 
If the memberships are roughly the same, the data fit well into k clusters. Otherwise, you try a different k.
Also, you could do PCA (principal component analysis) to reduce your 50 dimensions to some more tractable number. If a PCA run suggests that most of your variance is coming from, say, 4 out of the 50 dimensions, then you can pick k on that basis, to explore how the four cluster memberships are assigned.
 What is the link between the number of dimensions and the number of clusters?  I can easily build a 1 dimensional distribution with k clusters for arbitrary K. "If the memberships are roughly the same" -- this assumes the data is divided evenly into clusters, which is quite a strong assumption. What do you mean by "the same cluster memberships"? Do you compare the clustering on the training folds with the clustering on the test fold? If so, I'm not sure how you can compare them, since they have completely non overlapping data points.
Take a look at this wikipedia page on determining the number of clusters in a data set. 
Also you might want to try Agglomerative hierarchical clustering out. This approach does not need to know the number of clusters, it will incrementally form clusters of cluster till only one exists. This technique also exists in SciPy (scipy.cluster.hierarchy). 

One interesting approach is that of evidence accumulation by Fred and Jain. This is based on combining multiple runs of k-means with a large number of clusters, aggregating them into an overall solution. Nice aspects of the approach include that the number of clusters is determined in the process and that the final clusters don't have to be spherical.

There are visualization that should hint good parameters. For k-means you could visualize several runs with different k using Graphgrams (see the WEKA graphgram package - best obtained by the package manager or here. An introduction and examples can also be found here.

You should also make sure that each dimension is in fact independent. Many so called multi-dimensional datasets have multiple representations of the same thing.
It is not wrong to have these in your data. It is wrong to use multiple versions of the same thing as support for a cluster argument.
http://en.wikipedia.org/wiki/Cronbach's_alpha

One way to do it is to run k-means with large k (much larger than what you think is the correct number), say 1000. then, running mean-shift algorithm on the these 1000 point (mean shift uses the whole data but you will only "move" these 1000 points). mean shift will find the amount of clusters then.
Running mean shift without the k-means before is a possibility but it is just too slow usually O(N^2*#steps), so running k-means before will speed things up: O(NK#steps)

If the cluster number is unknow, why not use Hierarchical Clustering instead?
At the begining, every isolated one is a cluster, then every two cluster will be merged if their distance is lower than a threshold, the algorithm will end when no more merger goes.
The Hierarchical clustering algorithm can carry out a suitable "K" for your data.
How can I find the center of a cluster of data points? 
Let's say I plotted the position of a helicopter every day for the past year and came up with the following map:

Any human looking at this would be able to tell me that this helicopter is based out of Chicago.
How can I find the same result in code?
I'm looking for something like this:
$geoCodeArray = array([GET=http://pastebin.com/grVsbgL9]);
function findHome($geoCodeArray) {
    // magic
    return $geoCode;
}

Ultimately generating something like this:

UPDATE: Sample Dataset
Here's a map with a sample dataset: http://batchgeo.com/map/c3676fe29985f00e1605cd4f86920179
Here's a pastebin of 150 geocodes: http://pastebin.com/grVsbgL9
The above contains 150 geocodes.  The first 50 are in a few clusters close to Chicago.  The remaining are scattered throughout the country, including some small clusters in New York, Los Angeles, and San Francisco.
I have about a million (seriously) datasets like this that I'll need to iterate through and identify the most likely "home".  Your help is greatly appreciated.
UPDATE 2: Airplane switched to Helicopter
The airplane concept was drawing too much attention toward physical airports.  The coordinates can be anywhere in the world, not just airports.  Let's assume it's a super helicopter not bound by physics, fuel, or anything else.  It can land where it wants. ;)
 Can you share a link with such data? Sure.  Map: batchgeo.com/map/c3676fe29985f00e1605cd4f86920179  and Geocodes: pastebin.com/grVsbgL9 looking at the map I'm not able to judge whether the plane is based in Chicago or in San Francisco. I don't expect an algorithm to be better than me at this. Oh, wow, thanks Ryan!!! I really appreciate. Any human would be able to tell that helicopter has 20x the range of any known helicopter.
The following solution works even if the points are scattered all over the Earth, by converting latitude and longitude to Cartesian coordinates. It does a kind of KDE (kernel density estimation), but in a first pass the sum of kernels is evaluated only at the data points. The kernel should be chosen to fit the problem. In the code below it is what I could jokingly/presumptuously call a Trossian, i.e., 2-d²/h² for d≤h and h²/d² for d>h (where d is the Euclidean distance and h is the "bandwidth" $global_kernel_radius), but it could also be a Gaussian (e-d²/2h²), an Epanechnikov kernel (1-d²/h² for d<h, 0 otherwise), or another kernel. An optional second pass refines the search locally, either by summing an independent kernel on a local grid, or by calculating the centroid, in both cases in a surrounding defined by $local_grid_radius.
In essence, each point sums all the points it has around (including itself), weighing them more if they are closer (by the bell curve), and also weighing them by the optional weight array $w_arr. The winner is the point with the maximum sum. Once the winner has been found, the "home" we are looking for can be found by repeating the same process locally around the winner (using another bell curve), or it can be estimated to be the "center of mass" of all points within a given radius from the winner, where the radius can be zero.
The algorithm must be adapted to the problem by choosing the appropriate kernels, by choosing how to refine the search locally, and by tuning the parameters. For the example dataset, the Trossian kernel for the first pass and the Epanechnikov kernel for the second pass, with all 3 radii set to 30 mi and a grid step of 1 mi could be a good starting point, but only if the two sub-clusters of Chicago should be seen as one big cluster. Otherwise smaller radii must be chosen.
function find_home($lat_arr, $lng_arr, $global_kernel_radius,
                                       $local_kernel_radius,
                                       $local_grid_radius, // 0 for no 2nd pass
                                       $local_grid_step,   // 0 for centroid
                                       $units='mi',
                                       $w_arr=null)
{
   // for lat,lng <-> x,y,z see http://en.wikipedia.org/wiki/Geodetic_datum
   // for K and h see http://en.wikipedia.org/wiki/Kernel_density_estimation

   switch (strtolower($units)) {
      /*  */case 'nm' :
      /*or*/case 'nmi': $m_divisor = 1852;
      break;case  'mi': $m_divisor = 1609.344;
      break;case  'km': $m_divisor = 1000;
      break;case   'm': $m_divisor = 1;
      break;default: return false;
   }
   $a  = 6378137 / $m_divisor; // Earth semi-major axis      (WGS84)
   $e2 = 6.69437999014E-3;     // First eccentricity squared (WGS84)

   $lat_lng_count = count($lat_arr);
   if ( !$w_arr) {
      $w_arr = array_fill(0, $lat_lng_count, 1.0);
   }
   $x_arr = array();
   $y_arr = array();
   $z_arr = array();
   $rad = M_PI / 180;
   $one_e2 = 1 - $e2;
   for ($i = 0; $i < $lat_lng_count; $i++) {
      $lat = $lat_arr[$i];
      $lng = $lng_arr[$i];
      $sin_lat = sin($lat * $rad);
      $sin_lng = sin($lng * $rad);
      $cos_lat = cos($lat * $rad);
      $cos_lng = cos($lng * $rad);
      // height = 0 (!)
      $N = $a / sqrt(1 - $e2 * $sin_lat * $sin_lat);
      $x_arr[$i] = $N * $cos_lat * $cos_lng;
      $y_arr[$i] = $N * $cos_lat * $sin_lng;
      $z_arr[$i] = $N * $one_e2  * $sin_lat;
   }
   $h = $global_kernel_radius;
   $h2 = $h * $h;
   $max_K_sum     = -1;
   $max_K_sum_idx = -1;
   for ($i = 0; $i < $lat_lng_count; $i++) {
      $xi = $x_arr[$i];
      $yi = $y_arr[$i];
      $zi = $z_arr[$i];
      $K_sum  = 0;
      for ($j = 0; $j < $lat_lng_count; $j++) {
         $dx = $xi - $x_arr[$j];
         $dy = $yi - $y_arr[$j];
         $dz = $zi - $z_arr[$j];
         $d2 = $dx * $dx + $dy * $dy + $dz * $dz;
         $K_sum += $w_arr[$j] * ($d2 <= $h2 ? (2 - $d2 / $h2) : $h2 / $d2); // Trossian ;-)
         // $K_sum += $w_arr[$j] * exp(-0.5 * $d2 / $h2); // Gaussian
      }
      if ($max_K_sum < $K_sum) {
          $max_K_sum = $K_sum;
          $max_K_sum_i = $i;
      }
   }
   $winner_x   = $x_arr  [$max_K_sum_i];
   $winner_y   = $y_arr  [$max_K_sum_i];
   $winner_z   = $z_arr  [$max_K_sum_i];
   $winner_lat = $lat_arr[$max_K_sum_i];
   $winner_lng = $lng_arr[$max_K_sum_i];

   $sin_winner_lat = sin($winner_lat * $rad);
   $cos_winner_lat = cos($winner_lat * $rad);
   $sin_winner_lng = sin($winner_lng * $rad);
   $cos_winner_lng = cos($winner_lng * $rad);
   $east_x  = -$local_grid_step * $sin_winner_lng;
   $east_y  =  $local_grid_step * $cos_winner_lng;
   $east_z  =  0;
   $north_x = -$local_grid_step * $sin_winner_lat * $cos_winner_lng;
   $north_y = -$local_grid_step * $sin_winner_lat * $sin_winner_lng;
   $north_z =  $local_grid_step * $cos_winner_lat;

   if ($local_grid_radius > 0 && $local_grid_step > 0) {
      $r = intval($local_grid_radius / $local_grid_step);
      $r2 = $r * $r;
      $h = $local_kernel_radius;
      $h2 = $h * $h;
      $max_L_sum     = -1;
      $max_L_sum_idx = -1;
      for ($i = -$r; $i <= $r; $i++) {
         $winner_east_x = $winner_x + $i * $east_x;
         $winner_east_y = $winner_y + $i * $east_y;
         $winner_east_z = $winner_z + $i * $east_z;
         $j_max = intval(sqrt($r2 - $i * $i));
         for ($j = -$j_max; $j <= $j_max; $j++) {
            $x = $winner_east_x + $j * $north_x;
            $y = $winner_east_y + $j * $north_y;
            $z = $winner_east_z + $j * $north_z;
            $L_sum  = 0;
            for ($k = 0; $k < $lat_lng_count; $k++) {
               $dx = $x - $x_arr[$k];
               $dy = $y - $y_arr[$k];
               $dz = $z - $z_arr[$k];
               $d2 = $dx * $dx + $dy * $dy + $dz * $dz;
               if ($d2 < $h2) {
                  $L_sum += $w_arr[$k] * ($h2 - $d2); // Epanechnikov
               }
            }
            if ($max_L_sum < $L_sum) {
                $max_L_sum = $L_sum;
                $max_L_sum_i = $i;
                $max_L_sum_j = $j;
            }
         }
      }
      $x = $winner_x + $max_L_sum_i * $east_x + $max_L_sum_j * $north_x;
      $y = $winner_y + $max_L_sum_i * $east_y + $max_L_sum_j * $north_y;
      $z = $winner_z + $max_L_sum_i * $east_z + $max_L_sum_j * $north_z;

   } else if ($local_grid_radius > 0) {
      $r = $local_grid_radius;
      $r2 = $r * $r;
      $wx_sum = 0;
      $wy_sum = 0;
      $wz_sum = 0;
      $w_sum  = 0;
      for ($k = 0; $k < $lat_lng_count; $k++) {
         $xk = $x_arr[$k];
         $yk = $y_arr[$k];
         $zk = $z_arr[$k];
         $dx = $winner_x - $xk;
         $dy = $winner_y - $yk;
         $dz = $winner_z - $zk;
         $d2 = $dx * $dx + $dy * $dy + $dz * $dz;
         if ($d2 <= $r2) {
            $wk = $w_arr[$k];
            $wx_sum += $wk * $xk;
            $wy_sum += $wk * $yk;
            $wz_sum += $wk * $zk;
            $w_sum  += $wk;
         }
      }
      $x = $wx_sum / $w_sum;
      $y = $wy_sum / $w_sum;
      $z = $wz_sum / $w_sum;
      $max_L_sum_i = false;
      $max_L_sum_j = false;

   } else {
      return array($winner_lat, $winner_lng, $max_K_sum_i, false, false);
   }

   $deg = 180 / M_PI;
   $a2 = $a * $a;
   $e4 = $e2 * $e2;
   $p = sqrt($x * $x + $y * $y);
   $zeta = (1 - $e2) * $z * $z / $a2;
   $rho  = ($p * $p / $a2 + $zeta - $e4) / 6;
   $rho3 = $rho * $rho * $rho;
   $s = $e4 * $zeta * $p * $p / (4 * $a2);
   $t = pow($s + $rho3 + sqrt($s * ($s + 2 * $rho3)), 1 / 3);
   $u = $rho + $t + $rho * $rho / $t;
   $v = sqrt($u * $u + $e4 * $zeta);
   $w = $e2 * ($u + $v - $zeta) / (2 * $v);
   $k = 1 + $e2 * (sqrt($u + $v + $w * $w) + $w) / ($u + $v);
   $lat = atan($k * $z / $p) * $deg;
   $lng = atan2($y, $x) * $deg;

   return array($lat, $lng, $max_K_sum_i, $max_L_sum_i, $max_L_sum_j);
}

The fact that distances are Euclidean and not great-circle should have negligible effects for the task at hand. Calculating great-circle distances would be much more cumbersome, and would cause only the weight of very far points to be significantly lower - but these points already have a very low weight. In principle, the same effect could be achieved by a different kernel. Kernels that have a complete cut-off beyond some distance, like the Epanechnikov kernel, don't have this problem at all (in practice).
The conversion between lat,lng and x,y,z for the WGS84 datum is given exactly (although without guarantee of numerical stability) more as a reference than because of a true need. If the height is to be taken into account, or if a faster back-conversion is needed, please refer to the Wikipedia article.
The Epanechnikov kernel, besides being "more local" than the Gaussian and Trossian kernels, has the advantage of being the fastest for the second loop, which is O(ng), where g is the number of points of the local grid, and can also be employed in the first loop, which is O(n²), if n is big.
 @Ryan: $K_sum += $w_arr[$j] * ($d2 <= $h2 ? 1 : $h2 / $d2); is the kernel of my previous answer (the "canyon peak" kernel). The results should be slightly different only because coordinates are 3D instead of the planar approximation. @Ryan: thanks a lot for your bounty, I'm really happy this algorithm has stood your tests! As for the lack of upvotes, I think I'll have to get used to it (not only in Stackoverflow) - in Italy we say: "laughs well who laughs last" I was just thinking about your lack of upvotes. ;)  Rossmo's gets a lot of "interesting" points in my book, but I ultimately couldn't get a working model with which I was satisfied.  But I expect this to be purely user error.  They say I can't give the next 200pts bounty "tip" for another 12 hours.  Until then, thanks for all the help.
This can be solved by finding a jeopardy surface. See Rossmo's Formula.
This is the predator problem. Given a set of geographically-located carcasses, where is the lair of the predator? Rossmo's formula solves this problem.
 That's fascinating. Can you help me understand how I might translate this formula into some pseudo-programming-code? I'll be ultimately using PHP, but if I can get a handle on how to translate that formula to a programming language, I can figure the rest out. Rossmo's formula gives you the probability that the predator lives at any particular point (x,y). So, what you would do is divide the area into zones like invisal has, then compute the probability for the point in the center of each zone. Then, having narrowed the location down to a zone, you would subdivide the zone and repeat the process. Note that in the normal Rossmo formula Manhatten distance is used (for city streets), but in this application you can use normal Euclidean distance. Thank Tyler, but I'm still stuck on how to translate the formula on the Wikipedia page into code. Can you point me in the right direction for that? I wonder how on Earth (or, at least, on the USA) Rossmo's formula can be rephrased for Euclidean distances @Azhrilla tried to convert Rossmo's Formula into code below - stackoverflow.com/a/17274433/834525  Unfortunately I'm having trouble getting it to work in PHP, but for others looking into this question, take a look further down the page.
Find the point with the largest density estimate.
Should be pretty much straightforward. Use a kernel radius that roughly covers a large airport in diameter. A 2D Gaussian or Epanechnikov kernel should be fine.
http://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation
This is similar to computing a Heap Map: http://en.wikipedia.org/wiki/Heat_map
and then finding the brightest spot there. Except it computes the brightness right away.
For fun I read a 1% sample of the Geocoordinates of DBpedia (i.e. Wikipedia) into ELKI, projected it into 3D space and enabled the density estimation overlay (hidden in the visualizers scatterplot menu). You can see there is a hotspot on Europe, and to a lesser extend in the US. The hotspot in Europe is Poland I believe. Last I checked, someone apparently had created a Wikipedia article with Geocoordinates for pretty much any town in Poland. The ELKI visualizer, unfortunately, neither allows you to zoom in, rotate, or reduce the kernel bandwidth to visually find the most dense point. But it's straightforward to implement yourself; you probably also don't need to go into 3D space, but can just use latitudes and longitudes.

Kernel Density Estimation should be available in tons of applications. The one in R is probably much more powerful. I just recently discovered this heatmap in ELKI, so I knew how to quickly access it. See e.g. http://stat.ethz.ch/R-manual/R-devel/library/stats/html/density.html for a related R function.
On your data, in R, try for example:
library(kernSmooth)
smoothScatter(data, nbin=512, bandwidth=c(.25,.25))

this should show a strong preference for Chicago.
library(kernSmooth)
dens=bkde2D(data, gridsize=c(512, 512), bandwidth=c(.25,.25))
contour(dens$x1, dens$x2, dens$fhat)
maxpos = which(dens$fhat == max(dens$fhat), arr.ind=TRUE)
c(dens$x1[maxpos[1]], dens$x2[maxpos[2]])

yields [1]  42.14697 -88.09508, which is less than 10 miles from Chicago airport.
To get better coordinates try:

rerunning on a 20x20 miles area around the estimated coordinates
a non-binned KDE in that area
better bandwidth selection with dpik
higher grid resolution

 Thanks. I've read through your answer previously but had a hard time getting started with some actual code. I will try to run this today to see how it performs. I'm also building the importer I need to test multiple data sets to ensure reliability over many cases. Thanks again for the further effort and clarification.
in Astrophysics we use the so called "half mass radius". Given a distribution and its center, the half mass radius is the minimum radius of a circle that  contains half of the points of your distribution.
This quantity is a characteristic length of a distribution of points.
If you want that the home of the helicopter is where the points are maximally concentrated so it is the point that has the minimum half mass radius!
My algorithm is as follows: for each point you compute this half mass radius centring the distribution in the current point. The "home" of the helicopter will be the point with the minimum half mass radius.
I've implemented it and the computed center is 42.149994 -88.133698 (which is in Chicago)
I've also used the 0.2 of the  total mass instead of the 0.5(half) usually used in Astrophysics.
This is my (in python) alghorithm that finds the home of the helicopter:
import math

import numpy

def inside(points,center,radius):
     ids=(((points[:,0]-center[0])**2.+(points[:,1]-center[1])**2.)<=radius**2.)
     return points[ids]

points = numpy.loadtxt(open('points.txt'),comments='#')

npoints=len(points)
deltar=0.1

idcenter=None
halfrmin=None

for i in xrange(0,npoints):
    center=points[i]
    radius=0.

    stayHere=True
    while stayHere:
         radius=radius+deltar
         ninside=len(inside(points,center,radius))
         #print 'point',i,'r',radius,'in',ninside,'center',center
         if(ninside>=npoints*0.2):
              if(halfrmin==None or radius<halfrmin):
                   halfrmin=radius
                   idcenter=i
                   print 'point',i,halfrmin,idcenter,points[idcenter]
              stayHere=False

#print halfrmin,idcenter
print points[idcenter]

 Can you clarify your def inside... lines?  I'm translating this to PHP and having a hard time understanding that part.  Does this function just return the number of points inside the radius? I'm intrigued by this algorithm and would like to try it out. Can you look at my code and help me identify the problem? I am a little unclear on the constraints of the loops, and where it breaks.  Here's my translation to PHP: codepad.org/MMS0bOeq Can you take a look and let me know what I'm misinterpreting? Hi, I don't have PHP at work and I'll try your code at home. Why did you use such a strange metric for the distance? However I think there is a typo in cos(radians($long2) - radians($long2)) which is zero. BTW your guess about def inside is right, it returns the points inside a certain circle of radius radius and centered on center. Thanks.  The distance accounts for the 3D earth as opposed to a 2D surface. I'll look into the typo (I'm not sure if it should be long1-long2 or long2-long1).  Thanks for the heads up.  I was mostly confused about the loops in your python code.  I don't understand where the loops start and end, so I was confused bringing it to PHP.  Any further help is appreciated. I've found your second typo: $coordinateArr[$centroidKey][0] . ',' . $coordinateArr[$centroidKey][1]. But there is something tremendously scary in your code: You have multipied the $distance by 3959 (I guess Earth radius in [mi]), in this case  deltar=0.1 is too small! This is my code working: codepad.org/9Ro4lcQQ . BTW: I've choosen the Earth radius equal 1 in my code. About the cycle: It stops on if(ninside>=npoints*0.2): because the 0.2-mass-radius is found, and the code just search for the minimum one.
You can use DBSCAN for that task. 
DBSCAN is a density based clustering with a notion of noise. You need two parameters: 
First the number of points a cluster should have at minimum "minpoints". 
And second a neighbourhood parameter called "epsilon" that sets a distance threshold to the surrounding points that should be included in your cluster.
The whole algorithm works like this:

Start with an arbitrary point in your set that hasn't been visited yet
Retrieve points from the epsilon neighbourhood mark all as visited

if you have found enough points in this neighbourhood (> minpoints parameter) you start a new cluster and assign those points. Now recurse into step 2 again for every point in this cluster.
if you don't have, declare this point as noise 

go all over again until you've visited all points

It is really simple to implement and there are lots of frameworks that support this algorithm already. To find the mean of your cluster, you can simply take the mean of all the assigned points from its neighbourhood. 
However, unlike the method that @TylerDurden proposes, this needs a parameterization- so you need to find some hand tuned parameters that fit your problem. 
In your case, you can try to set the minpoints to 10% of your total points if the plane is likely to stay 10% of the time you track at an airport. The density parameter epsilon depends on the resolution of your geographic sensor and the distance metric you use- I would suggest the haversine distance for geographic data. 
 That's cool. Makes sense. Something like: $epsilon = 10; foreach($geoCodeArray as $geocode) { $count = neighborsWithin($epsilon,$geocode); } // sort by geocode, count DESC.  Not bad. Actually, just plain density estimation makes more sense here than DBSCAN. Because he actually doesn't want to cluster, but find the density maximum only. And KDE is a bit more clever than the ad-hoc density estimate DBSCAN uses.

How about divide the map into many zones and then find the center of plane in zone with the most plane. Algorithm will be something like this

set Zones[40]
foreach Plane in Planes
   Zones[GetZone(Plane.position)].Add(Plane)

set MaxZone = Zones[0]
foreach Zone in Zones
   if MaxZone.Length() < Zone.Length()
           MaxZone = Zone

set Center
foreach Plane in MaxZone
     Center.X += Plane.X
     Center.Y += Plane.Y
Center.X /= MaxZone.Length
Center.Y /= MaxZone.Length

 what if the place we are looking for is exactly where the corners of four zones meet? +1 for the algorithm. @WalterTross you could make a variable size mesh or even simpler: run the same code as above but with starting point in X/Y say 1/10th of the square. Store all densities in a List, then find the maximum one and retrieve the position of X, Y.
All I have on this machine is an old compiler so I made an ASCII version of this. It "draws" (in ASCII) a map - dots are points, X is where the real source is, G is where the guessed source is. If the two overlap, only X is shown.
Examples (DIFFICULTY 1.5 and 3 respectively):

The points are generated by picking a random point as the source, then randomly distributing points, making them more likely to be closer to the source.
DIFFICULTY is a floating point constant that regulates the initial point generation - how much more likely the points are to be closer to the source - if it is 1 or less, the program should be able to guess the exact source, or very close. At 2.5, it should still be pretty decent. At 4+, it will start to guess worse, but I think it still guesses better than a human would.
It could be optimized by using binary search over X, then Y - this would make the guess worse, but would be much, much faster. Or by starting with larger blocks, then splitting the best block further (or the best block and the 8 surrounding it). For a higher resolution system, one of these would be necessary. This is quite a naive approach, though, but it seems to work well in an 80x24 system. :D
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <math.h>

#define Y 24
#define X 80

#define DIFFICULTY 1 // Try different values... 

static int point[Y][X];

double dist(int x1, int y1, int x2, int y2)
{
    return sqrt((y1 - y2)*(y1 - y2) + (x1 - x2)*(x1 - x2));
}

main()
{
    srand(time(0));
    int y = rand()%Y;
    int x = rand()%X;

    // Generate points
    for (int i = 0; i < Y; i++)
    {
        for (int j = 0; j < X; j++)
        {
            double u = DIFFICULTY * pow(dist(x, y, j, i), 1.0 / DIFFICULTY);
            if ((int)u == 0)
                u = 1;
            point[i][j] = !(rand()%(int)u);
        }
    }

    // Find best source
    int maxX = -1;
    int maxY = -1;
    double maxScore = -1;
    for (int cy = 0; cy < Y; cy++)
    {
        for (int cx = 0; cx < X; cx++)
        {
            double score = 0;
            for (int i = 0; i < Y; i++)
            {
                for (int j = 0; j < X; j++)
                {
                    if (point[i][j] == 1)
                    {
                        double d = dist(cx, cy, j, i);
                        if (d == 0)
                            d = 0.5;
                        score += 1000 / d;
                    }
                }
            }
            if (score > maxScore || maxScore == -1)
            {
                maxScore = score;
                maxX = cx;
                maxY = cy;
            }
        }
    }

    // Print out results
    for (int i = 0; i < Y; i++)
    {
        for (int j = 0; j < X; j++)
        {
            if (i == y && j == x)
                printf("X");
            else if (i == maxY && j == maxX)
                printf("G");            
            else if (point[i][j] == 0)
                printf(" ");
            else if (point[i][j] == 1)
                printf(".");
        }
    }
    printf("Distance from real source: %f", dist(maxX, maxY, x, y));

    scanf("%d", 0);

} 


Virtual earth has a very good explanation of how you can do it relatively quick. They also have provided code examples. Please have a look at http://soulsolutions.com.au/Articles/ClusteringVirtualEarthPart1.aspx 
 There's actually some solid code underneath that link.  Thanks.  It will take some time to go through and test, but I appreciate the help.
A simple mixture model seems to work pretty well for this problem. 
In general, to get a point that minimizes the distance to all other points in a dataset, you can just take the mean. In this case, you want to find a point that minimizes the distance from a subset of concentrated points. If you postulate that a point can either come from the concentrated set of points of interest or from a diffuse set of background points, then this gives a mixture model.
I have included some python code below. The concentrated area is modeled by a high-precision normal distribution and the background point are modeled by either a low-precision normal distribution or a uniform distribution over a bounding box on the dataset (there is a line of code that can be commented out to switch between these options). Also, mixture models can be somewhat unstable, so running the EM algorithm a few times with random initial conditions and choosing the run with the highest log-likelihood gives better results.
If you are actually looking at airplanes, then adding some sort of time dependent dynamics will probably improve your ability to infer the home base immensely.
I would also be wary of Rossimo's formula because it includes some pretty-strong assumptions about crime distributions.
#the dataset
sdata='''41.892694,-87.670898
42.056048,-88.000488
41.941744,-88.000488
42.072361,-88.209229
42.091933,-87.982635
42.149994,-88.133698
42.171371,-88.286133
42.23241,-88.305359
42.196811,-88.099365
42.189689,-88.188629
42.17646,-88.173523
42.180531,-88.209229
42.18168,-88.187943
42.185496,-88.166656
42.170485,-88.150864
42.150634,-88.140564
42.156743,-88.123741
42.118555,-88.105545
42.121356,-88.112755
42.115499,-88.102112
42.119319,-88.112411
42.118046,-88.110695
42.117791,-88.109322
42.182189,-88.182449
42.194145,-88.183823
42.189057,-88.196182
42.186513,-88.200645
42.180917,-88.197899
42.178881,-88.192062
41.881656,-87.6297
41.875521,-87.6297
41.87872,-87.636566
41.872073,-87.62661
41.868239,-87.634506
41.86875,-87.624893
41.883065,-87.62352
41.881021,-87.619743
41.879998,-87.620087
41.8915,-87.633476
41.875163,-87.620773
41.879125,-87.62558
41.862763,-87.608757
41.858672,-87.607899
41.865192,-87.615795
41.87005,-87.62043
42.073061,-87.973022
42.317241,-88.187256
42.272546,-88.088379
42.244086,-87.890625
42.044512,-88.28064
39.754977,-86.154785
39.754977,-89.648437
41.043369,-85.12207
43.050074,-89.406738
43.082179,-87.912598
42.7281,-84.572754
39.974226,-83.056641
38.888093,-77.01416
39.923692,-75.168457
40.794318,-73.959961
40.877439,-73.146973
40.611086,-73.740234
40.627764,-73.234863
41.784881,-71.367187
42.371988,-70.993652
35.224587,-80.793457
36.753465,-76.069336
39.263361,-76.530762
25.737127,-80.222168
26.644083,-81.958008
30.50223,-87.275391
29.436309,-98.525391
30.217839,-97.844238
29.742023,-95.361328
31.500409,-97.163086
32.691688,-96.877441
32.691688,-97.404785
35.095754,-106.655273
33.425138,-112.104492
32.873244,-117.114258
33.973545,-118.256836
33.681497,-117.905273
33.622982,-117.734985
33.741828,-118.092041
33.64585,-117.861328
33.700707,-118.015137
33.801189,-118.251343
33.513132,-117.740479
32.777244,-117.235107
32.707939,-117.158203
32.703317,-117.268066
32.610821,-117.075806
34.419726,-119.701538
37.750358,-122.431641
37.50673,-122.387695
37.174817,-121.904297
37.157307,-122.321777
37.271492,-122.033386
37.435238,-122.217407
37.687794,-122.415161
37.542025,-122.299805
37.609506,-122.398682
37.544203,-122.0224
37.422151,-122.13501
37.395971,-122.080078
45.485651,-122.739258
47.719463,-122.255859
47.303913,-122.607422
45.176713,-122.167969
39.566,-104.985352
39.124201,-94.614258
35.454518,-97.426758
38.473482,-90.175781
45.021612,-93.251953
42.417881,-83.056641
41.371141,-81.782227
33.791132,-84.331055
30.252543,-90.439453
37.421248,-122.174835
37.47794,-122.181702
37.510628,-122.254486
37.56943,-122.346497
37.593373,-122.384949
37.620571,-122.489319
36.984249,-122.03064
36.553017,-121.893311
36.654442,-121.772461
36.482381,-121.876831
36.15042,-121.651611
36.274518,-121.838379
37.817717,-119.569702
39.31657,-120.140991
38.933041,-119.992676
39.13785,-119.778442
39.108019,-120.239868
38.586082,-121.503296
38.723354,-121.289062
37.878444,-119.437866
37.782994,-119.470825
37.973771,-119.685059
39.001377,-120.17395
40.709076,-73.948975
40.846346,-73.861084
40.780452,-73.959961
40.778829,-73.958931
40.78372,-73.966012
40.783688,-73.965325
40.783692,-73.965615
40.783675,-73.965741
40.783835,-73.965873
'''

import StringIO
import numpy as np
import re

import matplotlib.pyplot as plt

def lp(l):
    return map(lambda m: float(m.group()),re.finditer('[^, \n]+',l))

data=np.array(map(lp,StringIO.StringIO(sdata)))

xmn=np.min(data[:,0])
xmx=np.max(data[:,0])
ymn=np.min(data[:,1])
ymx=np.max(data[:,1])

# area of the point set bounding box
area=(xmx-xmn)*(ymx-ymn)

M_ITER=100 #maximum number of iterations
THRESH=1e-10 # stopping threshold

def em(x):
    print '\nSTART EM'
    mlst=[]

    mu0=np.mean( data , 0 ) # the sample mean of the data - use this as the mean of the low-precision gaussian

    # the mean of the high-precision Gaussian - this is what we are looking for
    mu=np.random.rand( 2 )*np.array([xmx-xmn,ymx-ymn])+np.array([xmn,ymn])

    lam_lo=.001  # precision of the low-precision Gaussian
    lam_hi=.1 # precision of the high-precision Gaussian
    prz=np.random.rand( 1 ) # probability of choosing the high-precision Gaussian mixture component

    for i in xrange(M_ITER):
        mlst.append(mu[:])

        l_hi=np.log(prz)+np.log(lam_hi)-.5*lam_hi*np.sum((x-mu)**2,1)
        #low-precision normal background distribution
        l_lo=np.log(1.0-prz)+np.log(lam_lo)-.5*lam_lo*np.sum((x-mu0)**2,1)
        #uncomment for the uniform background distribution
        #l_lo=np.log(1.0-prz)-np.log(area)

        #expectation step
        zs=1.0/(1.0+np.exp(l_lo-l_hi))

        #compute bound on the likelihood 
        lh=np.sum(zs*l_hi+(1.0-zs)*l_lo)
        print i,lh

        #maximization step
        mu=np.sum(zs[:,None]*x,0)/np.sum(zs) #mean
        lam_hi=np.sum(zs)/np.sum(zs*.5*np.sum((x-mu)**2,1)) #precision
        prz=1.0/(1.0+np.sum(1.0-zs)/np.sum(zs)) #mixure component probability

        try:
            if np.abs((lh-old_lh)/lh)<THRESH:
                break
        except: 
            pass

        old_lh=lh

        mlst.append(mu[:])

    return lh,lam_hi,mlst    

if __name__=='__main__':

    #repeat the EM algorithm a number of times and get the run with the best log likelihood
    mx_prm=em(data)
    for i in xrange(4):
        prm=em(data)

        if prm[0]>mx_prm[0]:
            mx_prm=prm

        print prm[0]
        print mx_prm[0]

    lh,lam_hi,mlst=mx_prm
    mu=mlst[-1]

    print 'best loglikelihood:', lh
    #print 'final precision value:', lam_hi
    print 'point of interest:', mu
    plt.plot(data[:,0],data[:,1],'.b')

    for m in mlst:
        plt.plot(m[0],m[1],'xr')

    plt.show()


You can easily adapt the Rossmo's formula, quoted by Tyler Durden to your case with few simple notes:
The formula :
This formula give something close to a probability of presence of the base operation for a predator or a serial killer. In your case it could give the probability of a base to be in a certain point. I'll explain later how to use it.  U can write it this way :
Proba(base on point A)= Sum{on all spots} ( Phi/(dist^f)+(1-Phi)(B*(g-f))/(2B-dist)^g )
Using Euclidian distance
You want an Euclidian distance and not the Manhattan's one because an airplane or helicopter is not bound to road/streets. So using Euclidian distance is the correct way, if you are tracking an airplane & not a serial killer. So "dist" in the formula is the euclidian distance between the spot ur testing and the spot considered
Taking reasonable variable B
Variable B was used to represent the rule "reasonably smart killer will not kill his neighbor". In your case the will also applied because no one use an airplane/roflcopter to get to the next street corner. we can suppose that the minimal journey is for example 10km or anything reasonable when applied to your case.
Exponential factor f
Factor f is used to add a weight to the distance. For example if all the spots are in a small area you could want a big factor f because the probability of the airport/base/HQ will decrease fast if all your datapoint are in the same sector. g works in a similar way, allow to choose the size of "base is unlikely to be just next to the spot" area
Factor Phi :
Again this factor has to be determined using your knowledge of the problem. It permits to choose the most accurate factor between "base is close to spots" and "i'll not use the plane to make 5 m" if for example u think that the second one is almost irrelevent you can set Phi to 0.95 (0<Phi<1) If both are interesting phi will be around 0.5
How to implement it as something usefull :
First you want to divide your map into little squares : meshing the map ( just like invisal did) (the smaller the squares ,the more accurate the result (in general)) then using the formula to find the more probable location. In fact the mesh is just an array with all possible locations. (if u want to be accurate you increase the number of possible spots but it will require more computational time and PhP is not well-known for it's amazing speed)
Algorithm :
//define all the factors you need(B , f , g , phi)

for(i=0..mesh_size) // computing the probability of presence for each square of the mesh
{
  P(i)=0;
  geocode squarePosition;//GeoCode of the square's center 
  for(j=0..geocodearray_size)//sum on all the known spots
  {
     dist=Distance(geocodearray[j],squarePosition);//small function returning distance between two geocodes

         P(i)+=(Phi/pow(dist,f))+(1-Phi)*pow(B,g-f)/pow(2B-dist,g);
  }
 }

return geocode corresponding to max(P(i))

Hope that it will help you

First I would like to express my fondness of your method in illustrating and explaining the problem .. 
If I were in your shoes, I would go for a density based algorithm like DBSCAN
and then after clustering the areas and removing the noise points a few areas (choices) will remain .. then I'll take the cluster with the highest density of points and calculate the average point and find the nearest real point to it . done, found the place! :).
Regards,
 Thanks.  I haven't gotten around to translating the DBSCAN approach into a php script to test on my data sets yet, but I will get to it. tell me how it goes :)
Why not something like this:

For each point, calculate it's distance from all other points and sum the total.
The point with the smallest sum is your center.

Maybe sum isn't the best metric to use. Possibly the point with the most "small distances"?
 What is the "point with the smallest point"? That doesn't make any sense. sorry typo, fixed now. though i'm still contemplating how well this will work. The problem with this is that it does not identify the center, it identifies the point that is closest to the center of gravity of all the points, but the problem is that the center of gravity of the points may not be near the actual base. For example, if you use your algorithm on the points shown in the post it will result in one of the points in Iowa being chosen--a completely wrong answer. The problem here is you think the center of gravity of the points is the base of the predator, but that is not true. The predator's motion is not symmetric to his base but will go longer distances in certain directions. Once you have the point closest to the center, you can do the same (for some area around this point) for all sector-point distances.
Sum over the distances. Take the point with the smallest summed distance.
function () {
    for i in points P:
        S[i] = 0
        for j in points P:
            S[i] += distance(P[i], P[j])
    return min(S);
}

 Once you have the point closest to the center, you can do the same (for some area around this point) for all sector-point distances. This function looked good for several data sets, but unfortunately proved unworkable when data had clusters on east and west with a couple points in the middle.  In this case the middle was preferred, despite it clearly not the "home". Similar to @Tyler-Durden's comment above on DrewKhoury's answer.  Thanks. I think en.wikipedia.org/wiki/Hierarchical_clustering might be what you are looking for. Nice work on the fiddle!  If you have time I'd love to learn more about your hierarchical clustering strategy.
You can take a minimum spanning tree and remove the longest edges. The smaller trees give you the centeroid to lookup. The algorithm name is single-link k-clustering. There is a post here: https://stats.stackexchange.com/questions/1475/visualization-software-for-clustering.
 Can you clarify a bit more?  For example, how do you find the "minimum spanning tree"?  Is this just another way to say the point with the minimum distance to all the other points?  For example, the "minimum spanning tree" of a map with 100 points in California and 100 points in New York and 1 point in Iowa would yield the point in Iowa as the MST? A minimum spanning  tree is a tree datastucture.  Think of a water or electricity grid of a city. The connected vertices are minimized. When remove long edges  from the tree it gives you smaller trees.Python Implementation of OPTICS (Clustering) Algorithm 
I'm looking for a decent implementation of the OPTICS algorithm in Python. I will use it to form density-based clusters of points ((x,y) pairs).
I'm looking for something that takes in (x,y) pairs and outputs a list of clusters, where each cluster in the list contains a list of (x, y) pairs belonging to that cluster.
 did you look at SciPy: docs.scipy.org/doc/scipy/reference/cluster.html? Is OPTICS such an unfamiliar/unknown algorithm that my question gets no attention? =( @pyrony - I'm looking for a density-based clustering approach. Furthermore, specifying the 'k' value in K-Means is ill-suited for the kind of task I'm working on. There is some psudocode in the linked wikipedea article, have you tried translating this? If so what problems did you encounter? For me it would be the first option - it's the best way to learn more about the implementation you eventually end up using...
EDIT: the following is known to not be a complete implementation of OPTICS.
I did a quick search and found the following (Optics). I can't vouch for its quality, however the algorithm seems pretty simple, so you should be able to validate/adapt it quickly.
Here is a quick example of how to build clusters on the output of the optics algorithm:
def cluster(order, distance, points, threshold):
    ''' Given the output of the options algorithm,
    compute the clusters:

    @param order The order of the points
    @param distance The relative distances of the points
    @param points The actual points
    @param threshold The threshold value to cluster on
    @returns A list of cluster groups
    '''
    clusters = [[]]
    points   = sorted(zip(order, distance, points))
    splits   = ((v > threshold, p) for i,v,p in points)
    for iscluster, point in splits: 
        if iscluster: clusters[-1].append(point)
        elif len(clusters[-1]) > 0: clusters.append([])
    return clusters

    rd, cd, order = optics(points, 4)
    print cluster(order, rd, points, 38.0)

 Thanks Bashwork but that seems the exact same code as what vartec has suggested. The problem with that is, I couldn't figure out how I would extract the clustering structure (which elements belongs to which cluster) from the output of that algorithm. Please have a look at the 'Note' at the very bottom of my question. So the code gives you the output you need to extract clusters (the order and the reachability distance).  If you look at the wikipedia section for extracting clusters, you simply need to walk the ordered results with a threshold on the distances (lower threshold means more clusters). (en.wikipedia.org/wiki/OPTICS_algorithm).  If this doesn't make sense I can give some example code. I just ran the code you have posted and the result I get with a threshold of 38 is [[31.0, 87.0], [73.0, 9.0]] [[5.0, 8.0]] [[97.0, 9.0]] (3 clusters). I lower the threshold to 10 and there is only 1 cluster. The test data I used is the same as the one used in the link you gave (testX). I would appreciate it if you can correct the code and I'll award your bounty. Based on my understanding of the algorithm, those results are correct as a cluster is created every time the ordered collection descends below the given threshold. In the case of 38, there are three valleys while in the case of 10 there is only one (the zero result). The threshold basically controls what should be considered a valley. I would take all this here with a grain of salt. It diverts considerably from my understanding of this algorithm. You might want to get the official implementation (not in python) and compare the results.
I'm not aware of a complete and exact python implementation of OPTICS. The links posted here seem just rough approximations of the OPTICS idea. They also do not use an index for acceleration, so they will run in O(n^2) or more likely even O(n^3).
OPTICS has a number of tricky things besides the obvious idea. In particular, the thresholding  is proposed to be done with relative thresholds ("xi") instead of absolute thresholds as posted here (at which point the result will be approximately that of DBSCAN!).
The original OPTICS paper contains a suggested approach to converting the algorithm's output into actual clusters:
http://www.dbs.informatik.uni-muenchen.de/Publikationen/Papers/OPTICS.pdf
The OPTICS implementation in Weka is essentially unmaintained and just as incomplete. It doesn't actually produce clusters, it only computes the cluster order. For this it makes a duplicate of the database - it isn't really Weka code.
There seems to be a rather extensive implementation available in ELKI in Java by the group that published OPTICS in the first place. You might want to test any other implementation against this "official" version.
 Indeed, there are a lots of incomplete OPTICS implementations and clones of the Weka version around. You should take the ELKI version as reference. The relative thresholding is to my mind the point where a relatively clear exposition and approach transition into something much more cloudy, with additional heuristics and hidden parameters. There is probably no way around this, but I definitely feel that the intermediate ordered reachability values constitute a nice result that stands on its own. What happens afterwards is open to different approaches, and the one chosen in this paper is not so self-evident as to be the only one worth considering. There are at least two more methods proposed for how to exact clusters from the plot. Yet, without such a cluster extraction method, is it actually a clustering algorithm? At some point, you do want to get clusters out of it, not just a plot. Note that absolute thresholding means you are doing DBSCAN, not OPTICS. Just more complicated and thus slower.
While not technically OPTICS there is an HDBSCAN* implementation for python available at https://github.com/lmcinnes/hdbscan . This is equivalent to OPTICS with an infinite maximal epsilon, and a different cluster extraction method. Since the implementation provides access to the generated cluster hierarchy you can extract clusters from that via more traditional OPTICS methods as well if you would prefer.
Note that despite not limiting the epsilon parameter this implementation still achieves O(n log(n)) performance using kd-tree and ball-tree based minimal spanning tree algorithms, and can handle quite large datasets.

There now exists the library pyclustering that contains, amongst others, a Python and a C++ implementation of OPTICS.

It is now implemented in the development version (scikit-learn v0.21.dev0) of sklearn (a clustering and maschine learning module for python)
here is the link:
https://scikit-learn.org/dev/modules/generated/sklearn.cluster.OPTICS.html

See "Density-based clustering approaches" on 
http://www.chemometria.us.edu.pl/index.php?goto=downloads
 Thanks for the answer vartec, but the implementation seemed incomplete to me. I'm looking for something that takes in (x,y) pairs and outputs a list of clusters, where each cluster in the list contains a list of (x,y) pairs belonging to that cluster.
You want to look at a space-filling-curve or a spatial index. A sfc reduce the 2d complexity to a 1d complexity. You want to look at Nick's hilbert curve quadtree spatial index blog. You want to download my implementation of a sfc at phpclasses.org (hilbert-curve).
 Thanks epitaph but how exactly does this answer my question? Can you clarify your answer a bit more? A sfc is a clustering algorithm using a fractal. A hilbert curve has a fractal dimension of 2. If you have 2d data you can easily subdivide this data into smaller tiles. Basically it's a reordering. It's like storing them in a quadtree. You can also use an adaptive sfc where emtpy regions are skipped or have a lower granularity of the sfc. Sfc is often used in maps, like google maps. Sounds good and worth trying. Thank you. But I'm still looking for an OPTICS implementation in Python. No problem, I didn't know about OPTICS, I didn't understand it either. It seems to me it is using a spatial index but I don't understand why. With a spatial index, OPTICS can run in O(n log n) instead of O(n^2). So for large, indexable data sets, this makes quite a difference. An R*-Tree is probably a better choice than a sfc though.Scikit-Learn: Predicting new points with DBSCAN 
I am using DBSCAN to cluster some data using Scikit-Learn (Python 2.7):
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(random_state=0)
dbscan.fit(X)

However, I found that there was no built-in function (aside from "fit_predict") that could assign the new data points, Y, to the clusters identified in the original data, X. The K-means method has a "predict" function but I want to be able to do the same with DBSCAN. Something like this:
dbscan.predict(X, Y)

So that the density can be inferred from X but the return values (cluster assignments/labels) are only for Y. From what I can tell, this capability is available in R so I assume that it is also somehow available in Python. I just can't seem to find any documentation for this.
Also, I have tried searching for reasons as to why DBSCAN may not be used for labeling new data but I haven't found any justifications.

Clustering is not classification.
Clustering is unlabeled. If you want to squeeze it into a prediction mindset (which is not the best idea), then it essentially predicts without learning. Because there is no labeled training data available for clustering. It has to make up new labels for the data, based on what it sees. But you can't do this on a single instance, you can only "bulk predict".
But there is something wrong with scipys DBSCAN:

random_state : numpy.RandomState, optional :

The generator used to initialize the centers. Defaults to numpy.random.


DBSCAN does not "initialize the centers", because there are no centers in DBSCAN.
Pretty much the only clustering algorithm where you can assign new points to the old clusters is k-means (and its many variations). Because it performs a "1NN classification" using the previous iterations cluster centers, then updates the centers. But most algorithms don't work like k-means, so you can't copy this.
If you want to classify new points, it is best to train a classifier on your clustering result.
What the R version maybe is doing, is using a 1NN classificator for prediction; maybe with the extra rule that points are assigned the noise label, if their 1NN distance is larger than epsilon, mabye also using the core points only. Maybe not.
Get the DBSCAN paper, it does not discuss "prediction" IIRC.
 Scikit-learn's k-means clustering has a method to "predict": predict(X):	Predict the closest cluster each sample in X belongs to., and that's typically what one intends to do with "prediction" in the clustering context. @Sid except that only for k-means "closest" makes any sense, and will be consistent with the cluster labels. With DBSCAN, this will not produce the same labels as fit_predict, i.e., it will be inconsistent.
While Anony-Mousse has some good points (Clustering is indeed not classifying) I think the ability of assigning new points has it's usefulness. *
Based on the original paper on DBSCAN and robertlaytons ideas on github.com/scikit-learn, I suggest running through core points and assigning to the cluster of the first core point that is within eps of you new point. 
Then it is guaranteed that your point will at least be a border point of the assigned cluster according to the definitions used for the clustering. 
(Be aware that your point might be deemed noise and not assigned to a cluster)
I've done a quick implementation:
import numpy as np
import scipy as sp

def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):
    # Result is noise by default
    y_new = np.ones(shape=len(X_new), dtype=int)*-1 

    # Iterate all input samples for a label
    for j, x_new in enumerate(X_new):
        # Find a core sample closer than EPS
        for i, x_core in enumerate(dbscan_model.components_): 
            if metric(x_new, x_core) < dbscan_model.eps:
                # Assign label of x_core to x_new
                y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]
                break

    return y_new

The labels obtained by clustering (dbscan_model = DBSCAN(...).fit(X) and the labels obtained from the same model on the same data  (dbscan_predict(dbscan_model, X)) sometimes differ. I'm not quite certain if this is a bug somewhere or a result of randomness. 
EDIT: I Think the above problem of differing prediction outcomes could stem from the possibility that a border point can be close to multiple clusters. Please update if you test this and find an answer. Ambiguity might be solved by shuffling core points every time or by picking the closest instead of the first core point. 
*) Case at hand: I'd like to evaluate if the clusters obtained from a subset of my data makes sense for other subset or is simply a special case.
If it generalises it supports the validity of the clusters and the earlier steps of pre-processing applied.
 is it possible to predict new data points with agglomerative clustering? Possible yes, but I think the above concerns are at least as relevant. In the above case I exploited that DBSCAN has a notion of closeness. IIRC Aglo. Clustering doesn't, so you have to introduce a new one, e.g. a K-NN inspired one. I suggest really paying attention to @anony-mousse 's answer. From sklearn's user guide: even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order.
Here a slightly different and more efficient implementation. Also, instead of taking the first best core point that is within the eps radius, the core point that is closest to the sample is taken.
def dbscan_predict(model, X):

    nr_samples = X.shape[0]

    y_new = np.ones(shape=nr_samples, dtype=int) * -1

    for i in range(nr_samples):
        diff = model.components_ - X[i, :]  # NumPy broadcasting

        dist = np.linalg.norm(diff, axis=1)  # Euclidean distance

        shortest_dist_idx = np.argmin(dist)

        if dist[shortest_dist_idx] < model.eps:
            y_new[i] = model.labels_[model.core_sample_indices_[shortest_dist_idx]]

    return y_new


Although it's not the exact same algorithm you can preform approximate predictions for new points with sklearn HDBSCAN. See here.
It works like this:
clusterer = hdbscan.HDBSCAN(min_cluster_size=15, prediction_data=True).fit(data)
test_labels, strengths = hdbscan.approximate_predict(clusterer, test_points)

Choosing eps and minpts for DBSCAN (R)? 
I've been searching for an answer for this question for quite a while, so I'm hoping someone can help me.  I'm using dbscan from the fpc library in R.  For example, I am looking at the USArrests data set and am using dbscan on it as follows:
library(fpc)
ds <- dbscan(USArrests,eps=20)

Choosing eps was merely by trial and error in this case.  However I am wondering if there is a function or code available to automate the choice of the best eps/minpts.  I know some books recommend producing a plot of the kth sorted distance to its nearest neighbour.    That is, the x-axis represents "Points sorted according to distance to kth nearest neighbour" and the y-axis represents the "kth nearest neighbour distance".    
This type of plot is useful for helping choose an appropriate value for eps and minpts.  I hope I have provided enough information for someone to be help me out.  I wanted to post a pic of what I meant however I'm still a newbie so can't post an image just yet.

There is no general way of choosing minPts. It depends on what you want to find. A low minPts means it will build more clusters from noise, so don't choose it too small.
For epsilon, there are various aspects. It again boils down to choosing whatever works on this data set and this minPts and this distance function and this normalization. You can try to do a knn distance histogram and choose a "knee" there, but there might be no visible one, or multiple.
OPTICS is a successor to DBSCAN that does not need the epsilon parameter (except for performance reasons with index support, see Wikipedia). It's much nicer, but I believe it is a pain to implement in R, because it needs advanced data structures (ideally, a data index tree for acceleration and an updatable heap for the priority queue), and R is all about matrix operations.
Naively, one can imagine OPTICS as doing all values of Epsilon at the same time, and putting the results in a cluster hierarchy.
The first thing you need to check however - pretty much independent of whatever clustering algorithm you are going to use - is to make sure you have a useful distance function and appropriate data normalization. If your distance degenerates, no clustering algorithm will work.
 I'd be surprised if implementing it in R were dramatically harder than in other languages ("R is all about matrix operations" is really quite wrongheaded--the data.frame, probably the most-used data structure in R, is not a matrix but a list.).  For performance reasons, when it does get implemented it'll likely be in Rcpp though. Oh sorry. It was Matlab where these things were a really big pain apparently. For R, some indexing existing in the "rann" package. But I believe fpc does not use that, and as R does not have a "database query" API, it cannot autoconnect the modules. In my experiments, fpc DBSCAN was by a factor of 10x slower than other implementations. Only Weka was even much worse (another 8x slower). Performance in R is sensitive to implementation.  I'm not denying that the algorithm might be harder, but in practice generic algorithms like this tend to be written as libraries and then accessed (LINPACK, GEOS, etc.)--that avoids duplication of optimization effort across lots of languages.  R is designed to be reasonable for applied statistical practitioners, and extensible for programmers.  Part of that extensibility means using other libraries and languages where helpful. That pretty much holds for any language... and yet, R packages seem to be mostly stand-alone and interact rather little. Which sometimes is also good, if you think of the .jar mess many Apache projects bring with them...
One common and popular way of managing the epsilon parameter of DBSCAN is to compute a k-distance plot of your dataset. Basically, you compute the k-nearest neighbors (k-NN) for each data point to understand what is the density distribution of your data, for different k. the KNN is handy because it is a non-parametric method. Once you choose a minPTS (which strongly depends on your data), you fix k to that value. Then you use as epsilon the k-distance corresponding to the area of the k-distance plot (for your fixed k) with a low slope.
 This is actually just the method discussed in the original DBCAN paper. And is what the OP said he's already heard about but would like alternative suggestions for. I don't see where he says he does not want it. actually, it looks like he says the inverse. the R package dbscan has a function called kNNdistplot which produces this type of graph.
MinPts
As Anony-Mousse explained, 'A low minPts means it will build more clusters from noise, so don't choose it too small.'.
minPts is best set by a domain expert who understands the data well. Unfortunately many cases we don't know the domain knowledge, especially after data is normalized. One heuristic approach is use ln(n), where n is the total number of points to be clustered.
epsilon
There are several ways to determine it:
1) k-distance plot 
In a clustering with minPts = k, we expect that core pints and border points' k-distance are within a certain range, while noise points can have much greater k-distance, thus we can observe a knee point in the k-distance plot. However, sometimes there may be no obvious knee, or there can be multiple knees, which makes it hard to decide

2) DBSCAN extensions like OPTICS
OPTICS produce hierarchical clusters, we can extract significant flat clusters from the hierarchical clusters by visual inspection, OPTICS implementation is available in Python module pyclustering. One of the original author of DBSCAN and OPTICS also proposed an automatic way to extract flat clusters, where no human intervention is required, for more information you can read this paper.
3) sensitivity analysis
Basically we want to chose a radius that is able to cluster more truly regular points (points that are similar to other points), while at the same time detect out more noise (outlier points). We can draw a percentage of regular points (points belong to a cluster) VS. epsilon analysis, where we set different epsilon values as the x-axis, and their corresponding percentage of regular points as the y axis, and hopefully we can spot a segment where the percentage of regular points value is more sensitive to the epsilon value, and we choose the upper bound epsilon value as our optimal parameter.
 "One heuristic approach is use ln(n), where n is the total number of points to be clustered." Do you have a citation for this? @MarkWhite It's used in Section 4.1 of ST-DBSCAN: An algorithm for clustering spatial–temporal data The original DBSCAN paper suggests to base minpts on the data dimensionality 2*dim rather than the data set size n. There is also automatic cluster extraction already in the first OPTICS paper. @NAGA I don't think there is anything wrong put it in such a way, even domain knowledge is not mandatory to use DBSCAN, it's almost always beneficial to set minPts based on domain knowledge. In the scenario where domain knowledge is not there, we can then rely on some other heuristic approaches, as what is described in the original paper.
For details on choosing parameters, see the paper below on p. 11:
Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 19.


For two-dimensional data: use default value of minPts=4 (Ester et    al., 1996)
For more than 2 dimensions: minPts=2*dim  (Sander et al., 1998)


Once you know which MinPts to choose, you can determine Epsilon:


Plot the k-distances with k=minPts (Ester et al., 1996)
Find the 'elbow' in the graph--> The k-distance value is your Epsilon value.



See this webpage, section 5: http://www.sthda.com/english/wiki/dbscan-density-based-clustering-for-discovering-clusters-in-large-datasets-with-noise-unsupervised-machine-learning
It gives detailed instructions on how to find epsilon. MinPts ... not so much. 
 I tried this method to find the "knee" but when I plot my data given k=5, it doesn't look as if it has a "knee". I tried to increase the value of k (from 5 to even 1000), but the plot looks quite the same. Also, the document doesn't say why the knee is corresponding to the optimal epsilon. Sorry, was meant to attach the screenshot link in the previous comment. Inline LinkPCA For categorical features? 
In my understanding, I thought PCA can be performed only for continuous features. But while trying to understand the difference between onehot encoding and label encoding came through a post in the following link:
When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?
It states that one hot encoding followed by PCA is a very good method, which basically means PCA is applied for categorical features.
Hence confused, please suggest me on the same.
 I would like to ask if the following articles approach to converting categorical variables to numeric by summing their ASCII byte representation is a good idea? blog.davidvassallo.me/2015/10/28/…
I disagree with the others.
While you can use PCA on binary data (e.g. one-hot encoded data) that does not mean it is a good thing, or it will work very well.
PCA is desinged for continuous variables. It tries to minimize variance (=squared deviations). The concept of squared deviations breaks down when you have binary variables.
So yes, you can use PCA. And yes, you get an output. It even is a least-squared output - it's not as if PCA would segfault on such data. It works, but it is just much less meaningful than you'd want it to be; and supposedly less meaningful than e.g. frequent pattern mining.

MCA is a known technique for categorical data dimension reduction. In R there is a lot of package to use MCA and even mix with PCA in mixed contexts. In python exist a a mca library too. MCA apply similar maths that PCA, indeed the French statistician used to say, "data analysis is to find correct matrix to diagonalize"
http://gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/

Basically, PCA finds and eliminate less informative (duplicate) information on feature set and reduce the dimension of feature space. In other words, imagine a N-dimensional hyperspace, PCA finds such M (M < N) features that the data variates most. In this way data may be represented as M-dimensional feature vectors. Mathematically, it is some-kind of a eigen-values & eigen vectors calculation of a feature space.
So, it is not important whether the features are continuous or not. 
PCA is used widely on many application. Mostly for eliminating noisy, less informative data that comes from some sensor or hardware before classification/recognition.
Edit:
Statistically speaking, categorical features can be seen as discrete random variables in interval [0,1]. Computation for expectation E{X} and variance E{(X-E{X})^2) are still valid and meaningful for discrete rvs. I still stand for the applicability of PCA in case of categorical features. 
Consider a case where you would like to predict whether "It is going to rain for a given day or not". You have categorical feature X which is "Do I have to go to work for the given day", 1 for yes and 0 for no. Clearly weather conditions do not depend on our work schedule, so P(R|X)=P(R). Assuming 5 days of work for every week, we have more 1s than 0s for X in our randomly collected dataset. PCA would probably lead to dropping this low-variance dimension in your feature representation. 
At the end of the day, PCA is for dimension reduction with minimal loss of information. Intuitively, we rely on variance of the data on a given axis to measure its usefulness for the task. I don't think there is any theoretical limitation for applying it to categorical features. Practical value depends on application and data which is also the case for continuous variables. 
 Well it kinda boils down to calculating the eigenvectors of the covariance matrix, thus having binary data (e.g one-hot) how would you interpret the distance to the mean from  a binary point?
The following publication shows great and meaningful results when computing PCA on categorical variables treated as simplex vertices:

Niitsuma H., Okada T. (2005) Covariance and PCA for Categorical Variables. In: Ho T.B., Cheung D., Liu H. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2005. Lecture Notes in Computer Science, vol 3518. Springer, Berlin, Heidelberg
https://doi.org/10.1007/11430919_61

It is available via https://arxiv.org/abs/0711.4452 (including as a PDF).
 Why did you roll back that edit? This paper was published in 2005, in spite of the 2018 date at the top. See arxiv.org/abs/0711.4452, which is the source of the PDF you link to (submitted 2007 certainly means it can’t have been published after that point). And, much more importantly, the citation reference for this paper is 100% clear this is a work form 2005.
PCA is a dimensionality reduction method that can be applied any set of features. Here is an example using OneHotEncoded (i.e. categorical) data:
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
X = enc.fit_transform([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]).toarray()

print(X)

> array([[ 1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.],
       [ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]])


from sklearn.decomposition import PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X)

print(X_pca)

> array([[-0.70710678,  0.79056942,  0.70710678],
       [ 1.14412281, -0.79056942,  0.43701602],
       [-1.14412281, -0.79056942, -0.43701602],
       [ 0.70710678,  0.79056942, -0.70710678]])

 thanks for the detailed explanation. Can u please suggest me on how to intrepret the results of one hot encoder in your code. If I recall correctly, the PCA algorithm projects the features onto a different space by solving for the eigenvectors and eigenvalues. Then it looks at the top N (3 in this case) largest eigenvalues and takes those eigenvector components. The idea is to encode the most useful data in fewer features. Oh you were asking about the one hot encoder ... There are two options for feature 1 (0 and 1), three options for feature 2 (0, 1, and 2) and four options for feature 3 (0, 1, 2, and 3). That totals up to 9 options, and hence why we have 9 one hot encoded features. Hopefully that gets you thinking along the right lines to understand what is happening. actually I meant what u answered me at first:) You CAN apply PCA when one-hot encoding - the question is if it makes sense?
I think  pca is reducing var by leverage the linear relation between vars.
If there's only one categoral var coded in onehot, there's not linear relation between the onehoted cols. so it can't reduce by pca. 
But if there exsits other vars, the onehoted cols may be can presented by linear relation of other vars.
So may be it can reduce by pca, depends on the relation of vars.
How does clustering (especially String clustering) work? 
I heard about clustering to group similar data. I want to know how it works in the specific case for String.
I have a table with more than different 100,000 words. 
I want to identify the same word with some differences (eg.: house, house!!, hooouse, HoUse, @house, "house", etc...).
What is needed to identify the similarity and group each word in a cluster? What algorithm is more recommended for this?

To understand what clustering is imagine a geographical map. You can see many distinct objects (such as houses). Some of them are close to each other, and others are far. Based on this, you can split all objects into groups (such as cities). Clustering algorithms make exactly this thing - they allow you to split your data into groups without previous specifying groups borders. 
All clustering algorithms are based on the distance (or likelihood) between 2 objects. On geographical map it is normal distance between 2 houses, in multidimensional space it may be Euclidean distance (in fact, distance between 2 houses on the map also is Euclidean distance). For string comparison you have to use something different. 2 good choices here are Hamming and Levenshtein distance. In your particular case Levenshtein distance if more preferable (Hamming distance works only with the strings of same size). 
Now you can use one of existing clustering algorithms. There's plenty of them, but not all can fit your needs. For example, pure k-means, already mentioned here will hardly help you since it requires initial number of groups to find, and with large dictionary of strings it may be 100, 200, 500, 10000 - you just don't know the number. So other algorithms may be more appropriate. 
One of them is expectation maximization algorithm. Its advantage is that it can find number of clusters automatically. However, in practice often it gives less precise results than other algorithms, so it is normal to use k-means on top of EM, that is, first find number of clusters and their centers with EM and then use k-means to adjust the result. 
Another possible branch of algorithms, that may be suitable for your task, is hierarchical clustering. The result of cluster analysis in this case in not a set of independent groups, but rather tree (hierarchy), where several smaller clusters are grouped into one bigger, and all clusters are finally part of one big cluster. In your case it means that all words are similar to each other up to some degree. 
 Very nice explanation. Thank you so much. But I have one doubt now. The distance of one string to other must be calculated for another each word? Example: if I have 100 words, I will compare each word with the 99 words? Or this change according to the algorithm (as example, only comparing with the cluster centers)? Yes, it depends on the algorithm, but in general most of them compare elements to each other many times. Clustering algorithms by themselves are computationally very difficult (e.g. k-means is a NP-hard task), but many of them have heuristic improvements that make them much easier to perform. See docs for particular algorithm you are interested in. k-means on top of EM? Never heard of that. The advice given by e.g. Bishop (''Pattern Recognition and Machine Learning'', Springer 2006) is the exact opposite: EM is better but slow to start, so bootstrap it with a few rounds of k-means optimization. Also, suggesting EM or k-means in conjunction with string edit distances makes no sense. k-means needs not only a distance metric, but also a well-defined mean of a set of samples, which for strings under edit distance is impossible to define. @larsmans: EM on top of k-means gives faster convergence and better protection against local minima. k-means on top of EM gives automatic class count discovery and all advantages of k-means. I don't see contradiction here. EM is better than k-means? Sorry, but without concrete task and dataset such a statement makes no sense for me. Anyway, I didn't mean that k-means will definitely work better for string clustering, what I wanted to stress is that one can easily combine both, and Bishop's quote confirms it.
There is a package called stringdist that allows for string comparison using several different methods. Copypasting from that page:

Hamming distance: Number of positions with same symbol in both strings. Only defined for strings of equal length.
Levenshtein distance: Minimal number of insertions, deletions and replacements needed for transforming string a into string b.
(Full) Damerau-Levenshtein distance: Like Levenshtein distance, but transposition of adjacent symbols is allowed.
Optimal String Alignment / restricted Damerau-Levenshtein distance: Like (full) Damerau-Levenshtein distance but each substring may only be edited once.
Longest Common Substring distance: Minimum number of symbols that have to be removed in both strings until resulting substrings are identical.
q-gram distance: Sum of absolute differences between N-gram vectors of both strings.
Cosine distance: 1 minus the cosine similarity of both N-gram vectors.
Jaccard distance: 1 minues the quotient of shared N-grams and all observed N-grams.
Jaro distance: The Jaro distance is a formula of 4 values and effectively a special case of the Jaro-Winkler distance with p = 0.
Jaro-Winkler distance: This distance is a formula of 5 parameters determined by the two compared strings (A,B,m,t,l) and p chosen from [0, 0.25].

That will give you the distance. You might not need to perform a cluster analysis, perhaps sorting by the string distance itself is sufficient. I have created a script to provide the basic functionality here... feel free to improve it as needed.

You can use an algorithm like the Levenshtein distance for the distance calculation and k-means for clustering.

the Levenshtein distance is a string metric for measuring the amount of difference between two sequences

Do some testing and find a similarity threshold per word that will decide your groups.
 What algorithm is more recommended for string clustering? What do you mean more recommended? There are some clustering algorithms, right? Giving the example with house in the question, what algorithm can be more adequate to get this type of result? I want to put all that words inside the same cluster. You could use k-means for clustering, using the Levenshtein distance for distance calculation. And how would you compute the means?Data Mining open source tools [closed] 









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 7 years ago.



I'm due to take up a project which is into data mining. Before I jump in I wanted to probe around for different data mining tools (preferably open source) which allows web based reporting. In my scenario the data would be provided to me, so I'm not supposed to crawl for it.
In a nutshell, I am looking for a tool which does - Data Analysis, Web based Reporting, provides some kind of a dashboard and mining features.
I have worked on the Microsoft Analysis Services and BOXI and off late I have been looking at Pentaho, which seems to be a good option.
Please share your experiences on any such tool which you know of.
cheers
 One more point - I would like a tool which can interface well with either .NET code or even Python.
I believe WEKA is the best open source DM software out there.
Check it: http://www.cs.waikato.ac.nz/ml/weka/
 But only for classification, a subtopic of data mining.
Weka is great, but you might want to try the Orange Data Mining toolkit instead.
http://www.ailab.si/orange/
Edit: And as of November 2010, I must say I really like KNIME.
 +1 for KNIME. I discovered this a few weeks ago, and have been very impressed with what it can do. Supports Java, Python, and R scripts, and the BIRT add-on makes writing reports a breeze.
R has a lot of excellent packages related to data mining.  In particular, look at:

The machine learning view on CRAN.
The natural language processing view on CRAN.

It also ties into Weka (see the RWeka package).  And it can be integrated with either .Net (through COM) or Python (through RPy or RPy2).  
I would agree regarding Pentaho for a reporting platform, although it's a very large project depending upon what you're using it for.

You should also check out Apache Mahout . It can be quite useful for some large-scale machine learning tasks such as user clustering.
 The Apache licence is the biggest plus, because other mentioned libraries use GPL that prohibits commercial use cases
RapidMiner is my preferred data mining tool.

I would try with the new google tools.
-first you need to get the api id for the google-storage, which is where you are going to store and manipulate the data you are going to analyze. 
-Then you need to get the api id for google-prediction-api (http://code.google.com/apis/predict/docs/getting-started.html), which for what I saw it is a fantastic outsourced data mining processor. The Prediction API allows you to get more from your data and makes its patterns more accessible. Besides using traditional numeric and nominal data you can also use text data that thanks to this api can be utilized for exampled to categorize emails by language. 
-Finally you can use bigQuery that will allow you to perform Ad-hoc analysis, Standardized reporting, Data exploration App prototyping (http://code.google.com/apis/bigquery/)

KEEL (http://keel.es) is written in Java and is good for using evolutionary computation for data mining.

Have a look at list of Open Source software's for Machine learning maintained by JMLR.
you can find it here:
http://mloss.org/software/
http://jmlr.csail.mit.edu/mloss/
They represent State of Art!
My issue with Weka is that a number of algorithms in it are outdated.

i believe RapidMiner is an excellent tool that should be added to this list.

WEKA (Already mentioned),
Orange (http://orange.biolab.si/),
Tanagra (http://data-mining-tutorials.blogspot.com) you can find good tutorials there.
Are very good tools for data mining. 

You could check my software, the SPMF data mining framework.
It is an open-source Java software that offers more than 70 algorithms for:

frequent itemset mining,
association rule mining,
sequential pattern mining
sequential rule mining.
and more..


Pentaho is a very professional solution. Definitely a very good choice. 

You can look at Data Mining SDK and its blog.

A list of some open source data mining tools are listed here:
http://dataminingtools.net/browse.php

Eclipse BIRT http://www.eclipse.org/birt/phoenix/project/description.php

I believe KNIME deserves to join this list as well.

Weka is strong for classification and /machine learning/. To many, this is considered to be more a part of artificial intelligence than of actual data-mining. RapidMiner is largely along the same lines, but with a much nicer UI. Pentaho is the professional support for Weka AFAICT.
You might want to have a look at ELKI, http://elki.dbs.ifi.lmu.de/ which is a comparable project that focuses on clustering algorithms and outlier detections, two other key tasks of data-mining.

you can take a look at data mining tool, weka
Here is a link to a collection of tutorials and videos on WEKA
Tutorials:http://www.dataminingtools.net/browsetutorials.php?tag=weka 
Videos:
http://www.dataminingtools.net/videos.php?id=6 

Along with the tools, i would strongly suggest learning Python and R. These languages help a lot during analysis. Also, large datasets can be 'custom-analysed'. You might also create your own custom dashboard using Javascript(check out the numerous charting and visualization libraries)

I am a python-er myself and I have to say:
Yes! All of that can be done in Python.
I last played around with Beautiful Soup[0].  It's a really simple to use module that lets you grab/mine data from html and xml (excellent for 'screen scraping').
If you dont know python, .... well It's really easy to learn.
[0]http://www.crummy.com/software/BeautifulSoup/
 Data mining is all about discovering "hidden" knowledge in data, it has nothing to do (at least directly) with screen scrapping, but thanks for pointing me at Beautiful Soup, I'll play around with it. =) All of it can be done in ASSEMBLER, too!Difference between Closed and open Sequential Pattern Mining Algorithms 
I want to use some algorithms to mine my log data.
I found a pattern mining framework on: http://www.philippe-fournier-viger.com/spmf/index.php?link=algorithms.php
I have tried several algorithms, the BIDE+ algorithm performs the best.
The BIDE+ algorithm is for mining frequent closed sequential patterns from a sequence database.
Can someone explain the definition about "closed" sequential patterns and open ones?

Glad that you are using my SPMF software.
The support of a sequential pattern is the number of sequences that contains the sequential pattern.
A frequent sequential pattern is a pattern that appears in at least "minsup" sequences of a sequence database, where minsup is a parameter set by the user.
A frequent closed sequential pattern is a frequent sequential pattern such that it is not included in another sequential pattern having exactly the same support.
Algorithms such as PrefixSpan finds frequent sequential patterns.  Algorithms such as BIDE+ finds frequent closed sequential patterns. BIDE+ is usually much faster than PrefixSpan because it uses pruning techniques to avoid generating all sequential patterns. Moreover, the set of closed patterns is usually much smaller than the set of sequential patterns so BIDE+ is also more memory efficient.
Another important thing to know is that closed sequential patterns are a compact and lossless representation of all sequential patterns.  This means that the set of closed sequential patterns is usually much smaller but it is lossless, which means that it allows recovering the full set of sequential patterns (no information is loss), which is very convenient.
I can give you a simple example.
Let's consider  4 sequences:
a  b  c  d  e
a  b  d
b  e  a  
b  c  d  e

Let's say that minsup = 2.     
b c is a frequent sequential patterns because it appears in two sequences (it has a support of 2). b c is not a closed sequential patterns because it is contained in a larger sequential pattern b c d having the same support.
b c d has a support of 2. It is also not a closed sequential pattern because it is contained in a larger sequential pattern b c d e having the same support.  b c d e is a closed sequential pattern because there it is not included in any other sequential pattern having the same support.
By the way, you can also check my survey about sequential pattern mining. It gives a good introduction about this topic and the different algorithms.
 This is going to help w my dissertation.  Seriously.  Thank you! First of all,thanks for your survey and SPMF,and here your explanation is quite clear,but the example may be not quite appropriate since the pattern b c appears in three sequences(1,3,4), so a minor change may be better. @LancelotHolmes  Thanks for the comment.  Yes, you are right. Fixed that error. Glad you like the survey and SPMF :-) Great explanation of the closed-open concept.
Check out this chapter on
Frequent Itemsets and Frequent item sets Mining & Association Rules 

Google for "closed frequent itemsets". There will be plenty of pages explaining this, as will any data mining book (look for the APRIORI algorithm).
"Closed" says that there is no larger itemset with the same support. There can be larger itemsets, but they must have lower support.
For most use cases it is either sufficient to look at maximal or at closed itemsets only.
What makes the distance measure in k-medoid “better” than k-means? 
I am reading about the difference between k-means clustering and k-medoid clustering.
Supposedly there is an advantage to using the pairwise distance measure in the k-medoid algorithm, instead of the more familiar sum of squared Euclidean distance-type metric to evaluate variance that we find with k-means.  And apparently this different distance metric somehow reduces noise and outliers.
I have seen this claim but I have yet to see any good reasoning as to the mathematics behind this claim.  
What makes the pairwise distance measure commonly used in k-medoid better?  More exactly, how does the lack of a squared term allow k-medoids to have the desirable properties associated with the concept of taking a median?
 stats.stackexchange.com can be better place to get more deep and theoritical answers. See my updated answer, for the notion of breakdown point from robust statistics. The medoid is likely a robust statistic, the mean is not at all robust.
1. K-medoid is more flexible
First of all, you can use k-medoids with any similarity measure. K-means however, may fail to converge - it really must only be used with distances that are consistent with the mean. So e.g. Absolute Pearson Correlation must not be used with k-means, but it works well with k-medoids.
2. Robustness of medoid
Secondly, the medoid as used by k-medoids is roughly comparable to the median (in fact, there also is k-medians, which is like K-means but for Manhattan distance). If you look up literature on the median, you will see plenty of explanations and examples why the median is more robust to outliers than the arithmetic mean. Essentially, these explanations and examples will also hold for the medoid. It is a more robust estimate of a representative point than the mean as used in k-means.
Consider this 1-dimensional example:
[1, 2, 3, 4, 100000]

Both the median and medoid of this set are 3. The mean is 20002.
Which do you think is more representative of the data set? The mean has the lower squared error, but assuming that there might be a measurement error in this data set ...
Technically, the notion of breakdown point is used in statistics. The median has a breakdown point of 50% (i.e. half of the data points can be incorrect, and the result is still unaffected), whereas the mean has a breakdown point of 0 (i.e. a single large observation can yield a bad estimate).
I do not have a proof, but I assume the medoid will have a similar breakdown point as the median.
3. k-medoids is much more expensive
That's the main drawback. Usually, PAM takes much longer to run than k-means. As it involves computing all pairwise distances, it is O(n^2*k*i); whereas k-means runs in O(n*k*i) where usually, k times the number of iterations is k*i << n.
 Thanks for your comments.  But I'm still not seeing a correlation between the lack of squared term in the similarity measure, and the concept of median. It's not the squared term itself. It's the sum as much, which is not robust to outliers. Put an really extreme value into your data. Say, your data is 0,1,2,3,100000000. Compare the mean and the median, which is more robust? A little handwavy with respect the part regarding the analogy between k-medoids and the median? Well, obviously they aren't the same. But if you make an outlier more extreme by delta, this will not affect the medoid much, just like the median; because all other candidates are affected the same way.
I think this has to do with the selection of the center for the cluster. k-means will select the "center" of the cluster, while k-medoid will select the "most centered" member of the cluster.
In a cluster with outliers (i.e. points far away from the other members of the cluster) k-means will place the center of the cluster towards the outliers, whereas k-medoid will select one of the more clustered members (the medoid) as the center.
It now depends on what you use clustering for. If you just wanted to classify a bunch of objects then you don't really care about where the center is; but if the clustering was used to train a decider which will now classify new objects based on those center points, then k-medoid will give you a center closer to where a human would place the center.
In wikipedia's words:
"It [k-medoid] is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances."
Here's an example:
Suppose you want to cluster on one dimension with k=2. One cluster has most of its members around 1000 and the other around -1000; but there is an outlier (or noise) at 100000.
It obviously belongs to the cluster around 1000 but k-means will put the center point away from 1000 and towards 100000. This may even make some of the members of the 1000 cluster (say a member with value 500) to be assigned to the -1000 cluster.
k-medoid will select one of the members around 1000 as the medoid, it'll probably select one that is bigger than 1000, but it will not select an outlier.

Just a tiny note added to @Eli's answer, K-medoid is more robust to noise and outliers than k-means because the latter selects the cluster center, which is mostly just a "virtue point", on the other hand the former chooses the "actual object" from the cluster. 
Suppose you have five 2D points in one cluster with the coordinates of (1,1),(1,2),(2,1),(2,2), and (100,100). If we don't consider the object exchanges among the clusters, with k-means you will get the center of cluster (21.2,21.2) which is pretty distracted by the point (100,100). However, with k-medoid will choose the center among (1,1),(1,2),(2,1),and (2,2) according to its algorithm.
Here is a fun applet ( E.M. Mirkes, K-means and K-medoids applet. University of Leicester, 2011 ) that you can randomly generate dataset in the 2D plane and compare k-medoid and k-means learning process.
scikit-learn DBSCAN memory usage 
UPDATED: In the end, the solution I opted to use for clustering my large dataset was one suggested by Anony-Mousse below. That is, using ELKI's DBSCAN implimentation to do my clustering rather than scikit-learn's. It can be run from the command line and with proper indexing, performs this task within a few hours. Use the GUI and small sample datasets to work out the options you want to use and then go to town. Worth looking into. Anywho, read on for a description of my original problem and some interesting discussion.
I have a dataset with ~2.5 million samples, each with 35 features (floating point values) that I'm trying to cluster. I've been trying to do this with scikit-learn's implementation of DBSCAN, using the Manhattan distance metric and a value of epsilon estimated from some small random samples drawn from the data. So far, so good. (here is the snippet, for reference)
db = DBSCAN(eps=40, min_samples=10, metric='cityblock').fit(mydata)

My issue at the moment is that I easily run out of memory. (I'm currently working on a machine with 16 GB of RAM)
My question is, is DBSCAN calculating the pairwise distance matrix on the fly as it runs, and that's what's gobbling up my memory? (2.5 million ^ 2) * 8 bytes is obviously stupidly large, I would understand that. Should I not be using the fit() method? And more generally, is there a way around this issue, or am I generally barking up the wrong tree here?
Apologies if the answer winds up being obvious. I've been puzzling over this for a few days. Thanks!
Addendum: Also if anyone could explain the difference between fit(X) and fit_predict(X) to me more explicitly I'd also appreciate that--I'm afraid I just don't quite get it.
Addendum #2: To be sure, I just tried this on a machine with ~550 GB of RAM and it still blew up, so I feel like DBSCAN is likely trying to make a pairwise distance matrix or something I clearly don't want it to do. I guess now the big question is how to stop that behavior, or find other methods that might suit my needs more. Thanks for bearing with me here. 
Addendum #3(!): I forgot to attach the traceback, here it is,
Traceback (most recent call last):
  File "tDBSCAN.py", line 34, in <module>
    db = DBSCAN(eps=float(sys.argv[2]), min_samples=10, metric='cityblock').fit(mydata)
  File "/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/base.py", line 329, in fit_predict
    self.fit(X)
  File "/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/cluster/dbscan_.py", line 186, in fit
    **self.get_params())
  File "/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/cluster/dbscan_.py", line 69, in dbscan
    D = pairwise_distances(X, metric=metric)
  File "/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/metrics/pairwise.py", line 651, in pairwise_distances
    return func(X, Y, **kwds)
  File "/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/metrics/pairwise.py", line 237, in manhattan_distances
    D = np.abs(X[:, np.newaxis, :] - Y[np.newaxis, :, :])
MemoryError


The problem apparently is a non-standard DBSCAN implementation in scikit-learn.
DBSCAN does not need a distance matrix. The algorithm was designed around using a database that can accelerate a regionQuery function, and return the neighbors within the query radius efficiently (a spatial index should support such queries in O(log n)).
The implementation in scikit however, apparently, computes the full O(n^2) distance matrix, which comes at a cost both memory-wise and runtime-wise.
So I see two choices:

You may want to try the DBSCAN implementation in ELKI instead, which when used with an R*-tree index usually is substantially faster than a naive implementation.
Otherwise, you may want to reimplement DBSCAN, as the implementation in scikit apparently isn't too good. Don't be scared of that: DBSCAN is really simple to implement yourself. The trickiest part of a good DBSCAN implementation is actually the regionQuery function. If you can get this query fast, DBSCAN will be fast. And you can actually reuse this function for other algorithms, too.

Update: by now, sklearn no longer computes a distance matrix and can, e.g., use a kd-tree index. However, because of "vectorization" it will still precompute the neighbors of every point, so the memory usage of sklearn for large epsilon is O(n²), whereas to my understanding the version in ELKI will only use O(n) memory. So if you run out of memory, choose a smaller epsilon and/or try ELKI.
 Actually it seems that it would not be too hard to improve the sklearn implementation. We have a ball-tree data structure which exactly supports the radius query. I am not very familiar with dbscan so I didn't know it only needed these queries. We should definitely improve there. I think that the sklearn implementation has significantly improved with sklearn 0.14: The ball-tree implementation now supports a good selection of metrics and DBSCAN has been adapted to not internally compute the entire pairwise distance matrix. So it seems to be an option again, unfortunately haversine distance is still not supported by the pairwise metrics package. Relevant github ticket (beware, the changes are spread out over many pull requests and tickets): github.com/scikit-learn/scikit-learn/issues/1938 I agree, sklearn has improved its DBSCAN. Still, ELKI remains to be more powerful when it comes to index acceleration and cluster analysis. For example, it also has OPTICS, and other DBSCAN-derivatives. The problem is ELKI doesn't have good documentation or a 'hello world' example. I found the tutorials such as thr mouse example on the web site "hello world" enough. And the javadoc is pretty good, too.
You can do this using scikit-learn's DBSCAN with the haversine metric and ball-tree algorithm. You do not need to precompute a distance matrix. 
This example clusters over a million GPS latitude-longitude points with DBSCAN/haversine and avoids memory usage problems:
df = pd.read_csv('gps.csv')
coords = df.as_matrix(columns=['lat', 'lon'])
db = DBSCAN(eps=eps, min_samples=ms, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))

Note that this specifically uses scikit-learn v0.15, as some earlier/later versions seem to require a full distance matrix to be computed, which blows up your RAM real quick. But if you use Anaconda, you can quickly set this up with:
conda install scikit-learn=0.15

Or, create a clean virtual environment for this clustering task:
conda create -n clusterenv python=3.4 scikit-learn=0.15 matplotlib pandas jupyter
activate clusterenv

 confirmed, sklearn v0.15.2 requires far less memory than v0.17.1 to run the same model fit
This issue with sklearn is discussed here: 

https://github.com/scikit-learn/scikit-learn/issues/5275

There are two options presented there;
One is to use OPTICS (which requires sklearn v21+), which is an alternative but closely related algorithm to DBSCAN:

https://scikit-learn.org/dev/modules/generated/sklearn.cluster.OPTICS.html

The others are to precompute the adjacency matrix, or to use sample weights.
Some more details on these options can be found under Notes here:

https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html

 Nice nice, I just tried with the OPTICS one and it worked, took around 2 minutes with a ndarray of 43000 lines, with DBSCAN with the same ndarray I was getting the memory crash error.
I faced the same problem when i was using old version on sklearn 0.19.1 because the complexity was O(N^2).
But now the problem has been resolved in new version 0.20.2 and no memory error anymore, and the complexity become O(n.d) where d is the average number of neighbors.
it's not the idol complexity but much better than old versions.
Check the notes in this release, to avoid high memory usage:
https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html

The DBSCAN algorithm actually does compute the distance matrix, so no chance here.
For this much data, I would recommend using MiniBatchKMeans.
You can not use the Manhattan metric there out of the box, but you could do your own implementation. Maybe try the standard implementation with the euclidean metric first.
I don't know many clustering algorithms that don't perform pairwise distances.
Using the newly embedded cheat-sheet bottom center: though luck.
 There's no way to compute them on the fly? The way I understand DBSCAN I'm not clear on why I couldn't just start with a random point, compute its distance to some other point, and compare it to epsilon, chucking it out or adding it as a neighbor over and over again... @JamesT: while it would be possible, the current scikit-learn implementation simply doesn't do that. It doesn't really scale up to large numbers of samples because it takes quadratic space and time. Incorrect. DBSCAN does not need a distance matrix (and in particular, not a matrix). A good implementation should use a spatial index, to significantly reduce the number of distance computations needed. It should be implemented in O(n) memory and O(n log n) runtime. The DBSCAN algorithm in itself does not require to compute the whole distance matrix. See for instance the basic pseudocode on Wikipedia en.wikipedia.org/wiki/DBSCAN#Algorithm Previous versions on scikit relied on the full computation of the distance matrix but it is no longer the case @titus in my experience v0.15.2 requires far less memory than v0.17.1 to run the same code. Any idea why?Fast (< n^2) clustering algorithm 
I have 1 million 5-dimensional points that I need to group into k clusters with k << 1 million. In each cluster, no two points should be too far apart (e.g. they could be bounding spheres with a specified radius). That means that there probably has to be many clusters of size 1.
But! I need the running time to be well below n^2. n log n or so should be fine. The reason I'm doing this clustering is to avoid computing a distance matrix of all n points (which takes n^2 time or many hours), instead I want to just compute distances between clusters.
I tried the pycluster k-means algorithm but quickly realized it's way too slow. I've also tried the following greedy approach:  

Slice space into 20 pieces in each dimension. (so there are 20^5 total pieces). I will store clusters in these gridboxes, according to their centroids.  
For each point, retrieve the gridboxes that are within r (maximum bounding sphere radius). If there is a near enough cluster, add it to that cluster, otherwise make a new cluster.  

However, this seems to give me more clusters than I want. I also implemented approaches similar to this twice, and they give very different answers.
Are there any standard approaches to clustering in faster than n^2 time? Probabilistic algorithms are ok.
 You may explore BIRCH people.cs.ubc.ca/~rap/teaching/504/2005/slides/Birch.pdf Thanks, I will look at that. It seems like it basically uses a multi phase approach, which was also what I was thinking about doing next. E.g. first remove points that are far from all other points, then do some one-pass clustering, then do a rebalancing etc. Where each step is fast. But it's a lot of trouble to implement. Perhaps a dual KD tree algorithm can work here? Perhaps this is useful: cs.cmu.edu/~avrim/Papers/bbg-clustering.pdf The fastest method I have been able to find for 3 dimensional pixel data (C++ source) can be found here: modejong.com/blog/post17_divquant_clustering
Consider an approximate nearest neighbor (ANN) algorithm or locality sensitive hashing (LSH). They don't directly solve the clustering problem, but they will be able to tell you which points are "close" to one another. By altering the parameters, you can define close to be as close as you want. And it's fast.
More precisely, LSH can provide a hash function, h, such that, for two points x and y, and distance metric d,
d(x,y) <= R1  =>  P(h(x) = h(y)) >= P1
d(x,y) >= R2  =>  P(h(x) = h(y)) <= P2

where R1 < R2 and P1 > P2. So yes, it is probabilistic. You can postprocess the retrieved data to arrive at true clusters.
Here is information on LSH including the E2LSH manual. ANN is similar in spirit; David Mount has information here, or try FLANN (has Matlab and Python bindings).
 Hmm, that's quite helpful. So given that I have an approximate nearest neighbors algorithm (I actually think scipy kdtree has this feature, although I don't know if its fast), are you suggesting an algorithm like the following:  1. For each point, approximately compute all nearest neighbors of radius less than r. 2. Sort the points from fewest close neighbors to most. 3. Make a list that will contain the integer cluster number for each point. 4. Iterating through the sorted list (of neighbor sets), assign all points in the set to a new cluster. (sort of a backwards greedy algorithm) Perhaps that may work. I like to imagine each bin in the hash table as becoming "one point". So instead of clustering one million individual points, you instead cluster bins which are collections of points, the assumption being that points in the same bin are already close enough. But finding those bins is fast. That's just one idea. update: that package doesn't seem to really work, to my estimation. also i don't think it implements a euclidean hash. you definitely want the original link Steve provided: mit.edu/~andoni/LSH. It links to matlab code here vision.caltech.edu/malaa/software/research/image-search Denis: FLANN was the first one I tried because of the Python compatibility. If I recall, the interface was easy to use. Once I understood the concept better, I ended up writing my own which wasn't too hard. Of the many data structures available for solving ANN, FLANN chooses the best one for the input data. I knew what inputs I had, so I wrote one for me. But for flexibility and ease of use, FLANN is good. I'm sorry, but in my opinionion LSH is not the right solution. LSH was thought for high dimensional spaces (you don't need a ref. for this, every paper talking about LSH state this). Otherwise, kd-trees has been proved to be much better. In fact LSH complexity is pseudo-polynomial, while kd-tree is O(logn). Here, where we use 5-dim points, kd-tree should be much better.
You might like to try my research project called K-tree. It scales well with large inputs with respect to k-means and forms a hierarchy of clusters. The trade-off is that it produce clusters with higher distortion. It has an average case runtime of O(n log n) and worst case of O(n**2) that only happens if you have some weird topology. More details of the complexity analysis are in my Masters thesis. I have used it with very high dimensional text data and had no problems.
Sometimes bad splits can happen in the tree where all data goes to one side (cluster). The trunk in SVN deals with this differently than the current release. It randomly splits the data if there is a bad split. The previous method can force the tree to become too deep if there are bad splits.
 K-means scales with O(nki), so are you sure your approach actually is superior? It also seems closely related to Bisecting k-means (which should run in O(ni), i.e. scale better to large k).
Put the data into an index such as an R*-tree (Wikipedia), then you can run many density-based clustering algorithms (such as DBSCAN (Wikipedia) or OPTICS (Wikipedia)) in O(n log n).
Density-based clustering (Wikipedia) seems to be precisely what you want ("not too far apart")
 R/R+/R*-Tree and KD-tree seem to be very similar. If you're implementing the data structure yourself, a KD-Tree might be far easier. OTOH, a R-Tree is good for successively adding data, because it's self-balancing, whereas the KD-Tree might get inefficient if it's not completely recomputed after addition of new data. See this SO question.
Below is a little test bench to see how fast
scipy.spatial.cKDTree
is on your data,
and to get a rough idea of how the distances between nearby points scatter.
A nice way to run K-cluster for various K
is to build an MST of nearest pairs, and remove the K-1 longest; see
Wayne, Greedy Algorithms .
Visualizing the clusters would be fun -- project to 2d with PCA ?
(Just curious, is your K 10, 100, 1000 ?)
Added 17 Dec: real runtimes: 100000 x 5 10 sec, 500000 x 5 60sec
#!/usr/bin/env python
# time scipy.spatial.cKDTree build, query

from __future__ import division
import random
import sys
import time
import numpy as np
from scipy.spatial import cKDTree as KDTree
    # http://docs.scipy.org/doc/scipy/reference/spatial.html
    # $scipy/spatial/kdtree.py is slow but clean, 0.9 has cython
__date__ = "2010-12-17 dec denis"

def clumpiness( X, nbin=10 ):
    """ how clumpy is X ? histogramdd av, max """
        # effect on kdtree time ? not much
    N, dim = X.shape
    histo = np.histogramdd( X, nbin )[0] .astype(int)  # 10^dim
    n0 = histo.size - histo.astype(bool).sum()  # uniform: 1/e^lambda
    print "clumpiness: %d of %d^%d data bins are empty  av %.2g  max %d" % (
        n0, nbin, dim, histo.mean(), histo.max())

#...............................................................................
N = 100000
nask = 0  # 0: ask all N
dim = 5
rnormal = .9
    # KDtree params --
nnear = 2  # k=nnear+1, self
leafsize = 10
eps = 1  # approximate nearest, dist <= (1 + eps) * true nearest
seed = 1

exec "\n".join( sys.argv[1:] )  # run this.py N= ...
np.random.seed(seed)
np.set_printoptions( 2, threshold=200, suppress=True )  # .2f
nask = nask or N
print "\nkdtree:  dim=%d  N=%d  nask=%d  nnear=%d  rnormal=%.2g  leafsize=%d  eps=%.2g" % (
    dim, N, nask, nnear, rnormal, leafsize, eps)

if rnormal > 0:  # normal point cloud, .9 => many near 1 1 1 axis
    cov = rnormal * np.ones((dim,dim)) + (1 - rnormal) * np.eye(dim)
    data = np.abs( np.random.multivariate_normal( np.zeros(dim), cov, N )) % 1
        # % 1: wrap to unit cube
else:
    data = np.random.uniform( size=(N,dim) )
clumpiness(data)
ask = data if nask == N  else random.sample( data, sample )
t = time.time()

#...............................................................................
datatree = KDTree( data, leafsize=leafsize )  # build the tree
print "%.1f sec to build KDtree of %d points" % (time.time() - t, N)

t = time.time()
distances, ix = datatree.query( ask, k=nnear+1, eps=eps )
print "%.1f sec to query %d points" % (time.time() - t, nask)

distances = distances[:,1:]  # [:,0] is all 0, point to itself
avdist = distances.mean( axis=0 )
maxdist = distances.max( axis=0 )
print "distances to %d nearest: av" % nnear, avdist, "max", maxdist

# kdtree:  dim=5  N=100000  nask=100000  nnear=2  rnormal=0.9  leafsize=10  eps=1
# clumpiness: 42847 of 10^5 data bins are empty  av 1  max 21
# 0.4 sec to build KDtree of 100000 points
# 10.1 sec to query 100000 points
# distances to 2 nearest: av [ 0.07  0.08] max [ 0.15  0.18]

# kdtree:  dim=5  N=500000  nask=500000  nnear=2  rnormal=0.9  leafsize=10  eps=1
# clumpiness: 2562 of 10^5 data bins are empty  av 5  max 80
# 2.5 sec to build KDtree of 500000 points
# 60.1 sec to query 500000 points
# distances to 2 nearest: av [ 0.05  0.06] max [ 0.13  0.13]
# run: 17 Dec 2010 15:23  mac 10.4.11 ppc 


People have the impression that k-means is slow, but slowness is really only an issue for the EM algorithm (Lloyd's). Stochastic gradient methods for k-means are orders of magnitude faster than EM (see www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf).
An implementation is here: http://code.google.com/p/sofia-ml/wiki/SofiaKMeans

I have a Perl module that does exactly what you want Algorithm::ClusterPoints.
First, it uses the algorithm you have described in your post to divide the points in multidimensional sectors and then it uses brute force to find clusters between points in adjacent sectors.
The complexity varies from O(N) to O(N**2) in very degraded cases. 
update:
@Denis: no, it is much worse:
For d dimensions, the sector (or little hypercube) size s is determined so that its diagonal l is the minimum distance c allowed between two points in different clusters.
l = c
l = sqrt(d * s * s)
s = sqrt(c * c / d) = c / sqrt(d)

Then you have to consider all the sectors that touch the hypersphere with diameter r = 2c + l centered in the pivot sector.
Roughly, we have to consider ceil(r/s) rows of sectors in every directions and that means n = pow(2 * ceil(r/s) + 1, d).
For instance, for d=5 and c=1 we get l=2.236, s=0.447, r=3.236 and n=pow(9, 5)=59049
Actually we have to check less neighbor sectors as here we are considering those that touch the hypercube of size (2r+1)/s and we only need to check those touching the circumscribed hypersphere.
Considering the bijective nature of the "are on the same cluster" relation we can also half the number of sectors that have to be checked.
Specifically, Algorithm::ClusterPoints for the case where d=5 checks 3903 sectors.
 Thanks! It sounds like what you're doing is dividing space into a grid, then using something smarter than my greedy algorithm to locally find clusters.  Too bad I'm mediocre at programming and don't know perl so I'm only using python. Also, for extra credit I actually desire to run my algorithm for a full set of 1M points, but later be able to quickly see what the best clusters would have been if I'd only used various subsets (say 500K). The hash style approaches discussed in the previous answer may accomplish this as I only need to redo the post processing. so each sector (little hypercube) looks at 3^5 - 1 = 242 neighbours in 5d ? wow, going up fast with dim.  Real runtimes would be interesting.How would one use Kernel Density Estimation as a 1D clustering method in scikit learn? 
I need to cluster a simple univariate data set into a preset number of clusters. Technically it would be closer to binning or sorting the data since it is only 1D, but my boss is calling it clustering, so I'm going to stick to that name. 
The current method used by the system I'm on is K-means, but that seems like overkill.
Is there a better way of performing this task?
Answers to some other posts are mentioning KDE (Kernel Density Estimation), but that is a density estimation method, how would that work? 
I see how KDE returns a density, but how do I tell it to split the data into bins? 
How do I have a fixed number of bins independent of the data (that's one of my requirements) ? 
More specifically, how would one pull this off using scikit learn? 
My input file looks like: 
 str ID     sls
 1           10
 2           11 
 3            9
 4           23
 5           21
 6           11  
 7           45
 8           20
 9           11
 10          12

I want to group the sls number into clusters or bins, such that:
Cluster 1: [10 11 9 11 11 12] 
Cluster 2: [23 21 20] 
Cluster 3: [45] 

And my output file will look like: 
 str ID     sls    Cluster ID  Cluster centroid
    1        10       1               10.66
    2        11       1               10.66
    3         9       1               10.66 
    4        23       2               21.33   
    5        21       2               21.33
    6        11       1               10.66
    7        45       3               45
    8        20       2               21.33
    9        11       1               10.66 
    10       12       1               10.66

 What is the concern with k-means? Performance? kmeans is more efficient than kde @DavidMaust 1) When I tried running sklearn's k-means on univariate data, I started getting errors. I had to trick it by having it cluster on 2d data which was identical copies of the original 1d data. 2) According to this post it's a bad idea. @lejlot see my reply to David Maust. Have you tried writing some code?
Write code yourself. Then it fits your problem best!
Boilerplate: Never assume code you download from the net to be correct or optimal... make sure to fully understand it before using it.
%matplotlib inline

from numpy import array, linspace
from sklearn.neighbors.kde import KernelDensity
from matplotlib.pyplot import plot

a = array([10,11,9,23,21,11,45,20,11,12]).reshape(-1, 1)
kde = KernelDensity(kernel='gaussian', bandwidth=3).fit(a)
s = linspace(0,50)
e = kde.score_samples(s.reshape(-1,1))
plot(s, e)


from scipy.signal import argrelextrema
mi, ma = argrelextrema(e, np.less)[0], argrelextrema(e, np.greater)[0]
print "Minima:", s[mi]
print "Maxima:", s[ma]
> Minima: [ 17.34693878  33.67346939]
> Maxima: [ 10.20408163  21.42857143  44.89795918]

Your clusters therefore are
print a[a < mi[0]], a[(a >= mi[0]) * (a <= mi[1])], a[a >= mi[1]]
> [10 11  9 11 11 12] [23 21 20] [45]

and visually, we did this split:
plot(s[:mi[0]+1], e[:mi[0]+1], 'r',
     s[mi[0]:mi[1]+1], e[mi[0]:mi[1]+1], 'g',
     s[mi[1]:], e[mi[1]:], 'b',
     s[ma], e[ma], 'go',
     s[mi], e[mi], 'ro')


We cut at the red markers. The green markers are our best estimates for the cluster centers.
 I would be hesitant to call this method better than k-means. It does involve selecting an arbitrary bandwidth and then calculating 50 density estimates. That being said, I don't know if there is a better way to do it with kernel density estimation. You don't have to know k. You not only get better centers (less affected by outliers) but also sound splitting points (not only at half the way). There is plenty of literature on the bandwidth such as Silverman's rule. Also. who cares about computing 50 density estimates? You could precompute the kernel and do this in a fast convolution. I will also add that this is a particularly fast, non-linear scaling method to 1D clustering. hi I have posted a question about this answer, could you pls help me about it?  stackoverflow.com/questions/60355497/…Scikit-learn: How to run KMeans on a one-dimensional array? 
I have an array of 13.876(13,876) values between 0 and 1. I would like to apply sklearn.cluster.KMeans to only this vector to find the different clusters in which the values are grouped. However, it seems KMeans works with a multidimensional array and not with one-dimensional ones. I guess there is a trick to make it work but I don't know how. I saw that KMeans.fit() accepts "X : array-like or sparse matrix, shape=(n_samples, n_features)", but it wants the n_samples to be bigger than one
I tried putting my array on a np.zeros() matrix and run KMeans, but then is putting all the non-null values on class 1 and the rest on class 0. 
Can anyone help in running this algorithm on a one-dimensional array? 
Thanks a lot!

You have many samples of 1 feature, so you can reshape the array to (13,876, 1) using numpy's reshape:
from sklearn.cluster import KMeans
import numpy as np
x = np.random.random(13876)

km = KMeans()
km.fit(x.reshape(-1,1))  # -1 will be calculated to be 13876 here

 It worked like a charm, this detail got me stuck for one hour! If one uses MiniBatchKmeans on this kind of shaped data, one gets very different results. Is this expected behaviour?? this is related to the random state. If one fixes it, one gets the same results. can anyone help me to plot the cluster formed by the above code.
Read about Jenks Natural Breaks. Function in Python found the link from the article:
def get_jenks_breaks(data_list, number_class):
    data_list.sort()
    mat1 = []
    for i in range(len(data_list) + 1):
        temp = []
        for j in range(number_class + 1):
            temp.append(0)
        mat1.append(temp)
    mat2 = []
    for i in range(len(data_list) + 1):
        temp = []
        for j in range(number_class + 1):
            temp.append(0)
        mat2.append(temp)
    for i in range(1, number_class + 1):
        mat1[1][i] = 1
        mat2[1][i] = 0
        for j in range(2, len(data_list) + 1):
            mat2[j][i] = float('inf')
    v = 0.0
    for l in range(2, len(data_list) + 1):
        s1 = 0.0
        s2 = 0.0
        w = 0.0
        for m in range(1, l + 1):
            i3 = l - m + 1
            val = float(data_list[i3 - 1])
            s2 += val * val
            s1 += val
            w += 1
            v = s2 - (s1 * s1) / w
            i4 = i3 - 1
            if i4 != 0:
                for j in range(2, number_class + 1):
                    if mat2[l][j] >= (v + mat2[i4][j - 1]):
                        mat1[l][j] = i3
                        mat2[l][j] = v + mat2[i4][j - 1]
        mat1[l][1] = 1
        mat2[l][1] = v
    k = len(data_list)
    kclass = []
    for i in range(number_class + 1):
        kclass.append(min(data_list))
    kclass[number_class] = float(data_list[len(data_list) - 1])
    count_num = number_class
    while count_num >= 2:  # print "rank = " + str(mat1[k][count_num])
        idx = int((mat1[k][count_num]) - 2)
        # print "val = " + str(data_list[idx])
        kclass[count_num - 1] = data_list[idx]
        k = int((mat1[k][count_num] - 1))
        count_num -= 1
    return kclass

Use and visualization:
import numpy as np
import matplotlib.pyplot as plt

def get_jenks_breaks(...):...

x = np.random.random(30)
breaks = get_jenks_breaks(x, 5)

for line in breaks:
    plt.plot([line for _ in range(len(x))], 'k--')

plt.plot(x)
plt.grid(True)
plt.show()

Result:

Finding 2 & 3 word Phrases Using R TM Package 
I am trying to find a code that actually works to find the most frequently used two and three word phrases in R text mining package (maybe there is another package for it that I do not know). I have been trying to use the tokenizer, but seem to have no luck. 
If you worked on a similar situation in the past, could you post a code that is tested and actually works? Thank you so much!
 Ordered phrases, that is? Or co-occurences? Both would be useful.  Thank you!
You can pass in a custom tokenizing function to tm's DocumentTermMatrix function, so if you have package tau installed it's fairly straightforward.
library(tm); library(tau);

tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))

Where n in the tokenize_ngrams function is the number of words per phrase. This feature is also implemented in package RTextTools, which further simplifies things.
library(RTextTools)
texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
matrix <- create_matrix(texts,ngramLength=3)

This returns a class of DocumentTermMatrix for use with package tm.
 I realize this is a pretty stale thread, but has anybody tried this recently? In my hands, the first method gives the following error: > matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams)) Error in simple_triplet_matrix(i = i, j = j, v = as.numeric(v), nrow = length(allTerms),  :    'i, j, v' different lengths In addition: Warning messages: 1: In mclapply(unname(content(x)), termFreq, control) :   all scheduled cores encountered errors in user code 2: In simple_triplet_matrix(i = i, j = j, v = as.numeric(v), nrow = length(allTerms),  :   NAs introduced by coercion. I get the same error, @MAndrecPhD, when trying the library(RTextTools) example. I have the same problem. I have seen some people suggest that SnowballC package will solve it, but it does not for me. Any suggestions? If I add the following the simple_triplet_matrix error no longer appears  options(mc.cores=1)  however I get the following error instead Error in FUN(X[[i]], ...) : non-character argument
This is part 5 of the FAQ of the tm package:

5. Can I use bigrams instead of single tokens in a term-document matrix?
Yes. RWeka provides a tokenizer for arbitrary n-grams which can be
  directly passed on to the term-document matrix constructor. E.g.:

  library("RWeka")
  library("tm")

  data("crude")

  BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
  tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))

  inspect(tdm[340:345,1:10])

 This did the trick for me. Actually, the current version of the FAQ has a solution that doesn't require RWeka: tm.r-forge.r-project.org/faq.html#Bigrams if you can figure out how to get rJava working so that you can use the RWeka package. Would be nice to see a non-Java-dependent solution.
This is my own made up creation for different purposes but I think may applicable to your needs too:
#User Defined Functions
Trim <- function (x) gsub("^\\s+|\\s+$", "", x)

breaker <- function(x) unlist(strsplit(x, "[[:space:]]|(?=[.!?*-])", perl=TRUE))

strip <- function(x, digit.remove = TRUE, apostrophe.remove = FALSE){
    strp <- function(x, digit.remove, apostrophe.remove){
        x2 <- Trim(tolower(gsub(".*?($|'|[^[:punct:]]).*?", "\\1", as.character(x))))
        x2 <- if(apostrophe.remove) gsub("'", "", x2) else x2
        ifelse(digit.remove==TRUE, gsub("[[:digit:]]", "", x2), x2)
    }
unlist(lapply(x, function(x) Trim(strp(x =x, digit.remove = digit.remove, 
    apostrophe.remove = apostrophe.remove)) ))
}

unblanker <- function(x)subset(x, nchar(x)>0)

#Fake Text Data
x <- "I like green eggs and ham.  They are delicious.  They taste so yummy.  I'm talking about ham and eggs of course"

#The code using Base R to Do what you want
breaker(x)
strip(x)
words <- unblanker(breaker(strip(x)))
textDF <- as.data.frame(table(words))
textDF$characters <- sapply(as.character(textDF$words), nchar)
textDF2 <- textDF[order(-textDF$characters, textDF$Freq), ]
rownames(textDF2) <- 1:nrow(textDF2)
textDF2
subset(textDF2, characters%in%2:3)

 Hi, @Tyler-Rinker, I know this is a few years old now, but i get this error when testing your code: ` Error in FUN(c("", "", "", "", "", "", "", "", "", "", "", "", "", "",  :    could not find function "Trim" ` Added Trim if that helps. haha. Thanks, @Tyler_Rinker. I had a function of the exact same called trim but I didn't realize that was what it was looking for.  Thanks!
The corpus library has a function called term_stats that does what you want:
library(corpus)
corpus <- gutenberg_corpus(55) # Project Gutenberg #55, _The Wizard of Oz_
text_filter(corpus)$drop_punct <- TRUE # ignore punctuation
term_stats(corpus, ngrams = 2:3)
##    term             count support
## 1  of the             336       1
## 2  the scarecrow      208       1
## 3  to the             185       1
## 4  and the            166       1
## 5  said the           152       1
## 6  in the             147       1
## 7  the lion           141       1
## 8  the tin            123       1
## 9  the tin woodman    114       1
## 10 tin woodman        114       1
## 11 i am                84       1
## 12 it was              69       1
## 13 in a                64       1
## 14 the great           63       1
## 15 the wicked          61       1
## 16 wicked witch        60       1
## 17 at the              59       1
## 18 the little          59       1
## 19 the wicked witch    58       1
## 20 back to             57       1
## ⋮  (52511 rows total)

Here, count is the number of appearances, and support is the number of documents containing the term.

I add a similar problem by using tm and ngram packages.
After debugging mclapply, I saw there where problems on documents with less than 2 words with the following error
   input 'x' has nwords=1 and n=2; must have nwords >= n

So I've added a filter to remove document with low word count number:
    myCorpus.3 <- tm_filter(myCorpus.2, function (x) {
      length(unlist(strsplit(stringr::str_trim(x$content), '[[:blank:]]+'))) > 1
    })

Then my tokenize function looks like:
bigramTokenizer <- function(x) {
  x <- as.character(x)

  # Find words
  one.list <- c()
  tryCatch({
    one.gram <- ngram::ngram(x, n = 1)
    one.list <- ngram::get.ngrams(one.gram)
  }, 
  error = function(cond) { warning(cond) })

  # Find 2-grams
  two.list <- c()
  tryCatch({
    two.gram <- ngram::ngram(x, n = 2)
    two.list <- ngram::get.ngrams(two.gram)
  },
  error = function(cond) { warning(cond) })

  res <- unlist(c(one.list, two.list))
  res[res != '']
}

Then you can test the function with:
dtmTest <- lapply(myCorpus.3, bigramTokenizer)

And finally:
dtm <- DocumentTermMatrix(myCorpus.3, control = list(tokenize = bigramTokenizer))


Try tidytext package
library(dplyr)
library(tidytext)
library(janeaustenr)
library(tidyr

)
Suppose I have a dataframe CommentData that contains comment column and I want to find occurrence of two words together. Then try
bigram_filtered <- CommentData %>%
  unnest_tokens(bigram, Comment, token= "ngrams", n=2) %>%
  separate(bigram, c("word1","word2"), sep=" ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort=TRUE)

The above code creates tokens, and then remove stop words that doesn't help in analysis(eg. the,an,to etc.) Then you count occurrence of these words. You will be then using unite function to combine individual words and record their occurrence.
bigrams_united <- bigram_filtered %>%
  unite(bigram, word1, word2, sep=" ")
bigrams_united


Try this code. 
library(tm)
library(SnowballC)
library(class)
library(wordcloud)

keywords <- read.csv(file.choose(), header = TRUE, na.strings=c("NA","-","?"))
keywords_doc <- Corpus(VectorSource(keywords$"use your column that you need"))
keywords_doc <- tm_map(keywords_doc, removeNumbers)
keywords_doc <- tm_map(keywords_doc, tolower)
keywords_doc <- tm_map(keywords_doc, stripWhitespace)
keywords_doc <- tm_map(keywords_doc, removePunctuation)
keywords_doc <- tm_map(keywords_doc, PlainTextDocument)
keywords_doc <- tm_map(keywords_doc, stemDocument)

This is the bigrams or tri grams section that you could use
BigramTokenizer <-  function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
# creating of document matrix
keywords_matrix <- TermDocumentMatrix(keywords_doc, control = list(tokenize = BigramTokenizer))

# remove sparse terms 
keywords_naremoval <- removeSparseTerms(keywords_matrix, 0.95)

# Frequency of the words appearing
keyword.freq <- rowSums(as.matrix(keywords_naremoval))
subsetkeyword.freq <-subset(keyword.freq, keyword.freq >=20)
frequentKeywordSubsetDF <- data.frame(term = names(subsetkeyword.freq), freq = subsetkeyword.freq) 

# Sorting of the words
frequentKeywordDF <- data.frame(term = names(keyword.freq), freq = keyword.freq)
frequentKeywordSubsetDF <- frequentKeywordSubsetDF[with(frequentKeywordSubsetDF, order(-frequentKeywordSubsetDF$freq)), ]
frequentKeywordDF <- frequentKeywordDF[with(frequentKeywordDF, order(-frequentKeywordDF$freq)), ]

# Printing of the words
wordcloud(frequentKeywordDF$term, freq=frequentKeywordDF$freq, random.order = FALSE, rot.per=0.35, scale=c(5,0.5), min.freq = 30, colors = brewer.pal(8,"Dark2"))

Hope this helps. This is an entire code that you could use.
 I have tried all of the solutions but none was working with my data. And I don't know why. The result is alway 1gram (i.e. one word) no matter what value I leave in the ngams function (2, 3, 4, etc.)Using frequent itemset mining to build association rules? 
I am new to this area as well as the terminology so please feel free to suggest if I go wrong somewhere. I have two datasets like this:
Dataset 1:
A B C 0 E
A 0 C 0 0
A 0 C D E
A 0 C 0 E

The way I interpret this is at some point in time, (A,B,C,E) occurred together and so did (A,C), (A,C,D,E) etc.
Dataset 2:
5A 1B 5C  0 2E
4A  0 5C  0  0
2A  0 1C 4D 4E
3A  0 4C  0 3E

The way I interpret this is at some point in time, 5 occurrences of A, 1 occurrence of B, 5 occurrences of C and 2 occurrences of E happened and so on.
I am trying to find what items occur together and if possible, also find out the cause and effect for this. For this, I am not understanding how to go about using both the datasets (or if one is enough). It would be good to have a good tutorial on this but my primary question is which dataset to utilize and how to proceed in (i) building a frequent itemset and (ii) building association rules between them.
Can someone point me to a practical tutorials/examples (preferably in Python) or at least explain in brief words on how to approach this problem?

Some theoretical facts about association rules:

Association rules is a type of undirected data mining that finds patterns in the data where the target is not specified beforehand. Whether the patterns make sense is left to human interpretation.
The goal of association rules is to detect relationships or association between specific values of categorical variables in large sets.
And is rules can intrepreted as "70% of the the customers who buy wine and cheese also buy grapes".

To find association rules, you can use apriori algorithm. There already exists many python  implementation, although most of them are not efficient for practical usage:

source1: http://code.google.com/p/autoflash/source/browse/trunk/python/apriori.py?r=31
source2: http://www.nullege.com/codes/show/src%40l%40i%40libbyr-HEAD%40test_freq_item_algos.py/5/apriori/python

or use Orange data mining library, which has a good library for association rules.
Usage example:
'''
save first example as item.basket with format
A, B, C, E
A, C
A, C, D, E
A, C, E
open ipython same directory as saved file or use os module
>>> import os
>>> os.chdir("c:/orange")
'''
import orange

items = orange.ExampleTable("item")
#play with support argument to filter out rules
rules = orange.AssociationRulesSparseInducer(items, support = 0.1) 
for r in rules:
    print "%5.3f %5.3f %s" % (r.support, r.confidence, r)

To learn more about association rules/frequent item mining, then my selection of books are:

"Introduction to Data mining" - Vipin Kumar, best book for beginner
"Data mining and knowledge discovery handbook", for advanced user
"Mining massive data" - tips how to use in reallife and how build efficient solutions, free book, http://i.stanford.edu/~ullman/mmds.html
Ofcourse there are many fantastic scientific papers to read: by example do some search on MS Acedemic about Frequent Pattern mining 

There is no short way.
 This is more helpful than the orange documentation on their association rules class. Thank you!
It seems like a neat way to handle this type of problems is using a Bayesian network. In particular as a Bayesian network structure learning problem. Once you have that you will be able to efficiently answer questions like p(A=1|B=0 and C=1) and so on.

If you have quantities for each items, then you could consider "high utility itemset mining". It is the problem of itemset mining but adapted for the case where items can have quantities in each transaction and also each item can have a weight.
If you just use the basic Apriori, then you would loose the information about quantities.
Guided mining of common substructures in large set of graphs 
I have a large (>1000) set of directed acyclic graphs with a large (>1000) set of vertices each. The vertices are labeled, the label's cardinality is small (< 30)
I want to identify (mine) substructures that appear frequently over the whole set of graphs.

A substructure is a graph of at least two directly connected vertices with specific labels. Such a substructure may appear once or more in one or more of the given input graphs. For example "a [vertex labeled A with two directly connected children labeled B] appears twice in graph U and once in graph V".
A substructure we are looking for must obey a set of pre-given rules which filter on the vertices' labels. As an example: A substructure that contains a vertex labeled A is interesting if the sub-graph is "a vertex labeled A that has at least one directly connected child labeled B and is not a directly connected sibling of a vertex labeled U or V". Substructures that do not conform to these rules may appear in the input graphs but are not of interest for the search.

The output we are looking for is a list of substructures and their (number of) appearances in the given graphs.
I have tried to look into things and (as it seems to always happen with me) the problem is NP-complete. As far as I can see gSpan is the most common algorithm to solve this problem. However, as stated above, I'm not looking for any common substructure in the graphs but only those that obey certain rules. One should be able so use that in order to reduce the search space.
Any insight on how to approach this problem?
Update: I should probably add that the aforementioned rules can be recursive up to a certain degree. For example "a vertex labeled A with at least two children labeled B, each having at least one child labeled A". The maximum recursion depth is somewhere between 1 and 10.
Update II: Pointing out that we are not searching for known or preferred substructures but mining them. There is no spoon needle. 
 Is there any limit on the number of possible labels? is there a specific minimum occurrence count of a "frequent" substructure? Also, if you're looking to solve this problem for an arbitrary set of pre-given rules (as seems to be the case), then you can't really rely on that to substantially reduce the search space, at least asymptotically There are less than 30 labels; a "frequent" substructure appears at least twice. A vertex can have more than one edge to like-labelled vertices. Short gSnap reference: cs.ucsb.edu/~xyan/papers/gSpan-short.pdf Is this the only constraint on substructures? The example substructure is much more specific. Does child refer to a direct child in the example? one issue i can see is that your 'filter' constraints can be empty in which case you'd still have to solve the np-hard problem.
I'm assuming in my answer that you are trying to minimize the running time and not wanting to spend an excessive amount of time writing the code to do it. One thing that I struggled with at first when learning to write highly efficient algorithms was that sometimes multiple passes can be way more efficient. In this case, I would say, fundamentally, you want to have two passes:
First, create a filter that allows you to ignore most (if not all) non-duplicated patterns. In order to do this:

Allocate two bit arrays (and consider cache sizes when doing this). The first will be a simple bloom filter. The second will be a duplicate bloom filter.
As you traverse the structure on a first pass, for each indexable structure, compute a hash value. Select the appropriate bit in your first bloom filter and set it. If that bit was already set, also set the corresponding bit in the duplicate bloom filter.

On your second pass, you will need to do the "heavier" process of actually confirming matches. In order to accomplish this:

Scan over the graph again and record any structures that match the duplicate bloom filter generated in the first pass.
Place those that match in a hash table (ideally using different bits from the hash computed)
When a duplicate is detected, store that information off where ever you'd like to collect it.

This algorithm will run very quickly on large datasets because it will significantly reduce the pressure on the appropriate cache level. There are also several enhancements that you can make in order to make it perform better in different circumstances.

In order to improve performance on a multithreaded system, it is actually safe to parallelize the first step. To do this, give each thread (or computer in a cluster) a piece of the graph. Each should compute its own copy of the two blooms. The blooms may then be combined into a final bloom. The reduction function is just (present, duplicate) = (present1 OR present2, duplicate1 OR duplicate2 OR (present1 AND present2)). This step is VERY fast. 
It is also completely safe to parallelize the second step, but it must be modified slightly. In order to do that, you will take the duplicate bloom filter from the first step and use that as a filter in the second step, the same as before. However, you can't complete the final comparison as easily. You must instead place the potential duplicates in hash buckets. Then, after each shard of data has been written into its own list of potential duplicate hash table, divide the data up by hash bucket and in a third step find the duplicates. Each hash bucket (from any output in the second step) must be processed by the same worker.
In cases where you have a large number of structures that you are indexing, you may improve performance by recursively applying the above algorithm. The adjustment is that you use each matching category for the output from the above algorithm as your input into the recursive pass. For example, if you index only structures that have up to 5 items in the first run of the algorithm, you can, when you recurse, take each set of duplicated subgraphs and run the algorithm on only those sub-graphs. This would only be necessary with very large sets of data, obviously.
Another adjustment you may consider if the graph is very large in order to increase the effectiveness of your bloom filters is to iterate on the algorithm. In the first pass, for example, you might only consider sub-graphs that have the first label as the base of the sub-graph. This would decrease the size required of your bloom filters and/or allow you to filter out more sub-graphs on the first pass.

A couple notes for tweaking the above:

Consider cache sizes. For example, on an Intel Haswell chip, each core has 32K in L1 cache and 256K in L2 cache. Each cache line will contain 512 bits, so if you fill up 1% of your bloom filter, most of the cache lines will be touched. Depending on how fast other parts of the algorithm are and given that other stuff uses these caches, you can safely create a bloom filter that has up to around 512 * 1024 entries (8 entries per bit per filter = 128k, on hyperthreaded systems, that is how much L2 you get) and still maintain most of the data set in L2 cache and the really active stuff in L1. For smaller datasets, keep this number down because there is no point in making it large. If you are only flagging features as potential duplicates when they aren't less than 1% of the time, that's totally fine.
Parallelizing this is, again, only really useful in cases where you have tons of data. I'm assuming that you might. If you do parallelize, you should consider the geometry. Placing partial sets of data on each computer will work with this algorithm. You can then run each iteration (in variation #4) on each computer. In cases where you have huge datasets that will avoid having to transfer all the data to all the computers.

Anyway, to sum up with a statement on the run-time complexity, I will say that it really depends. Many people ignore the fact that increasing the working set of data will cause memory accesses to not all be equal in cost. Fundamentally, the above algorithm, while highly performant, if tuned appropriately, will run very fast on a small data-set, but it really shines with much larger datasets because it allows high efficiency ways of keeping the working set of data in whatever cache level is appropriate (L1, L2, L3, RAM, local disk, local network, etc.) The complexity of the algorithm will depend on the data, but I do not believe an algorithm much faster can be created. I did leave out how you represent the subgraphs and there is work to be done there to achieve the optimal algorithm, but without knowing more, I can't determine the best way to store that information.
The reason that an algorithm can't run much faster than the one I've presented is that the first pass will require much less work to run than the second because it doesn't require branching and it is less work to do bitwise operations. We can therefore say that it adds little to the overall work we're doing. The second stage is also about as efficient as is possible. You must (barring a way to perfectly describe each possibility with a finite set of bits, which I'll explain a second) actually compare each graph feature and write the information somewhere. The only variable is how much work it is to check whether you need to do this. Checking a bit where you can arbitrarily scale the error rate towards 0% is as good as you can get.
For smallish datasets, the reason that two passes benefit you is that you may have a much larger number of bloom cardinality in a smaller amount of memory. But for really small sets of data, you might as well just use the second step and ignore the first. But, at a minimum, you'll need to store a pointer for each hash target. This means that you will need to write 32 or 64 times as much data for the same level of filtering. For small enough datasets, this doesn't matter because a read is a read and a write is a write, but for larger datasets, this can allow you to accomplish the same level of filtering while staying in a given level of cache. In cases where you must work across multiple computers or threads, the mechanism provided in this algorithm will be WAY more efficient as the data can be combined much faster and much more information about potential matches can be exchanged.
Now, lastly, as I alluded to, you may be able to get slightly better if the number of features that you check for on each iteration is reduced further. For example, if you are only checking for 32 possible labels and the number of children with a particular label in each pass (and this is bounded to 1024), you could represent this perfectly with 15 bits. If you limited the count to 255, you could store this information perfectly with 32K. In order to pull this off in your case, you'd need to use the iteration, recursion and sharding strategies that I mentioned above and you'd need to then also track the source graph, and some other information. I honestly doubt this would work well except in very limited situations, but I'm including it for completeness.
Anyway, this is my first answer on Stack Overflow, so don't be too hard on me. I hope this was helpful!
 I don't know anyone who started out with a canonical answer. You did good.
They way I read your question, you may want something like the code below.
It finds all matching subgraphs in a DAG in linear time.
It doesn't support filters, but you can check the results after they are found, and filter them manually.
It also may find graphs with some parts collapsed. Say you are looking for a tree a((b|c)|(c|d)), then it might find a subgraph, where the c node is shared between the two subtrees.
Again, you can inspect the output and filter out results like that.
Doing such an inspection is of course only possible if the output size is not too large. For that you will have to do some experiments on your own graphs.
from collections import namedtuple, defaultdict
Node = namedtuple('Node', ['label', 'children', 'id'])

# Simple tree patternA(B|AB)
pattern = Node('A', (Node('B', (), 1),
                     Node('A', (Node('B', (), 3),), 2)), 0)

# Generate random DAG
import random
labels = 'ABCDE'
dag = []
for _ in range(1000):
    label = random.choice(labels)
    children = tuple(random.sample(dag, min(len(dag)//2, 10)))
    dag.append(Node(label, children, len(dag)))

# Helper
def subtrees(pattern):
    yield pattern
    for sub in pattern.children:
        yield from subtrees(sub)

colors = defaultdict(list)
# Iterate the nodes in topologically sorted order
for node in dag:
    # Check if the node is the head of some sub-pattern
    for sub in subtrees(pattern):
        if node.label == sub.label \
                and all(any(sc.id in colors[nc.id]
                    for nc in node.children) for sc in sub.children):
            # If so, color the node with that sub-pattern's color
            colors[node.id].append(sub.id)

matches = [node for node in dag if pattern.id in colors[node.id]]
print('Found {} matches.'.format(len(matches)))

I believe this is very similar to the approach Stefan Haustein had in mind.

Edit: Here is what I'd start from:

Build an index of the 30x30 possible parent/child combinations to the corresponding nodes
Intersect the matches for a given substructure
Check further conditions manually

(Original post):

Find a way to build hash keys for substructures
Build a hash map from substructures to the corresponding nodes
Find candidates using the hash map, check the detailed conditions manually

 (1. looks hard, 2. memory intensive.) @graybeard For the example, 1. could just be a string hash for "abb". But what I have overlooked is that you'd also need to insert "a", "ab" and all other combinations. So unless the edge count is limited for each node, this might explode. Re 2) A hash map with a small multiple of about one million entries seems manageable, though?
Your question:

You have - A set of graphs and a set of rules (Let's call the rule a substructure pattern).
You want - A count of the occurrence of each of the substructure in the set of graphs.


Since, the graphs are DAGs, in the substructure search you won't be caught in cycle.
The simple solution pseudocode is:
for each graph G {                           //Sub-problem 4
    for each node N {                        //Sub-problem 3
        for each substructure pattern P {    //Sub-problem 2
            if N's structure is like P {     //Sub-problem 1
                PatternCountMap.Get(G).Get(P)++;
            }
        }
    }
}

At each place I have marked the sub-problem that needs to be handled.
If you don't know Map-Reduce, my solution won't be entirely clear to you. Let me know if that's the case. In general, the Map-Reduce code can always be run in a general programming fashion, except that it will take longer time for large data.

Sub-Problem 1
This problem can be simply written as:

Given a 'Root' node and given a pattern P, does the tree represented with this node as root follow the given pattern?

This problem is solvable. Simply travel down the graph starting from the 'root' and see if pattern is being followed. If it is, increase its count in the PatternCountMap, otherwise move on to the next pattern and see if the 'root' follows the next pattern.
The PatternCountMap is a HashMap>, which maps the Graphs to another HashMap which maps Patterns to their frequency. So, if P is found in Graphs G1 and G2, 12 and 17 times respectively, then PatternCountMap.Get(G1).Get(P) will be 12 and PatternCountMap.Get(G2).Get(P) will be 17 at the end of this algorithm's run.
Useful Hint: Since you do not want to recurse too deep, use iterative solutions. If you have to perform DFS, perform iterative DFS using a stack. The iterative DFS algorithm is pretty easy.

Sub-problem 2
Here we are just looping over each pattern (or rules). No magic here. For each rule we see if the node N of Graph G follows the rule.
Useful Hint: Preprocess the rules. For example, if one rule is followed, see what other rules can definitely not be followed to skip them. Or, if following one pattern means that another one can be followed too, see if the second rule can be shrunk because of the checking already done as part of the first one.

Sub-problem 3 & 4
These two are simple loops again like the Sub-problem 2. But there is one idea that can be applied here. And that is Map-Reduce (though [1]Map-Reduce does not 100% qualify for these problems).
You have numerous nodes from numerous different graphs. As long as you can identify the graph to which the node belongs, if a particular node follows a certain pattern, you can emit <N_G, P>, which means that Node N in Graph G follows the pattern (aka rule) P.
The map output can be collected in the reducers which can populate the PatternCountMap with the values. Much of that is handled by the Map-Reduce framework itself so a lot of things will be taken care of automatically for you.

After you have the PatternCountMap created, you have the count of each useful pattern in each graph and that is what you wanted.

[1]Map-Reduce is for problems that can be solved on commodity hardware. If the rules you are mining are complex, then commodity hardware may not be the one you want to run your algorithm on.
What is the difference between Big Data and Data Mining? [closed] 






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                
Closed 4 years ago.




As Wikpedia states 

The overall goal of the data mining process is to extract information
  from a data set and transform it into an understandable structure for
  further use

How is this related with Big Data? Is it correct if I say that Hadoop is doing data mining in a parallel manner? 
 This is not a good question for StackOverflow, because it is too opinion-based. This question appears to be off-topic because it is not about programming.
Big data is everything
Big data is a marketing term, not a technical term. Everything is big data these days. My USB stick is a "personal cloud" now, and my harddrive is big data. Seriously. This is a totally unspecific term that is largely defined by what the marketing departments of various very optimistic companies can sell - and the C*Os of major companies buy, in order to make magic happen. Update: and by now, the same applies to data science. It's just marketing.
Data mining is the old big data
Actually, data mining was just as overused... it could mean anything such as 

collecting data (think NSA)
storing data
machine learning / AI (which predates the term data mining)
non-ML data mining (as in "knowledge discovery", where the term data mining was actually coined; but where the focus is on new knowledge, not on learning of existing knowledge)
business rules and analytics
visualization
anything involving data you want to sell for truckloads of money

It's just that marketing needed a new term. "Business intelligence", "business analytics", ... they still keep on selling the same stuff, it's just rebranded as "big data" now.
Most "big" data mining isn't big
Since most methods - at least those that give interesting results - just don't scale, most data "mined" isn't actually big. It's clearly much bigger than 10 years ago, but not big as in Exabytes. A survey by KDnuggets had something like 1-10 GB being the average "largest data set analyzed". That is not big data by any data management means; it's only large by what can be analyzed using complex methods. (I'm not talking about trivial algorithms such a k-means).
Most "big data" isn't data mining
Now "Big data" is real. Google has Big data, and CERN also has big data. Most others probably don't. Data starts being big, when you need 1000 computers just to store it.
Big data technologies such as Hadoop are also real. They aren't always used sensibly (don't bother to run hadoop clusters less than 100 nodes - as this point you probably can get much better performance from well-chosen non-clustered machines), but of course people write such software.
But most of what is being done isn't data mining. It's Extract, Transform, Load (ETL), so it is replacing data warehousing. Instead of using a database with structure, indexes and accelerated queries, the data is just dumped into hadoop, and when you have figured out what to do, you re-read all your data and extract the information you really need, tranform it, and load it into your excel spreadsheet. Because after selection, extraction and transformation, usually it's not "big" anymore.
Data quality suffers with size
Many of the marketing promises of big data will not hold. Twitter produces much less insights for most companies than advertised (unless you are a teenie rockstar, that is); and the Twitter user base is heavily biased. Correcting for such a bias is hard, and needs highly experienced statisticians.
Bias from data is one problem - if you just collect some random data from the internet or an appliction, it will usually be not representative; in particular not of potential users. Instead, you will be overfittig to the existing heavy-users if you don't manage to cancel out these effects.
The other big problem is just noise. You have spam bots, but also other tools (think Twitter "trending topics" that cause reinforcement of "trends") that make the data much noiser than other sources. Cleaning this data is hard, and not a matter of technology but of statistical domain expertise. For example Google Flu Trends was repeatedly found to be rather inaccurate. It worked in some of the earlier years (maybe because of overfitting?) but is not anymore of good quality.
Unfortunately, a lot of big data users pay too little attention to this; which is probably one of the many reasons why most big data projects seem to fail (the others being incompetent management, inflated and unrealistic expectations, and lack of company culture and skilled people).
Hadoop != data mining
Now for the second part of your question. Hadoop doesn't do data mining. Hadoop manages data storage (via HDFS, a very primitive kind of distributed database) and it schedules computation tasks, allowing you to run the computation on the same machines that store the data. It does not do any complex analysis.
There are some tools that try to bring data mining to Hadoop. In particular, Apache Mahout can be called the official Apache attempt to do data mining on Hadoop. Except that it is mostly a machine learning tool (machine learning != data mining; data mining sometimes uses methods from machine learning). Some parts of Mahout (such as clustering) are far from advanced. The problem is that Hadoop is good for linear problems, but most data mining isn't linear. And non-linear algorithms don't just scale up to large data; you need to carefully develop linear-time approximations and live with losses in accuracy - losses that must be smaller than what you would lose by simply working on smaller data.
A good example of this trade-off problem is k-means. K-means actually is a (mostly) linear problem; so it can be somewhat run on Hadoop. A single iteration is linear, and if you had a good implementation, it would scale well to big data. However, the number of iterations until convergence also grows with data set size, and thus it isn't really linear. However, as this is a statistical method to find "means", the results actually do not improve much with data set size. So while you can run k-means on big data, it does not make a whole lot of sense - you could just take a sample of your data, run a highly-efficient single-node version of k-means, and the results will be just as good. Because the extra data just gives you some extra digits of precision of a value that you do not need to be that precise.
Since this applies to quite a lot of problems, actual data mining on Hadoop doesn't seem to kick off. Everybody tries to do it, and a lot of companies sell this stuff. But it doesn't really work much better than the non-big version. But as long as customers want to buy this, companies will sell this functionality. And as long as it gets you a grant, researchers will write papers on this. Whether it works or not. That's life.
There are a few cases where these things work. Google search is an example, and Cern. But also image recognition (but not using Hadoop, clusters of GPUs seem to be the way to go there) has recently benefited from an increase in data size. But in any of these cases, you have rather clean data. Google indexes everything; Cern discards any non-interesting data, and only analyzes interesting measurements - there are no spammers feeding their spam into Cern... and in image analysis, you train on preselected relevant images, not on say webcams or random images from the internet (and if so, you treat them as random images, not as representative data).

What is the difference between big data and Hadoop?
A: The difference between big data and the open source software program Hadoop is a distinct and fundamental one. The former is an asset, often a complex and ambiguous one, while the latter is a program that accomplishes a set of goals and objectives for dealing with that asset.
Big data is simply the large sets of data that businesses and other parties put together to serve specific goals and operations. Big data can include many different kinds of data in many different kinds of formats. For example, businesses might put a lot of work into collecting thousands of pieces of data on purchases in currency formats, on customer identifiers like name or Social Security number, or on product information in the form of model numbers, sales numbers or inventory numbers. All of this, or any other large mass of information, can be called big data. As a rule, it’s raw and unsorted until it is put through various kinds of tools and handlers.
Hadoop is one of the tools designed to handle big data. Hadoop and other software products work to interpret or parse the results of big data searches through specific proprietary algorithms and methods. Hadoop is an open-source program under the Apache license that is maintained by a global community of users. It includes various main components, including a MapReduce set of functions and a Hadoop distributed file system (HDFS).
The idea behind MapReduce is that Hadoop can first map a large data set, and then perform a reduction on that content for specific results. A reduce function can be thought of as a kind of filter for raw data. The HDFS system then acts to distribute data across a network or migrate it as necessary.
Database administrators, developers and others can use the various features of Hadoop to deal with big data in any number of ways. For example, Hadoop can be used to pursue data strategies like clustering and targeting with non-uniform data, or data that doesn't fit neatly into a traditional table or respond well to simple queries.
See the article posted at http://www.shareideaonline.com/cs/what-is-the-difference-between-big-data-and-hadoop/
Thanks
Ankush

This answer is really intended to add some specificity to the excellent answer from Anony-Mousse.
There's a lot of debate over exactly what Big Data is.  Anony-Mousse called out a lot of the issues here around the overuse of terms like analytics, big data, and data mining, but there are a few things I want to provide more detail on.
Big Data
For practical purposes, the best definition I've heard of big data is data that is inconvenient or does not function in a traditional relational database. This could be data of 1PB that cannot be worked with or even just data that is 1GB but has 5,000 columns.
This is a loose and flexible definition. There are always going to be setups or data management tools which can work around it, but, this is where tools like Hadoop, MongoDB, and others can be used more efficiently that prior technology.
What can we do with data that is this inconvenient/large/difficult to work with? It's difficult to simply look at a spreadsheet and to find meaning here, so we often use data mining and machine learning.
Data Mining
This was called out lightly above - my goal here is to be more specific and hopefully to provide more context. Data mining generally applies to somewhat supervised analytic or statistical methods for analysis of data. These may fit into regression, classification, clustering, or collaborative filtering. There's a lot of overlap with machine learning, however, this is still generally driven by a user rather that unsupervised or automated execution, which defines machine learning fairly well.
Machine Learning
Often, machine learning and data mining are used interchangeably. Machine learning encompasses a lot of the same areas as data mining but also includes AI, computer vision, and other unsupervised tasks. The primary difference, and this is definitely a simplification, is that user input is not only unnecessary but generally unwanted. The goal is for these algorithms or systems to self-optimize and to improve, rather than an iterative cycle of development.

Big Data is a TERM which consists of collection of frameworks and tools which could do miracles with the very large data sets including Data Mining. 
Hadoop is a framework which will split the very large data sets into blocks(by default 64 mb) then it will store it in HDFS (Hadoop Distributed File System) and then when its execution logic(MapReduce) comes with any bytecode to process the data stored at HDFS. It will  take the split based on block(splits can be configured) and impose the extraction and computation via Mapper and Reducer process. By this way you could do ETL process, Data Mining, Data Computation, etc., 
I would like to conclude that Big Data is a terminology which could play with very large data sets. Hadoop is a framework which can do parallel processing very well with its components and services. By that way you can acquire Data mining too..

Big Data is the term people use to say how storage is cheap and easy these days and how data is available to be analyzed. 
Data Mining is the process of trying to extract useful information from data.
Usually, Data Mining is related to Big Data for 2 reasons

when you have lots of data, patterns are not so evident, so someone could not just inspect and say "hah". He/she needs tools for that.
for many times lots of data can improve the statistical meaningful to your analysis because your sample is bigger.

Can we say hadoop is dois data mining in parallel? What is hadoop? Their site says
The Apache Hadoop software library is a framework that allows for the 
distributed processing of large data sets across clusters of computers 
using simple programming models

So the "parallel" part of your statement is true. The "data mining" part of it is not necessarily. You can just use hadoop to summarize tons of data and this is not necessarily data mining, for example. But for most cases, you can bet people are trying to extract useful info from big data using hadoop, so this is kind of a yes.
 In most cases, they seem to use Hadoop for ETL, then analyze the no-longer-big data with traditional software, including Excel (so not really "data mining" either).
I would say that BigData is a modernized framework for addressing the new business needs.
As many people might know BigData is all about 3 v's Volume,Variety and Velocity. BigData is a need to leverage a variety of data (structured and un structured data) and using clustering technique to address volume issue and also getting results in less time ie.velocity.
Where as Datamining is on ETL principle .i.e finding useful information from large datasets using modelling techinques. There are many BI tools available in market to achieve this.
Javascript and Scientific Processing? [closed] 






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                
Closed 4 years ago.




Matlab, R, and Python are powerful but either costly or slow for some data mining work I'd like to do.  I'm considering using Javascript both for 
speed, good visualization libraries, and to be able to use the browser as an interface. 
The first question I faced is the obvious one for science programming, how to do I/O to data files?  The second is client-side or server-side?  The last question, can I make something that is truly portable i.e. put it all on a USB and run from that?
I've spent a couple of weeks looking for answers.  Server2go seems to address client/server needs which I think means I can get data to and from the programs on the client side.  Server2go also allows running from a USB.  The data files I work with are usually XML and there seem to be several javascript converters to JSON.
However, after all the looking around, I'm not sure if my approach makes sense.  So before I commit further, any advice/thoughts/guidance on Javascript as a portable tool for scientific data processing?
 I warmly suggest not to use Javascript for scientific processing. It lacks in math libraries, memory management, strong typing. I agree, there's pretty bad support for doing maths to a scientific standard. @larsmans I viewed the benchmarks only from the POV of how well javascript did against C++.  While Python is not as slow as suggested, it is still much slower. Are you sure that JavaScript has comparable speed? Especially when it comes to math and matrix operations, it is fairly hard to beat anything that can use Fortran libraries like R and numpy do. @MikeB: I've got in contact with the Julia developers about the Python benchmark. Whether the comparison of JavaScript vs. C++ is fair depends on the algorithms you want to run; if they spend much time in matrix multiplication, you're screwed.
I have to agree with the comments that JavaScript is not a good fit for scientific processing. However, you know your requirements best; maybe you already found useful libraries that do what you need. Just be aware that you'll have to implement all logic yourself. There is no built in handling of complex numbers, or matrices or integrals or ... Usually programmer time is far more valuable than machine time. Personally, I'd look in to compiled languages; after I created a first version that isn't fast enough in whatever language I like the most.
Assuming that JavaScript is the way to go:
Data I/O
I can think of three options: 
Sending and receiving data with ajax to a server
Seems to be the solution you've found with Server2go. It requires you to write a server back end, but that can be kept quite simple. All it really needs to do be able to read and write files as a response to you client-side application.
Using a non-browser implementation of v8 which includes file I/O
For instance Node.js. You could then avoid the need for a server and simply use a command-line interface, and all code will be JavaScript. Other than that it is roughly equivalent to the first option.
Creating a file object using the file API which you ask the user to save or load
It is the worst option in my opinion, as user interaction is required. It would avoid the need for a server; your application could be a simple html file that loads all data files with ajax requests. You'd have to start Chrome with a special switch to allow ajax requests with the file:// protocol, as described here
These options are only concerned with file I/O and you can't do file I/O in JavaScript. This is because browsers cannot allow arbitrary web code to do arbitrary file I/O; the security implications would be horrendous. Each option describes one way to not do file I/O. 
The first communicates with a server that does the file I/O for the client. 
The second uses "special" versions of JavaScript, with conditions other than that of the browser so the security implications are not important. But that means you'll have to look up how file I/O is done in the actual implementation you use, it's not common to JavaScript.
The third requires the user to control the file I/O. 
Interface
Even if you don't use JavaScript to do the actual processing, which so far is the consensus, there is nothing stopping you from using a browser as the interface or JavaScript libraries for visualisation. That is something JavaScript is good at.
If you want to interactively control your data mining tool, you will need a server that can control the tool. Server2go should work, or the built in server in Node.js if you use that or... If you don't need interactive control of the data tool; that is you first generate the processed data, then look at the data a server can be avoided, by using the file//: protocol and JSONP. But really; avoiding a server shouldn't be a goal.
I won't go into detail about interface issues, as there is nothing specific to say and very nearly everything that has been written about javascript is about interface.
One thing, do use a declarative data binding library like Angular.js or Knockout.js.
 You should add a note that also Node.js requires a clientside front-end. Surely, processing the data serverside is an advantage. @Bergi I haven't actually used Node.js . I assumed it would provide some kind of command-line interface, which would mean that there is no need for a clientside interface. Unless you actually want to see your output... Yes, as far as I know (never used it myself) it does, but the OP talked about visualisation libraries which I'm quite sure are for clientside usage... @Bergi: Node does provide built-in REPL module, and you can use optimist for getopt-style command-line arguments.  As far as visualization, you don't have to use a browser.  node-canvas lets you use canvas drawing methods in Node and output directly to a png.  There are lots of visualization/charting libraries that you can plug in to canvas. @Odalrick Thank you for the response.  I think the file I/O is the item that gives me the most confusion.  I have gotten server2go working but that isn't the same as understanding what is actually happening.  Node.js remains a bit of a mystery to me and I don't think it can be run from a USB key.
JavaScript speed is heavily overrated. This is a Web 2.0 myth.
Let me explain this claim a bit (and don't just downvote me for saying something you do not want to hear!)
Sure, JavaScript V8 is a quite highly optimized VM. It does beat many other scripting languages in naive benchmarks.
However, it is a very limited scope language. It is meant for the "ADHS world" of web. It is a best effort, but it may just fail and you have little guarantees on things completing or completing on time.
Consider for example MongoDB. At first it seems to be good and fast and offer a lot. Until you see for example that the MapReduce is single-threaded only and thus really slow. It's not all gold that shines!
Now look at data mining relevant libraries such as BLAS. Basic linear algebra, math operations and such. All CPU manufacturers like Intel and AMD offer optimized versions for their CPUs. This is an optimization that requires detailed understanding of the individual CPUs, way beyond the capabilities of our current compilers. The libraries contain optimized codepaths for various CPUs all essentially doing the same thing.
And for these operations, using an optimized library such as BLAS can easily yield a 5-20x speedup; at the same time matrix operations that are often in O(n^2) or O(n^3) will dominate your overall runtime.
So a good language for data mining will let you go all the way to machine code!
Pythons SciPy and R are good choices here. They have the optimized libraries inside and easily accessible, but at the same time allow to do the wrapper stuff in a simpler language.
Have a look at this programming language benchmark:
http://benchmarksgame.alioth.debian.org/u32/which-programs-are-fastest.html
Pure JavaScript has a high variance, indicating that it can do some things fast (mostly regular expressions!) others much slower. It can clearly beat PHP, but it will be just as clearly be beaten by C and Java.
Multithreading is also important for modern data mining. Few large systems today have a single core, and you do want to make use of all cores. So you need libraries and a programming language that has a powerful set of multithreading operations. This is actually why Fortran and C are losing popularity here. Other languages such as Java are much better here.
 Excellent post.  I note Javascript has variance which is comparable or less than PhP, Python, and Ruby.  Platform specific libraries are a liability for a portable app, which is what I'm looking at.  When I chart Fortran, C++, Java 7 server, Lua, Ruby, PhP, and Python, Javascript kind of bridges the performance of the slow and stinkin' fast.  An appealing niche for something so portable.  You're right though, any big data number crunching is best done on a tailored hardware/software combination.  But for portability, javascript stills seems to rule the roost. Don't take the benchmark for literal though. If you look closely, there are situations where the Java code essentially consists of calling a C library via JNI (no wonder it doesn't beat C). And in fact, many of the C programms will in turn call Fortran subroutines.
Although this discussion is a bit old and I am not a Javascript guru by any stretch of the imagination, I find the above arguments doubtful about not having the processing speed or the capabilities for advance math operations. WebGL is a Javascipt API for rendering advance 2D and 3D graphics which relies heavily on advance math operations. I believe the capabilities are there from a technical point of view however what is lacking is good libraries to handling statistical analysis, natural language processing and other predictive analytics included in data mining. 
WebGL is based on openGL, which in turn uses libraries like BLAS (library info here).
Advances like node.js, w8 make it technically possible. What is lacking is libraries like we can find in R and Scilab to do the same operations.
 You may find math.js an interesting initiative in this regard.Text mining with PHP [closed] 






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 4 years ago.




I'm doing a project for a college class I'm taking.
I'm using PHP to build a simple web app that classify tweets as "positive" (or happy) and "negative" (or sad) based on a set of dictionaries. The algorithm I'm thinking of right now is Naive Bayes classifier or decision tree.
However, I can't find any PHP library that helps me do some serious language processing. Python has NLTK (http://www.nltk.org). Is there anything like that for PHP?
I'm planning to use WEKA as the back end of the web app (by calling Weka in command line from within PHP), but it doesn't seem that efficient.
Do you have any idea what I should use for this project? Or should I just switch to Python?
Thanks
 There is no indication whatsoever in either your post or the one your linked to as to why this is a fitting solution. Naive Bayesian classifiers are not really difficult to write yourself if you understand the basic principles. You could actually do everything in PHP that way.  San Jacinto already covered everything I'd have said about the NLP part.  One other thing I can tell you from a similar project I did just a couple of weeks ago is that sentiment classification using the standard bag-of-words approach doesn't really work very well. I didn't try anything like n-grams, though... I do have the feeling that they'd perform better, but of course that would give you tons of additional dimensions... Take a look at this link to an article on Bayesian opinion mining on php/ir phpir.com/bayesian-opinion-mining It's a site that's well worth bookmarking PEAR's Text_LanguageDetect can identify 52 human languages from text samples and return confidence scores for each. Isn't this an interesting option to take into account? @nuqqsa The question is about sentiment analysis, not language identification, and it asks for PHP, not Python.
If you're going to be using a Naive Bayes classifier, you don't really need a whole ton of NL processing. All you'll need is an algorithm to stem the words in the tweets and if you want, remove stop words.
Stemming algorithms abound and aren't difficult to code. Removing stop words is just a matter of searching a hash map or something similar. I don't see a justification to switch your development platform to accomodate the NLTK, although it is a very nice tool.

I did a very similar project a while ago - only classifying RSS news items instead of twitter - also using PHP for the front-end and WEKA for the back-end. I used PHP/Java Bridge which was relatively simple to use - a couple of lines added to your Java (WEKA) code and it allows your PHP to call its methods. Here's an example of the PHP-side code from their website:
<?php 
require_once("http://localhost:8087/JavaBridge/java/Java.inc");

$world = new java("HelloWorld");
echo $world->hello(array("from PHP"));
?>

Then (as someone has already mentioned), you just need to filter out the stop words. Keeping a txt file for this is pretty handy for adding new words (they tend to pile up when you start filtering out irrelevant words and account for typos). 
The naive-bayes model has strong independent-feature assumptions, i.e. it doesn't account for words that are commonly paired (such as an idiom or phrase) - just taking each word as an independent occurrence. However, it can outperform some of the more complex methods (such as word-stemming, IIRC) and should be perfect for a college class without making it needlessly complex.  

You can also use the uClassify API to do something similar to Naive Bayes.  You basically train a classifier as you would with any algorithm (except here you're doing it via the web interface or by sending xml documents to the API).  Then whenever you get a new tweet (or batch of tweets), you call the API to have it classify them.  It's fast and you don't have to worry about tuning it.  Of course, that means you lose the flexibility you get by controlling the classifier yourself, but that also means less work for you if that in itself is not the goal of the class project.

Try open calais - http://viewer.opencalais.com/ . It has api, PHP classes and many more. Also, LingPipe for this task - http://alias-i.com/lingpipe/index.html
 The former is a web interface, not a library (if there is a library, too, please provide a link to that). The latter is a library, but for Java, not PHP.
you can check this library https://github.com/Dachande663/PHP-Classifier very straight forward

you can also use thrift or gearman to deal with nltk
How to find out if a sentence is a question (interrogative)? 
Is there an open source Java library/algorithm for finding if a particular piece of text is a question or not?

I am working on a question answering system that needs to analyze if the text input by user is a question.
 
I think the problem can probably be solved by using opensource NLP libraries but its obviously more complicated than simple part of speech tagging. So if someone can instead tell the algorithm for it by using an existing opensource NLP library, that would be good too.

Also let me know if you know a library/toolkit that uses data mining to solve this problem. Although it will be difficult to get sufficient data for training purposes, I will be able to use stack exchange data for training.
 @Noel What about greek? They use ;. So is `"To be or not to be." a question or not? Because THAT is The Question. Can't expect user to always end their question with ?. "To be or not to be" is also a question. :) @belisarius that would take too long ;) @belisarius that's allways true, a little senseless ;-)
In a syntactic parse of a question, the correct structure will be in the form of:
(SBARQ (WH+ (W+) ...)
       (SQ ...*
           (V+) ...*)
       (?))

So, using anyone of the syntactic parsers available, a tree with an SBARQ node having an embedded SQ (optionally) will be an indicator the input is a question. The WH+ node (WHNP/WHADVP/WHADJP) contains the question stem (who/what/when/where/why/how) and the SQ holds the inverted phrase.
i.e.: 
(SBARQ 
  (WHNP 
    (WP What)) 
  (SQ 
    (VBZ is) 
    (NP 
      (DT the) 
      (NN question)))
  (. ?))

Of course, having a lot of preceeding clauses will cause errors in the parse (that can be worked around), as will really poorly-written questions. For example, the title of this post "How to find out if a sentence is a question?" will have an SBARQ, but not an SQ.
 s/any one of the syntactic parsers available/any parser using the Penn Treebank format/ For the statement  Doctor Who is a British science fiction television programme produced by the BBC  The Penn Treebank format is of the form  (TOP (NP (NP (NNP Doctor)) (SBAR (WHNP (WP Who)) (S (VP (VBZ is) (NP (NP (DT a) (JJ British) (NN science) (NN fiction) (NN television) (NN programme)) (VP (VBN produced) (PP (IN by) (NP (DT the) (NNP BBC))))))))))  It fails ... FYI, a parser for F#/C# is available as a nuget package: sergeytihon.wordpress.com/2013/07/11/… can we parse the same structure in python nltk penn treebank
Many quasi-questions/requests-for-info are posed in the grammatical form of a statement; e.g. "I would like to know who stole my bike". 
I would abandon all hope of determining from its structure whether the user's input is a question, and simply assume it is a question unless it is unambiguously not a question.  You could take an iterative, interactive approach so the system could refine its "understanding" of the user's input: 
     User: I would like to know how many angels fit on the head of a pin. 
     System: Do you have a question about angels?
     User: Yes.          
     System: Do you want to know if angels are fit?
     User: No.
     System: Do you want to know if angels have heads?
     User: Possibly.         
     System: Do you want to know if angels have pins?
     User: No.         
     System: Do you want to know if angels are numerous?
     User: No.
     System: Do you want to know the dimensions of an angel?
     User: Yes.
     System: Do you mean angels in paintings?
     User: No.
     System: Do you mean angels in myth and religious writing?
     User: Yes.
     System: Angels are metaphysical beings.


     User: I hear that Pennsylvania was founded by William Penn. Is that true?
     System: Do you have a question about hearing?
     User: No.
     System: Do you have a question about Pennsylvania?
     User: Yes.         
     System: Pennsylvania was founded by William Penn.
     User: When?         
     System: 1682.
     User: What does the name mean?
     System: What name?
     User: Pennsylvania!
     System: Do you want to know the meaning of Pennsylvania?
     User: Yes.
     System: Pennsylvania means Penn's Woods.

 Interesting approach. :) That is a nice method of doing it. Can i assume that this is purely theoretical. @Lee: Do you have a question about "doing it" ?
Take a look at Link Grammar Parser  It is a multi-language parser based on the concept of link pairs of related words in the sentence. It is written in C, but has a Java JNI interface as well.
random unit vector in multi-dimensional space 
I'm working on a data mining algorithm where i want to pick a random direction from a particular point in the feature space.  
If I pick a random number for each of the n dimensions from [-1,1] and then normalize the vector to a length of 1 will I get an even distribution across all possible directions?  
I'm speaking only theoretically here since computer generated random numbers are not actually random.

One simple trick is to select each dimension from a gaussian distribution, then normalize:
from random import gauss

def make_rand_vector(dims):
    vec = [gauss(0, 1) for i in range(dims)]
    mag = sum(x**2 for x in vec) ** .5
    return [x/mag for x in vec]

For example, if you want a 7-dimensional random vector, select 7 random values (from a Gaussian distribution with mean 0 and standard deviation 1). Then, compute the magnitude of the resulting vector using the Pythagorean formula (square each value, add the squares, and take the square root of the result). Finally, divide each value by the magnitude to obtain a normalized random vector.
If your number of dimensions is large then this has the strong benefit of always working immediately, while generating random vectors until you find one which happens to have magnitude less than one will cause your computer to simply hang at more than a dozen dimensions or so, because the probability of any of them qualifying becomes vanishingly small.
 Nice! Thank you for the additional suggestion. By the way, this is how boost boost.org/doc/libs/1_47_0/boost/random/uniform_on_sphere.hpp implements it. ;) Here's a reference on why this method is right mathworld.wolfram.com/HyperspherePointPicking.html A quick explanation of why this works: The probability P of a point being at a given <x,y> is P(x)*P(y). The gaussian distribution has roughly the form e^(-x^2), so e^(-x^2)*e^(-y^2) is e^(-(x^2+y^2)). That is a function only of the distance of the point from the origin, so the resulting distribution is radially symmetric. This generalizes easily to higher dimensions. Additional note: the Box–Muller transform may be used to generate independent pairs of normally distributed variables from independent pairs of uniformly distributed ones (with no 'waste').
You will not get a uniformly distributed ensemble of angles with the algorithm you described.  The angles will be biased toward the corners of your n-dimensional hypercube.
This can be fixed by eliminating any points with distance greater than 1 from the origin. Then you're dealing with a spherical rather than a cubical (n-dimensional) volume, and your set of angles should then be uniformly distributed over the sample space.
Pseudocode:
Let n be the number of dimensions, K the desired number of vectors:
vec_count=0
while vec_count < K
   generate n uniformly distributed values a[0..n-1] over [-1, 1]
   r_squared = sum over i=0,n-1 of a[i]^2
   if 0 < r_squared <= 1.0
      b[i] = a[i]/sqrt(r_squared)  ; normalize to length of 1
      add vector b[0..n-1] to output list
      vec_count = vec_count + 1
   else
      reject this sample
end while

 That's what I was worried about.  I just wasn't able to formalize it in my head the way you described.  Intuitively I know that I want my possible random vectors to describe a circle.  I'm just not seeing how to implement it in code. @Matt: I expanded my answer a bit, hope that helps. Why would you use an algo with non-deterministic run time AND a branch if you could solve this with a closed-form expression?
I had the exact same question when also developing a ML algorithm.
I got to the same conclusion as Jim Lewis after drawing samples for the 2-d case and plotting the resulting distribution of the angle.
Furthermore, if you try to derive the density distribution for the direction in 2d when you draw at random from [-1,1] for the x- and y-axis ,you will see that:
f_X(x) = 1/(4*cos²(x)) if 0 < x < 45⁰
and
f_X(x) = 1/(4*sin²(x)) if x > 45⁰
where x is the angle, and f_X is the probability density distribution.
I have written about this here:
https://aerodatablog.wordpress.com/2018/01/14/random-hyperplanes/

There is a boost implementation of the algorithm that samples from normal distributions:  random::uniform_on_sphere

#define SCL1 (M_SQRT2/2)
#define SCL2 (M_SQRT2*2)

// unitrand in [-1,1].
double u = SCL1 * unitrand();
double v = SCL1 * unitrand();
double w = SCL2 * sqrt(1.0 - u*u - v*v);

double x = w * u;
double y = w * v;
double z = 1.0 - 2.0 * (u*u + v*v);

 Code no more is hard to read for non-mechanical guys like me. Any comments on what it does, why it's better than the accepted answer, or something ?Information retrieval (IR) vs data mining vs Machine Learning (ML) 
People often throw around the terms IR, ML, and data mining, but I have noticed a lot of overlap between them.
From people with experience in these fields, what exactly draws the line between these?

This is just the view of one person (formally trained in ML); others might see things quite differently. 
Machine Learning is probably the most homogeneous of these three terms, and the most consistently applied--it's limited to the pattern-extraction (or pattern-matching) algorithms themselves. 
Of the terms you mentioned, "Machine Learning" is the one most used by Academic Departments to describe their Curricula, their academic departments, and their research programs, as well as the term most used in academic journals and conferences proceedings. ML is clearly the least context-dependent of the terms you mentioned. 
Information Retrieval and Data Mining are much closer to describing complete commercial processes--i.e., from user query to retrieval/delivery of relevant results. ML algorithms might be somewhere in that process flow, and in the more sophisticated applications, often are, but that's not a formal requirement. In addition, the term Data Mining seems usually to refer to application of some process flow on big data (i.e, > 2BG) and therefore usually includes a distributed processing (map-reduce) component near the front of that workflow.
So Information Retrieval (IR) and Data Mining (DM) are related to Machine Learning (ML) in an Infrastructure-Algorithm kind of way. In other words, Machine Learning is one source of tools used to solve problems in Information Retrieval. But it's only one source of tools. But IR doesn't depend on ML--for instance, a particular IR project might be storage and rapid retrieval of the fully-indexed data responsive to a user's search query IR, the crux of which is optimizing performance of the data flow, i.e., the round-trip from query to delivering the search results to the user. Prediction or pattern matching might not be useful here. Likewise, a DM project might use an ML algorithm for the predictive engine, yet a DM project is more likely to also be concerned with the entire processing flow--for instance, parallel computation techniques for efficient input of an enormous data volume (TB perhaps) which delivers a proto-result to a processing engine for computation of descriptive statistics (mean, standard deviation, distribution, etc. on the variables (columns).
Lastly consider the Netflix Prize. This competition was directed solely to Machine Learning--the focus was on the prediction algorithm, as evidenced by the fact that there was a single success criterion: accuracy of the predictions returned by the algorithm. Imagine if the 'Netflix Prize' were rebranded as a Data Mining competition. The success criteria would almost certainly be expanded to more accurately access the algorithm's performance in the actual commercial setting--so for instance overall execution speed (how quickly are the recommendations delivered to the user) would probably be considered along with accuracy.
The terms "Information Retrieval" and "Data Mining" are now in mainstream use, though for a while I only saw these terms in my job description or in vendor literature (usually next to the word "solution.") At my employer, we recently hired a "Data Mining" analyst. I don't know what he does exactly, but he wears a tie to work every day.
 (+1) I also like the distinction being made by Radford Neale: "Many machine learning problems have a large number of variables — maybe 10,000, or 100,000, or more (eg, genes, pixels). Data mining applications often involve very large numbers of cases — sometimes millions." (sta414, week1). Data mining also suffers from being a total buzzword. Todays, computing the mean value of a "big data" data set already is considered "data mining" by some, unfortunately. He wears a tie to work huh. That gives me a very good idea of what he might be doing :-)
I'd try to draw the line as follows:
Information retrieval is about finding something that already is part of your data, as fast as possible.
Machine learning are techniques to generalize existing knowledge to new data, as accurate as possible.
Data mining is primarly about discovering something hidden in your data, that you did not know before, as "new" as possible.
They intersect and often use techniques of one another. DM and IR both use index structures to accelerate processes. DM uses a lot of ML techniques, for example a pattern in the data set that is useful for generalization might be a new knowledge.
They are often hard to separate. Do yourself a favor and don't just go for the buzzwords. In my opinion the best way of distinguishing them is by their intention, as given above: find data, generalize to new data, find new properties of existing data.
 I do not agree with your view on machine learning. Your view is more focused on supervised learning (as your statement would be correct). Unsupervised learning however is about finding patterns that one does not know about, hence with no prior existing knowledge. Unsupervised learning is an oxymoron. Unsupervised methods are DM, not ML. They don't learn, how could they, so don't squeeze them into the learning view at all. I believe you are referring to storage, that unsupervised methods do not remember after they have executed. I do agree, the terminology is flawed in AI, but as it currently stands, unsupervised is under machine learning, so I do not agree with your post still. Also DM does not necessarily use unsupervised learning methods (although it mostly does) so saying unsupervised learning is equal to DM is indeed very wrong. Define "learning" if we want to get anywhere here. To me, "learning" is the generalization from training data. I don't see this happen e.g. in clustering - there is no training data. Personally I use the English meaning of the word, "The acquisition of knowledge or skills through study, experience, or being taught.". Supervised referring to being taught via learning data and unsupervised via study/experience therefore it learns. So I guess our different views arise from the interpretation of the word learning.
You can also add pattern recognition and (computational?) statistics as another couple of areas that overlap with the three you mentioned.
I'd say there is no well-defined line between them. What separates them is their history and their emphases. Statistics emphasizes mathematical rigor, data mining emphasizes scaling to large datasets, ML is somewhere in between.

Data mining is about discovering hidden patterns or unknown knowledge, which can be used 
for decision making by people.
Machine learning is about learning a model to classify new objects.  
Is it ok to define your own cost function for logistic regression? 
In least-squares models, the cost function is defined as the square of the difference between the predicted value and the actual value as a function of the input.
When we do logistic regression, we change the cost function to be a logarithmic function instead of defining it to be the square of the difference between the sigmoid function (the output value) and the actual output.
Is it OK to change and define our own cost function to determine the parameters?
 It is not that simple, there are a number of link functions possible for logistic regression, not just the canonical logit function. I would suggest reading some theory behind generalized linear models en.wikipedia.org/wiki/Generalised_linear_model If you go ahead and build a quadratic loss function in the very special case where the inputs $x$ are scalars, then your cost function becomes: $C(w,b):=   \Sigma_{x} | y(x) - \hat{y}(x)|^2=\Sigma_{x} | y(x) - \sigma(wx+b)|^2$. Now if you try to apply gradient descent on it, you'll see that: $C'(x)$ is a multiple of $\sigma'(wx+b)$. Now, sigmoid function being asymptotic, its derivative $\sigma'(z)$ becomes almost zero when the output $\sigma(z)$ is close to $0$ or $1$. This means: when the learning is bad, e.g. $\sigma(wx+b) \approx 0$, but $y(x)=1$, then $C'(w), C'(b)\approx 0$. (contd.)
Yes, you can define your own loss function, but if you're a novice, you're probably better off using one from the literature. There are conditions that loss functions should meet:

They should approximate the actual loss you're trying to minimize. As was said in the other answer, the standard loss functions for classification is zero-one-loss (misclassification rate) and the ones used for training classifiers are approximations of that loss.
The squared-error loss from linear regression isn't used because it doesn't approximate zero-one-loss well: when your model predicts +50 for some sample while the intended answer was +1 (positive class), the prediction is on the correct side of the decision boundary so the zero-one-loss is zero, but the squared-error loss is still 49² = 2401. Some training algorithms will waste a lot of time getting predictions very close to {-1, +1} instead of focusing on getting just the sign/class label right.(*)
The loss function should work with your intended optimization algorithm. That's why zero-one-loss is not used directly: it doesn't work with gradient-based optimization methods since it doesn't have a well-defined gradient (or even a subgradient, like the hinge loss for SVMs has).
The main algorithm that optimizes the zero-one-loss directly is the old perceptron algorithm.

Also, when you plug in a custom loss function, you're no longer building a logistic regression model but some other kind of linear classifier.
(*) Squared error is used with linear discriminant analysis, but that's usually solved in close form instead of iteratively.
 Regarding point 1., in logistic regression the model can never predict +50 because the output is scaled by the logistic function which bounds the input to (0,1).
The logistic function, hinge-loss, smoothed hinge-loss, etc. are used because they are upper bounds on the zero-one binary classification loss.
These functions generally also penalize examples that are correctly classified but are still near the decision boundary, thus creating a "margin."
So, if you are doing binary classification, then you should certainly choose a standard loss function.
If you are trying to solve a different problem, then a different loss function will likely perform better.

You don't choose the loss function, you choose the model
The loss function is usually directly determined by the model when you fit your parameters using Maximum Likelihood Estimation (MLE), which is the most popular approach in Machine Learning.
You mentioned the Mean Squared Error as a loss function for linear regression. Then "we change the cost function to be a logarithmic function", referring to the Cross Entropy Loss. We didn't change the cost function. In fact, the Mean Squared Error is the Cross Entropy Loss for linear regression, when we assume yto be normally distributed by a Gaussian, whose mean is defined by Wx + b.
Explanation
With MLE, you choose the parameters in way, that the likelihood of the training data is maximized. The likelihood of the whole training dataset is a product of the likelihoods of each training sample. Because that may underflow to zero, we usually maximize the log-likelihood of the training data / minimize the negative log-likelihood. Thus, the cost function becomes a sum of the negative log-likelihood of each training sample, which is given by:
-log(p(y | x; w))
where w are the parameters of our model (including the bias). Now, for logistic regression, that is the logarithm that you referred to. But what about the claim, that this also corresponds to the MSE for linear regression?
Example
To show the MSE corresponds to the cross-entropy, we assume that y is normally distributed around a mean, which we predict using w^T x + b. We also assume that it has a fixed variance, so we don't predict the variance with our linear regression, only the mean of the Gaussian. 
p(y | x; w) = N(y; w^T x + b, 1)
You can see, mean = w^T x + b and variance = 1
Now, the loss function corresponds to
-log N(y; w^T x + b, 1)
If we take a look at how the Gaussian N is defined, we see:

Now, take the negative logarithm of that. This results in:

We chose a fixed variance of 1. This makes the first term constant and reduces the second term to:
0.5 (y - mean)^2
Now, remember that we defined the mean as w^T x + b. Since the first term is constant, minimizing the negative logarithm of the Gaussian corresponds to minimizing
(y - w^T x + b)^2
which corresponds to minimizing the Mean Squared Error.

Yes, other cost functions can be used to determine the parameters. 
The squared error function (commonly used function for linear regression) is not very suitable for logistic regression. 
As in case of logistic regression the hypothesis is non-linear (sigmoid function), which makes the square error function to be non-convex. 
The logarithmic function is a convex function for which there is no local optima, so gradient descent works well.  

Assume that in your logistic regression model, you've scalar inputs x, and the the model outputs a probability $\hat{y}(x)=sigma(wx+b)$ for each input sample x  .If you go ahead and build a quadratic loss function in the very special case where the inputs $x$ are scalars, then your cost function becomes: $C(w,b):= \Sigma_{x} | y(x) - \hat{y}(x)|^2=\Sigma_{x} | y(x) - \sigma(wx+b)|^2$. Now if you try to apply gradient descent on it, you'll see that: $C'(w), C'(b)$ are  multiples of $\sigma'(wx+b)$. Now, sigmoid function being asymptotic, its derivative $\sigma'(z)$ becomes almost zero when the output $\sigma(z)$ is close to $0$ or $1$. This means: when the learning is bad, e.g. $\sigma(wx+b) \approx 0$, but $y(x)=1$, then $C'(w), C'(b)\approx 0$.
Now, the above situation is bad from two standpoints: (1) it makes gradient descent numerically much more expensive because even when we're far from minimizing C(w,b), we're not converging fast enough, and (2) it's counterintuitive to human learning: we learn fast when we make a big mistake. 
However, if you calculate the C'(w) and C'(b) for cross-entropy cost function, this problem doesn't occur, as unlike the derivatives of quadratic cost, the derivatives of cross entropy cost is not a multiple of $sigma'(wx+b)$, and hence when the logistic regression model outputs close to 0 or 1, the gradient  descent doesn't necessarily slow down, hence convergence to minima happens faster. You can find the relevant discussion here: http://neuralnetworksanddeeplearning.com/chap3.html, an excellent online book I highly recommend!
Besides, cross entropy cost functions are just negative log of maximum likelihood functions (MLE) used to estimate the model parameters, and in fact in the case of linear regression, minimizing the quadratic cost function is equivalent to maximizing the MLE, or equivalently, minimizing the negative log of MLE=cross entropy, with the underlying model assumption for linear regression-see P. 12 of http://cs229.stanford.edu/notes/cs229-notes1.pdf for more detail. Hence, for any machine learning model, be it classification and regression, finding the parameters by maximizing MLE (or minimizing cross entropy) has a statistical significance, whereas minimizing the quadratic cost for logistic regression doesn't have any (although it does for linear regression, as stated before).
I hope it clarifies things!

I'd like to say that the mathematics underlies mean square error is the Gaussian distribution of the error, while for logistic regression it is the distance of the two distributions: the underlying(ground truth) distribution and the predicted distribution. 
How can we measure the distance between two distributions? In information theory, it is the relative entropy(also known as KL divergence), and the relative entropy is equivalent to the cross-entropy. And the logistic regression function is a special case of the softmax regression which is equivalent to cross-entropy and maximum entropy. 
Hierarchical clustering of 1 million objects 
Can anyone point me to a hierarchical clustering tool (preferable in python) that can cluster ~1 Million objects? I have tried hcluster and also Orange.
hcluster had trouble with 18k objects. Orange was able to cluster 18k objects in seconds, but failed with 100k objects (saturated memory and eventually crashed). 
I am running on a 64bit Xeon CPU (2.53GHz) and 8GB of RAM + 3GB swap on Ubuntu 11.10.
 are your points in 2d, 3d, 10d, 128d ? @Denis I dont understand what you mean by that. AS such, the limitation seems to stem from the fact that a nxn distance matrix for 1M objects cant fit into memory, and each of the clustering libraries I stated above (orange and scipy) take an in memory distance matrix as input (which is not possible to provide as input for 1M objects...) the points/objects are simple text files, that I am trying to cluster based on the text they contain.... can you also explain me if this is 2d or what? thanks. @user940154 The idea of 2d or nd is in which feature dimension you operate? So, if you attach to each of the word 'length' and 'starting letter' features, that would be 2d feature space. Highdimensional and sparsed data requires special data structures to do efficient clustering.
To beat O(n^2), you'll have to first reduce your 1M points (documents)
to e.g. 1000 piles of 1000 points each, or 100 piles of 10k each, or ...
Two possible approaches:

build a hierarchical tree from say 15k points, then add the rest one by one:
time ~ 1M * treedepth
first build 100 or 1000 flat clusters,
then build your hierarchical tree of the 100 or 1000 cluster centres.

How well either of these might work depends critically
on the size and shape of your target tree --
how many levels, how many leaves ?
What software are you using,
and how many hours / days do you have to do the clustering ?
For the flat-cluster approach,
K-d_tree s
work fine for points in 2d, 3d, 20d, even 128d -- not your case.
I know hardly anything about clustering text;
Locality-sensitive_hashing ?
Take a look at scikit-learn clustering --
it has several methods, including DBSCAN.
Added: see also
google-all-pairs-similarity-search
"Algorithms for finding all similar pairs of vectors in sparse vector data", Beyardo et el. 2007
SO hierarchical-clusterization-heuristics
 I don't think there is a general way to beat O(n^2) for hierarchical clustering. You can do some stuff for the particular case of single-link (see my reply), and of course you can use other algorithms (e.g. DBSCAN). Which is much more sensible for this large data anyway than hierarchical clustering. Note that scikit-learns DBSCAN is O(n^2), as it does AFAIK not use indexes. On O(n^2): if you accept higher error rates, you can sample (my first trivial suggestion), or LSH. There are lots of papers on fast clustering some of them write-only. On hierarchical clustering, I agree, but it would be nice if the OP would say how big a tree he or she wants, and why.
The problem probably is that they will try to compute the full 2D distance matrix (about 8 GB naively with double precision) and then their algorithm will run in O(n^3) time anyway.
You should seriously consider using a different clustering algorithm. Hierarchical clustering is slow and the results are not at all convincing usually. In particular for millions of objects, where you can't just look at the dendrogram to choose the appropriate cut.
If you really want to continue hierarchical clustering, I belive that ELKI (Java though) has a O(n^2) implementation of SLINK. Which at 1 million objects should be approximately 1 million times as fast. I don't know if they already have CLINK, too. And I'm not sure if there actually is any sub-O(n^3) algorithm for other variants than single-link and complete-link.
Consider using other algorithms. k-means for example scales very well with the number of objects (it's just not very good usually either, unless your data is very clean and regular). DBSCAN and OPTICS are quite good in my opinion, once you have a feel for the parameters. If your data set is low dimensional, they can be accelerated quite well with an appropriate index structure. They should then run in O(n log n), if you have an index with O(log n) query time. Which can make a huge difference for large data sets. I've personally used OPTICS on a 110k images data set without problems, so I can imagine it scales up well to 1 million on your system.
scikit-learn: clustering text documents using DBSCAN 
I'm tryin to use scikit-learn to cluster text documents. On the whole, I find my way around, but I have my problems with specific issues. Most of the examples I found illustrate clustering using scikit-learn with k-means as clustering algorithm. Adopting these example with k-means to my setting works in principle. However, k-means is not suitable since I don't know the number of clusters. From what I read so far -- please correct me here if needed -- DBSCAN or MeanShift seem the be more appropriate in my case. The scikit-learn website provides examples for each cluster algorithm. The problem is now, that with both DBSCAN and MeanShift I get errors I cannot comprehend, let alone solve.
My minimal code is as follows:
docs = []
for item in [database]:
    docs.append(item)

vectorizer = TfidfVectorizer(min_df=1)
X = vectorizer.fit_transform(docs)

X = X.todense() # <-- This line was needed to resolve the isse

db = DBSCAN(eps=0.3, min_samples=10).fit(X)
...

(My documents are already processed, i.e., stopwords have been removed and an Porter Stemmer has been applied.)
When I run this code, I get the following error when instatiating DBSCAN and calling fit():
...
File "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/dbscan_.py", line 248, in fit
clust = dbscan(X, **self.get_params())
File "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/dbscan_.py", line 86, in dbscan
n = X.shape[0]
IndexError: tuple index out of range

Clicking on the line in dbscan_.py that throws the error, I noticed the following line
...
X = np.asarray(X)
n = X.shape[0]
...

When I use these to lines directly in my code for testing, I get the same error. I don't really know what np.asarray(X) is doing here, but after the command X.shape = (). Hence X.shape[0] bombs -- before, X.shape[0] correctly refers to the number of documents. Out of curiosity, I removed X = np.asarray(X) from dbscan_.py. When I do this, something is computing heavily. But after some seconds, I get another error:
...
File "/usr/lib/python2.7/dist-packages/scipy/sparse/csr.py", line 214, in extractor
(min_indx,max_indx) = check_bounds(indices,N)
File "/usr/lib/python2.7/dist-packages/scipy/sparse/csr.py", line 198, in check_bounds
max_indx = indices.max()
File "/usr/lib/python2.7/dist-packages/numpy/core/_methods.py", line 17, in _amax
out=out, keepdims=keepdims)
ValueError: zero-size array to reduction operation maximum which has no identity

In short, I have no clue how to get DBSCAN working, or what I might have missed, in general.

The implementation in sklearn seems to assume you are dealing with a finite vector space, and wants to find the dimensionality of your data set. Text data is commonly represented as sparse vectors, but now with the same dimensionality.
Your input data probably isn't a data matrix, but the sklearn implementations needs them to be one.
You'll need to find a different implementation. Maybe try the implementation in ELKI, which is very fast, and should not have this limitation.
You'll need to spend some time in understanding similarity first. For DBSCAN, you must choose epsilon in a way that makes sense for your data. There is no rule of thumb; this is domain specific. Therefore, you first need to figure out which similarity threshold means that two documents are similar.
Mean Shift may actually need your data to be vector space of fixed dimensionality.
 Quoting Homer: "Uh huh. Uh huh. Okay. Um, can you repeat the part of the stuff where you said all about the...things? Uh... the things?" :). I just started to play around, trying to follow and understand the examples. To get things working, not worrying about the results at the moment. I just can't see the difference between my setting and the examples. X.shape tells mit it's a (832, 20932) matrix which reflects my 832 documents and 20k+ different terms. But you're right, of course, I need to get a better understanding. I will have a look at ELKI. Thanks a lot! Short story: it's not a DBSCAN limitation, but it could be a scipy limitation. If np.asarray(X).shape returns a tuple, then it should not fail as above. I don't use numpy enough to be able to tell you how to properly convert a sparse matrix into a dense matrix. I found the problem: The expected format of matrix X differs between, e.g., k-means and DBSCAN. While both expect a (n_sample, n_features) matrix, k-means expects a spare matrix, DBSCAN a dense matrix. Thus, if I add X=X.todense() before calling fit(X), it works. That is essentially what I'm trying to say. Except that technically DBSCAN does not need a dense matrix. It's the sklearn version that does, for a reason unknown to me. Yeah, I had to get used the whole numpy matrix notions. The sklearn documentation is not intuitive without the required insights into numpy. Hence my problems. Thanks a lot for your help, I will mark your answer as correct.
It looks like sparse representations for DBSCAN are supported as of Jan. 2015. 
I upgraded sklearn to 0.16.1 and it worked for me on text.
Matlab - PCA analysis and reconstruction of multi dimensional data 
I have a large dataset of multidimensional data(132 dimensions).
I am a beginner at performing data mining and I want to apply Principal Components Analysis by using Matlab. However, I have seen that there are a lot of functions explained on the web but I do not understand how should they be applied.
Basically, I want to apply PCA and to obtain the eigenvectors and their corresponding eigenvalues out of my data.
After this step I want to be able to do a reconstruction for my data based on a selection of the obtained eigenvectors.
I can do this manually, but I was wondering if there are any predefined functions which can do this because they  should already be optimized.
My initial data is something like : size(x) = [33800 132]. So basically I have 132 features(dimensions) and 33800 data points. And I want to perform PCA on this data set.
Any help or hint would do.

Here's a quick walkthrough. First we create a matrix of your hidden variables (or "factors"). It has 100 observations and there are two independent factors.
>> factors = randn(100, 2);

Now create a loadings matrix. This is going to map the hidden variables onto your observed variables. Say your observed variables have four features. Then your loadings matrix needs to be 4 x 2
>> loadings = [
      1   0
      0   1
      1   1
      1  -1   ];

That tells you that the first observed variable loads on the first factor, the second loads on the second factor, the third variable loads on the sum of factors and the fourth variable loads on the difference of the factors.
Now create your observations:
>> observations = factors * loadings' + 0.1 * randn(100,4);

I added a small amount of random noise to simulate experimental error. Now we perform the PCA using the pca function from the stats toolbox:
>> [coeff, score, latent, tsquared, explained, mu] = pca(observations);

The variable score is the array of principal component scores. These will be orthogonal by construction, which you can check -
>> corr(score)
ans =
    1.0000    0.0000    0.0000    0.0000
    0.0000    1.0000    0.0000    0.0000
    0.0000    0.0000    1.0000    0.0000
    0.0000    0.0000    0.0000    1.0000

The combination score * coeff' will reproduce the centered version of your observations. The mean mu is subtracted prior to performing PCA. To reproduce your original observations you need to add it back in,
>> reconstructed = score * coeff' + repmat(mu, 100, 1);
>> sum((observations - reconstructed).^2)
ans =
   1.0e-27 *
    0.0311    0.0104    0.0440    0.3378

To get an approximation to your original data, you can start dropping columns from the computed principal components. To get an idea of which columns to drop, we examine the explained variable
>> explained
explained =
   58.0639
   41.6302
    0.1693
    0.1366

The entries tell you what percentage of the variance is explained by each of the principal components. We can clearly see that the first two components are more significant than the second two (they explain more than 99% of the variance between them). Using the first two components to reconstruct the observations gives the rank-2 approximation,
>> approximationRank2 = score(:,1:2) * coeff(:,1:2)' + repmat(mu, 100, 1);

We can now try plotting:
>> for k = 1:4
       subplot(2, 2, k);
       hold on;
       grid on
       plot(approximationRank2(:, k), observations(:, k), 'x');
       plot([-4 4], [-4 4]);
       xlim([-4 4]);
       ylim([-4 4]);
       title(sprintf('Variable %d', k));
   end


We get an almost perfect reproduction of the original observations. If we wanted a coarser approximation, we could just use the first principal component:
>> approximationRank1 = score(:,1) * coeff(:,1)' + repmat(mu, 100, 1);

and plot it,
>> for k = 1:4
       subplot(2, 2, k);
       hold on;
       grid on
       plot(approximationRank1(:, k), observations(:, k), 'x');
       plot([-4 4], [-4 4]);
       xlim([-4 4]);
       ylim([-4 4]);
       title(sprintf('Variable %d', k));
   end


This time the reconstruction isn't so good. That's because we deliberately constructed our data to have two factors, and we're only reconstructing it from one of them.
Note that despite the suggestive similarity between the way we constructed the original data and its reproduction,
>> observations  = factors * loadings'  +  0.1 * randn(100,4);
>> reconstructed = score   * coeff'     +  repmat(mu, 100, 1);

there is not necessarily any correspondence between factors and score, or between loadings and coeff. The PCA algorithm doesn't know anything about the way your data is constructed - it merely tries to explain as much of the total variance as it can with each successive component.

User @Mari asked in the comments how she could plot the reconstruction error as a function of the number of principal components. Using the variable explained above this is quite easy. I'll generate some data with a more interesting factor structure to illustrate the effect -
>> factors = randn(100, 20);
>> loadings = chol(corr(factors * triu(ones(20))))';
>> observations = factors * loadings' + 0.1 * randn(100, 20);

Now all of the observations load on a significant common factor, with other factors of decreasing importance. We can get the PCA decomposition as before
>> [coeff, score, latent, tsquared, explained, mu] = pca(observations);

and plot the percentage of explained variance as follows,
>> cumexplained = cumsum(explained);
   cumunexplained = 100 - cumexplained;
   plot(1:20, cumunexplained, 'x-');
   grid on;
   xlabel('Number of factors');
   ylabel('Unexplained variance')


 Awesome answer.  Thanks Chris! Great to learn.Thanks much. Though, I have a small doubt, why we need to create hidden variables at first? Can I begin with [w pc ev] = princomp(X); for analyzing my original given data? Thanks again. Yes - I only needed to generate data so that I had something to work with. If you already have data, you can of course work with that. I am trying to figure out how I can write a code which plots the reconstruction error as a function varying number of principal components.Can you please hint me? Thanks again. How can we use this to predict a new piece of data?
You have a pretty good dimensionality reduction toolbox at http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html
Besides PCA, this toolbox has a lot of other algorithms for dimensionality reduction.
Example of doing PCA:
Reduced = compute_mapping(Features, 'PCA', NumberOfDimension);

Machine learning challenge: diagnosing program in java/groovy (datamining, machine learning) 
I'm planning to develop program in Java which will provide diagnosis. The data set is divided into two parts one for training and the other for testing. My program should learn to classify from the training data (BTW which contain answer for 30 questions each in new column, each record in new line the last column will be diagnosis 0 or 1, in the testing part of data diagnosis column will be empty - data set contain about 1000 records) and then make predictions in testing part of data :/
I've never done anything similar so I'll appreciate any advice or information about solution to similar problem.
I was thinking about Java Machine Learning Library or Java Data Mining Package but I'm not sure if it's right direction... ? and I'm still not sure how to tackle this challenge...
Please advise.
All the best!
 +1: To counteract the downvote: this seems an interesting question. Diagonoising what? If you're attempting to diagnosis problems in a program you'll run into the halting problem.
Hi As Gann Bierner said, this is a classification problem. The best classification algorithm for your needs I know of is, Ross Quinlan algorithm. It's conceptually very easy to understand.
For off-the-shelf implementations of the classification algorithms, the best bet is Weka. http://www.cs.waikato.ac.nz/ml/weka/. I have studied Weka but not used, as I discovered it a little too late.
I used a much simpler implementation called JadTi. It works pretty good for smaller data sets such as yours. I have used it quite a bit, so can confidently tell so. JadTi can be found at:
http://www.run.montefiore.ulg.ac.be/~francois/software/jaDTi/
Having said all that, your challenge will be building a usable interface over web. To do so, the dataset will be of limited use. The data set basically works on the premise that you have the training set already, and you feed the new test dataset in one step, and you get the answer(s) immediately.
But my application, probably yours also, was a step by step user discovery, with features to go back and forth on the decision tree nodes.
To build such an application, I created a PMML document from my training set, and built a Java Engine that traverses each node of the tree asking the user to give an input (text/radio/list) and use the values as inputs to the next possible node predicate.
The PMML standard can be found here: http://www.dmg.org/ Here you need the TreeModel only. NetBeans XML Plugin is a good schema-aware editor for PMML authoring. Altova XML can do a better job, but costs $$.
It is also possible to use an RDBMS to store your dataset and create the PMML automagically! I have not tried that.
Good luck with your project, please feel free to let me know if you need further inputs.
 How can you so unequivocally state that decision trees are the best algorithm for the task at hand? You're right, sorry. I guess I'd first try just running it through an SVM since it'd be pretty easy to do quickly (eg. just put the data in a format libsvm understands and run it through) and usually provides great performance relative to the amount of work you have to do to get it to work. You could try boosting, naive bayes, (penalized) logistic regression (check out "glmnet" w/ related reading) ... I'd be hard pressed to pick one as "the best," though. @srini.venigalla Thank You for Your Input!
I strongly recommend you use Weka for your task
Its a collection of machine learning algorithms with a user friendly front-end which facilitates a lot of different kinds of feature and model selection strategies
You can do a lot of really complicated stuff using this without really having to do any coding or math
The makers have also published a pretty good textbook that explains the practical aspects of data mining
Once you get the hang of it, you could use its API to integrate any of its classifiers into your own java programs
 Thank you, I'll have a look on Weka. The software and textbook are really good for getting your head around machine learning, I highly recommend them.
There are various algorithms that fall into the category of "machine learning", and which is right for your situation depends on the type of data you're dealing with.
If your data essentially consists of mappings of a set of questions to a set of diagnoses each of which can be yes/no, then I think methods that could potentially work include neural networks and methods for automatically building a decision tree based on the test data.
I'd have a look at some of the standard texts such as Russel & Norvig ("Artificial Intelligence: A Modern Approach") and other introductions to AI/machine learning and see if you can easily adapt the algorithms they mention to your particular data. See also O'Reilly, "Programming Collective Intelligence" for some sample Python code of one or two algorithms that might be adaptable to your case.
If you can read Spanish, the Mexican publishing house Alfaomega have also published various good AI-related introductions in recent years.
 @ Neil Coffey - No knowledge of Spanish :( but I'll check O'Reilly book. Thank you.
This is a classification problem, not really data mining.  The general approach is to extract features from each data instance and let the classification algorithm learn a model from the features and the outcome (which for you is 0 or 1).  Presumably each of your 30 questions would be its own feature.
There are many classification techniques you can use.  Support vector machines is popular as is maximum entropy.  I haven't used the Java Machine Learning library, but at a glance I don't see either of these.  The OpenNLP project has a maximum entropy implementation.  LibSVM has a support vector machine implementation.  You'll almost certainly have to modify your data to something that the library can understand.
Good luck!
Update: I agree with the other commenter that Russel and Norvig is a great AI book which discusses some of this.  Bishop's "Pattern Recognition and Machine Learning" discusses classification issues in depth if you're interested in the down and dirty details.  
 Thanks, I've got a copy of this book, it's awesome indeed!
Your task is classical for neural networks, which are intended first of all to solve exactly classification tasks. Neural network has rather simple realization in any language, and it is the "mainstream" of "machine learning", closer to AI than anything other.
You just implement (or get existing implementation) standart neural network, for example multilayered network with learning by error back propagation, and give it learning examples in cycle. After some time of such learning you will get it working on real examples.
You can read more about neural networks starting from here:
http://en.wikipedia.org/wiki/Neural_network
http://en.wikipedia.org/wiki/Artificial_neural_network
Also you can get links to many ready implementations here:
http://en.wikipedia.org/wiki/Neural_network_software
What kind of artificial intelligence jobs are out there? [closed] 






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 8 years ago.




Throughout my academic years in computer science I fell in love with many aspects of artificial intelligence. From expert systems, neural networks, to data mining (classification). I wonder, if I was to transform this academic passion professionally, what kind of AI-related jobs are out there? 
 This may not be a job so much as an oppourtunity, but someone should take ALICE and turn that into a "live chat" help desk. www.alicebot.org Note that data-mining is much more than just classification. Classification is more of a machine learning / AI subject than "pure" data-mining (which is a lot about integrated data management and analysis). These two domains are surprisingly separate in the research community.
You would be surprised at the number of domains where AI-based approaches are used. From optimal industrial control, process management and optimization, to business rules and financial modeling, to text analysis, machine translation, search engines... 
Almost anywhere humans have been used to take complex decisions based on data, the amount of data modern electronic communications and acquisitions methods produce has become too much to handle without software. And only "intelligent" (or at least, less single-mindedly stupid) software can handle the complexity of the data, the complexity of the rules, and the numerous failure modes.

Professor for Artificial Intelligence courses. ;)
 What are your students going to do with their skills, teach more?
The most obvious answer to me are games.
I think games present a very interesting challenge for AI, because you're essentially playing to lose but in a fun way.
 i find the chosen answer is much better What you call AI in games most often has nothing to do with the scientific field of AI.
I know one software company in my city is using AI, that was developed as a Masters in Engineering project, to detect fraudulent bank/financial transactions.  It's pretty interesting stuff.  They look for strange recurring payments, or compare account numbers based on known terrorist organizations ...etc.  I'm not sure how many people are doing similar work, but i'm sure with the lock-down on financial institutions these days these types of applications will become more prevalent (it's working for them). 
 Yes, there is a major financial company that had an intrusion. They now use ai for fraud detection. Several others are using it as it "just works better" Funny enough, i now work at the company that i mentioned above and get to see how the AI stuff works.
Aside from direct application - AI people also are usually hardcore algorithms people by nature, and that kind of knowledge is sought after everywhere.
Best clustering algorithm? (simply explained) 
Imagine the following problem:

You have a database containing about 20,000 texts in a table called "articles"
You want to connect the related ones using a clustering algorithm in order to display related articles together
The algorithm should do flat clustering (not hierarchical)
The related articles should be inserted into the table "related"
The clustering algorithm should decide whether two or more articles are related or not based on the texts
I want to code in PHP but examples with pseudo code or other programming languages are ok, too

I've coded a first draft with a function check() which gives "true" if the two input articles are related and "false" if not. The rest of the code (selecting the articles from the database, selecting articles to compare with, inserting the related ones) is complete, too. Maybe you can improve the rest, too. But the main point which is important to me is the function check(). So it would be great if you could post some improvements or completely different approaches.
APPROACH 1
<?php
$zeit = time();
function check($str1, $str2){
    $minprozent = 60;
    similar_text($str1, $str2, $prozent);
    $prozent = sprintf("%01.2f", $prozent);
    if ($prozent > $minprozent) {
        return TRUE;
    }
    else {
        return FALSE;
    }
}
$sql1 = "SELECT id, text FROM articles ORDER BY RAND() LIMIT 0, 20";
$sql2 = mysql_query($sql1);
while ($sql3 = mysql_fetch_assoc($sql2)) {
    $rel1 = "SELECT id, text, MATCH (text) AGAINST ('".$sql3['text']."') AS score FROM articles WHERE MATCH (text) AGAINST ('".$sql3['text']."') AND id NOT LIKE ".$sql3['id']." LIMIT 0, 20";
    $rel2 = mysql_query($rel1);
    $rel2a = mysql_num_rows($rel2);
    if ($rel2a > 0) {
        while ($rel3 = mysql_fetch_assoc($rel2)) {
            if (check($sql3['text'], $rel3['text']) == TRUE) {
                $id_a = $sql3['id'];
                $id_b = $rel3['id'];
                $rein1 = "INSERT INTO related (article1, article2) VALUES ('".$id_a."', '".$id_b."')";
                $rein2 = mysql_query($rein1);
                $rein3 = "INSERT INTO related (article1, article2) VALUES ('".$id_b."', '".$id_a."')";
                $rein4 = mysql_query($rein3);
            }
        }
    }
}
?>

APPROACH 2 [only check()]
<?php
function square($number) {
    $square = pow($number, 2);
    return $square;
}
function check($text1, $text2) {
    $words_sub = text_splitter($text2); // splits the text into single words
    $words = text_splitter($text1); // splits the text into single words
    // document 1 start
    $document1 = array();
    foreach ($words as $word) {
        if (in_array($word, $words)) {
            if (isset($document1[$word])) { $document1[$word]++; } else { $document1[$word] = 1; }
        }
    }
    $rating1 = 0;
    foreach ($document1 as $temp) {
        $rating1 = $rating1+square($temp);
    }
    $rating1 = sqrt($rating1);
    // document 1 end
    // document 2 start
    $document2 = array();
    foreach ($words_sub as $word_sub) {
        if (in_array($word_sub, $words)) {
            if (isset($document2[$word_sub])) { $document2[$word_sub]++; } else { $document2[$word_sub] = 1; }
        }
    }
    $rating2 = 0;
    foreach ($document2 as $temp) {
        $rating2 = $rating2+square($temp);
    }
    $rating2 = sqrt($rating2);
    // document 2 end
    $skalarprodukt = 0;
    for ($m=0; $m<count($words)-1; $m++) {
        $skalarprodukt = $skalarprodukt+(array_shift($document1)*array_shift($document2));
    }
    if (($rating1*$rating2) == 0) { continue; }
    $kosinusmass = $skalarprodukt/($rating1*$rating2);
    if ($kosinusmass < 0.7) {
        return FALSE;
    }
    else {
        return TRUE;
    }
}
?>

I would also like to say that I know that there are lots of algorithms for clustering but on every site there is only the mathematical description which is a bit difficult to understand for me. So coding examples in (pseudo) code would be great.
I hope you can help me. Thanks in advance!
 There are WordPress plugins (yes, yuck, I know, spare me that) that do a surprisingly good job at this, they actually perform reasonable clustering (typically they do TF-IDF with back of words with k-means or something like that) and you can use them for inspirations (some of them are open source under MIT). I think Anony-Mousse is right: clustering is not the ideal tool here.  If each document belongs to just 1 cluster, then you have the problem of documents near the boundaries of a cluster being more similar to documents in other nearby clusters than to most of the documents in their own cluster.
The most standard way I know of to do this on text data like you have, is to use the 'bag of words' technique. 
First, create a 'histogram' of words for each article. Lets say between all your articles, you only have 500 unique words between them. Then this histogram is going to be a vector(Array, List, Whatever) of size 500, where the data is the number of times each word appears in the article. So if the first spot in the vector represented the word 'asked', and that word appeared 5 times in the article, vector[0] would be 5:
for word in article.text
    article.histogram[indexLookup[word]]++

Now, to compare any two articles, it is pretty straightforward. We simply multiply the two vectors:
def check(articleA, articleB)
    rtn = 0
    for a,b in zip(articleA.histogram, articleB.histogram)
        rtn += a*b
    return rtn > threshold

(Sorry for using python instead of PHP, my PHP is rusty and the use of zip makes that bit easier)
This is the basic idea. Notice the threshold value is semi-arbitrary; you'll probably want to find a good way to normalize the dot product of your histograms (this will almost have to factor in the article length somewhere) and decide what you consider 'related'. 
Also, you should not just put every word into your histogram. You'll, in general, want to include the ones that are used semi-frequently: Not in every article nor in only one article. This saves you a bit of overhead on your histogram, and increases the value of your relations. 
By the way, this technique is described in more detail here
 Thank you very much! I've tried to code your approach in PHP and here's the result: paste.bradleygill.com/index.php?paste_id=9290 I hope your PHP is still good enough to say if it's correct or not. It seems to me to be correct, however, depending on your application, you seriously want to consider persisting the state of the term vectors. Also, consider dividing the score by the length of article a times the length of article b. Otherwise you will see a bias for long articles that are only marginally related. Sorry, certainly a stupid question, but what do you exactly mean with "consider persisting the state of the term vectors." On the second point: Do you mean "$score = $score/$length_a*$length_b" or "$score = $score/($length_a*$length_b)"? Probably the first one, right? I mean, instead of creating that vector whenever you are about to compare two articles, generate that vector whenever anyone saves an article and store it in a database.   The second point: You want '$score = $score/($length_a*$length_b)'. If you check out the link I put up above, it has more about why you should do that(You are finding the 'angle' between the two vectors basically) Thank you for the quick replies. Now it should be finally correct: paste.bradleygill.com/index.php?paste_id=9326
Maybe clustering is the wrong strategy here?
If you want to display similar articles, use similarity search instead.
For text articles, this is well understood. Just insert your articles in a text search database like Lucene, and use your current article as search query. In Lucene, there exists a query called MoreLikeThis that performs exactly this: find similar articles.
Clustering is the wrong tool, because (in particular with your requirements), every article must be put into some cluster; and the related items would be the same for every object in the cluster. If there are outliers in the database - a very likely case - they could ruin your clustering. Furthermore, clusters may be very big. There is no size constraint, the clustering algorithm may decide to put half of your data set into the same cluster. So you have 10000 related articles for each article in your database. With similarity search, you can just get the top-10 similar items for each document!
Last but not least: forget PHP for clustering. It's not designed for this, and not performant enough. But you can probably access a lucene index from PHP well enough.

I believe you need to make some design decisions about clustering, and continue from there:

Why are you clustering texts? Do you want to display related documents together? Do you want to explore your document corpus via clusters? 
As a result, do you want flat or hierarchical clustering?
Now we have the complexity issue, in two dimensions: first, the number and type of features you create from the text - individual words may number in the tens of thousands. You may want to try some feature selection - such as taking the N most informative words, or the N words appearing the most times, after ignoring stop words.
Second, you want to minimize the number of times you measure similarity between documents. As bubaker correctly points out, checking similarity between all pairs of documents may be too much. If clustering into a small number of clusters is enough, you may consider K-means clustering, which is basically: choose an initial K documents as cluster centers, assign every document to the closest cluster, recalculate cluster centers by finding document vector means, and iterate. This only costs K*number of documents per iteration. I believe there are also heuristics for reducing the needed number of computations for hierarchical clustering as well. 

 Thanks, good questions! 1) I want to display related documents together. 2) The algorithm should do flat clustering. 3) This would be useful if the texts were long, but in my case the articles contain at most 510 characters. So it isn't really necessary, is it? 4) The approach with k-means sounds good but I need lots of clusters and new clusters must be continuously created. Can I use k-means, though? You can use K-means with K being very large. The cost is having to check the similarity of each document with every one of the clusters' centers. 'Continouously create new clusters' sounds like a top-down hierarchical clustering to me, but you can work in several epochs - start with a small K, run K-means until it converges, and use these clusters. Later, increase K, rerun K-means from the start, and use the resulting clusters, etc. Oh, I didn't know how k-means exactly works. If it works like that, I can't use it since I don't know the number of cluster centers. I have a database of news articles and all articles about the same topic should be grouped.
What does the similar_text function called in Approach #1 look like?  I think what you're referring to isn't clustering, but a similarity metric.  I can't really improve on the White Walloun's :-) histogram approach - an interesting problem to do some reading on.
However you implement check(), you've got to use it to make at least 200M comparisons (half of 20000^2).  The cutoff for "related" articles may limit what you store in the database, but seems too arbitrary to catch all useful clustering of texts, 
My approach would be to modify check() to return the "similarity" metric ($prozent or rtn).  Write the 20K x 20K matrix to a file and use an external program to perform a clustering to identify nearest neighbors for each article, which you could load into the related table.  I would do the clustering in R - there's a nice tutorial for clustering data in a file running R from php.
 The function similar_text() "calculates the similarity between two strings as described in Oliver [1993]". Yes, you're right, it's rather a similarity metric. But you need similarity checks for clustering, don't you?How can I perform K-means clustering on time series data? 
How can I do K-means clustering of time series data?
I understand how this works when the input data is a set of points, but I don't know how to cluster a time series with 1XM, where M is the data length. In particular, I'm not sure how to update the mean of the cluster for time series data.
I have a set of labelled time series, and I want to use the K-means algorithm to check whether I will get back a similar label or not. My X matrix will be N X M, where N is number of time series and M is data length as mentioned above.
Does anyone know how to do this? For example, how could I modify this k-means MATLAB code so that it would work for time series data? Also, I would like to be able to use different distance metrics besides Euclidean distance.
To better illustrate my doubts, here is the code I modified for time series data:

% Check if second input is centroids
if ~isscalar(k) 
    c=k;
    k=size(c,1);
else
    c=X(ceil(rand(k,1)*n),:); % assign centroid randomly at start
end

% allocating variables
g0=ones(n,1); 
gIdx=zeros(n,1);
D=zeros(n,k);

% Main loop converge if previous partition is the same as current
while any(g0~=gIdx)
%     disp(sum(g0~=gIdx))
    g0=gIdx;
    % Loop for each centroid
    for t=1:k
        %  d=zeros(n,1);
        % Loop for each dimension
        for s=1:n
            D(s,t) = sqrt(sum((X(s,:)-c(t,:)).^2)); 
        end
    end
    % Partition data to closest centroids
    [z,gIdx]=min(D,[],2);
    % Update centroids using means of partitions
    for t=1:k

        % Is this how we calculate new mean of the time series?
        c(t,:)=mean(X(gIdx==t,:));

    end
end


Time series are usually high-dimensional. And you need specialized distance function to compare them for similarity. Plus, there might be outliers.
k-means is designed for low-dimensional spaces with a (meaningful) euclidean distance. It is not very robust towards outliers, as it puts squared weight on them.
Doesn't sound like a good idea to me to use k-means on time series data. Try looking into more modern, robust clustering algorithms. Many will allow you to use arbitrary distance functions, including time series distances such as DTW.
 could you please suggest some of the robust clustering algorithms. and what is DTW? thanks. Grab any book on time series, and it will teach you DTW. Or google for "time series DTW". It's state of the art. As for clustering, look up DBSCAN and OPTICS on Wikipedia. They can be used with DTW, k-means cannot.
It's probably too late for an answer, but:

k-means can be used to cluster longitudinal data
Anony-Mousse is right, DWT distance is the way to go for time series

The methods above use R. You'll find more methods by looking, e.g., for "Iterative Incremental Clustering of Time Series".

I have recently come across the kml R package which claims to implement k-means clustering for longitudinal data. I have not tried it out myself.
Also the Time-series clustering - A decade review paper by S. Aghabozorgi, A. S. Shirkhorshidi and T. Ying Wah might be useful to you to seek out alternatives. Another nice paper although somewhat dated is Clustering of time series data-a survey by T. Warren Liao.

If you did really want to use clustering, then dependent on your application you could generate a low dimensional feature vector for each time series. For example, use time series mean, standard deviation, dominant frequency from a Fourier transform etc. This would be suitable for use with k-means, but whether it would give you useful results is dependent on your specific application and the content of your time series.

I don't think k-means is the right way for it either. As @Anony-Mousse suggested you can utilize DTW. In fact, I had the same problem for one of my projects and I wrote my own class for that in Python. The logic is; 

Create your all cluster combinations. k is for cluster count and n is for number of series. The number of items returned should be n! / k! / (n-k)!. These would be something like potential centers.
For each series, calculate distances for each center in each cluster groups and assign it to the minimum one.
For each cluster groups, calculate total distance within individual clusters.
Choose the minimum.

And, the Python implementation is here if you're interested.
How to find common phrases in a large body of text 
I'm working on a project at the moment where I need to pick out the most common phrases in a huge body of text. For example say we have three sentences like the following:

The dog jumped over the woman.
The dog jumped into the car.
The dog jumped up the stairs.

From the above example I would want to extract "the dog jumped" as it is the most common phrase in the text. At first I thought, "oh lets use a directed graph [with repeated nodes]":
directed graph http://img.skitch.com/20091218-81ii2femnfgfipd9jtdg32m74f.png
EDIT: Apologies, I made a mistake while making this diagram "over", "into" and "up" should all link back to "the".
I was going to maintain a count of how many times a word occurred in each node object ("the" would be 6; "dog" and "jumped", 3; etc.) but despite many other problems the main one came up when we add a few more examples like (please ignore the bad grammar :-)): 

Dog jumped up and down.
Dog jumped like no dog had ever jumped before.
Dog jumped happily.

We now have a problem since "dog" would start a new root node (at the same level as "the") and we would not identify "dog jumped" as now being the most common phrase. So now I am thinking maybe I could use an undirected graph to map the relationships between all the words and eventually pick out the common phrases but I'm not sure how this is going to work either, as you lose the important relationship of order between the words. 
So does anyone have any general ideas on how to identify common phrases in a large body of text and what data structure I would use.
Thanks,
Ben
 I'm not in a position to offer anything specific about algorithms to use. However, Have you noticed the arrival of igraph for representing and manipulating graphs? I use Python and the bindings for that make the underlying product look pretty nifty. igraph.sourceforge.net Just linking back to a later post    stackoverflow.com/questions/8898521/…
Check out this related question: What techniques/tools are there for discovering common phrases in chunks of text?  Also related to the longest common substring problem.
I've posted this before, but I use R for all of my data-mining tasks and it's well suited to this kind of analysis.  In particular, look at the tm package.  Here are some relevant links:

Paper about the package in the Journal of Statistical Computing: http://www.jstatsoft.org/v25/i05/paper.  The paper includes a nice example of an analysis of the R-devel
mailing list (https://stat.ethz.ch/pipermail/r-devel/) newsgroup postings from 2006.
Package homepage: http://cran.r-project.org/web/packages/tm/index.html
Look at the introductory vignette: http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

More generally, there are a large number of text mining packages on the Natural Language Processing view on CRAN.  
 I don't believe the longest common substring problem solves the problem as from what I've read an LCS algorithm is going to favour a longer less popular common string over a shorter more popular common string, correct me if I am wrong.   R looks interesting, I've only looked at it shortly before, will definitely take another look. Thank you. From the link above, Norman Ramsey's suggestion to use n-grams should help. Turns out I was being ignorant, this lead me to solve my problem. :)How exactly does sharkscope or PTR data mine all those hands? 
I'm very curious to know how this process works. These sites (http://www.sharkscope.com and http://www.pokertableratings.com) data mine thousands of hands per day from secure poker networks, such as PokerStars and Full Tilt.
Do they have a farm of servers running applications that open hundreds of tables (windows) and then somehow spider/datamine the hands that are being played?
How does this work, programming wise?
 I'm guessing that the data is given/sold to them. @jnpcl this is not true I'm afraid.  Some of those sites have agreements, but as far as I am aware they (originally) all grab the data without consent/awareness from the sites in question.
There are a few options. I've been researching it since I wanted to implement some of this functionality in a web app I'm working on. I'll use PokerStars for example, since they have, by far, the best security of any online poker site. 
First, realize that there is no way for a developer to rip real time information from the PokerStars application itself. You can't access the API. You can, though, do the following:
Screen Scraping/OCR
PokerStars does its best to sabotage screen/text scraping of their application (by doing simple things like pixel level color fluctuations) but with enough motivation you can easily get around this. Google AutoHotkey combined with ImageSearch.
API Access and XML Feeds
PokerStars doesn't offer public access to its API. But it does offer an XML feed to developers who are pre-approved. This XML feed offers:

PokerStars Site Summary - shows player, table, and tournament counts
PokerStars Current Tournament data - files with information about upcoming and active tournaments. The data is provided in two files:

PokerStars Static Tournament Data - provides tournament information that does not change frequently, and
PokerStars Dynamic Tournament Data - provides frequently changing tournament information

PokerStars Tournament Results - provides information about completed tournaments. The data is provided in two files:

PokerStars Tournament Results – provides basic information about completed tournaments, and
PokerStars Tournament Expanded Results – provides expanded information about completed tournaments.

PokerStars Tournament Leaders Board - provides information about top PokerStars players ranked using PokerStars Tournament Ranking System
PokerStars Tournament Leaders Board BOP - provides information about top PokerStars players ranked using PokerStars Battle Of Planets Ranking System
Team PokerStars – provides information about Team PokerStars players and their online activity

It's highly unlikely that these sites have access to the XML feed (or an improved one which would provide all the functionality they need) since PokerStars isn't exactly on good terms with most of these sites.
This leaves two options. Scraping the network connection for said data, which I think is borderline impossible (I don't have experience with this so I'm not sure; I've heard it's highly encrypted and not easy to tinker with, but I'm not sure) and, mentioned above, screen scraping/OCR. 
Option #2 is easy enough to implement and, with some work, can avoid detection. From what I've been able to gather, this is the only way they could be doing such massive data mining of PokerStars (I haven't looked into other sites but I've heard security on anything besides PokerStars/Full Tilt is quite horrendous). 
[edit]
Reread your question and realized I didn't unambiguously answer it.
Yes, they likely have a massive amount of servers running watching all currently running tables, tournaments, etc. Realize that there is a decent amount of money in what they're doing.
This, for instance, could be how they do it (speculation):
Said bot applications watch the tables and data mine all information that gets "posted" to the chat log. They do this by already having a table of images that correspond to, for example, all letters of the alphabet (since PokerStars doesn't post their text as... text. All text in their software is actually an image). So, the bot then rips an image of the chat log, matches it against the store, converts the data to a format they can work with, and throws it in a database. Done.
[edit]
No, the data isn't sold to them by the poker sites themselves. This would be a PR nightmare if it ever got out, which it would. And it wouldn't account for the functionality of these sites, which appears to be instantaneous. OPR, Sharkscope, etc. There are, without a doubt, applications running that are ripping the data real time from the poker software, likely using the methods I listed.
 now, THIS is an answer folks !Looks like you really know what you are talking about. thank you very much ! the question is not mine, the answer is accepted already. You got the bounty, way more valuable :) Haven't visited this question for a while, but thanks for your thorough answer :) I'm pretty sure they are using chat-scraping-ocr as well -- must be some infrastructure they have. Do you have any info on the XML feeds? Do you need to be an associate? (pokerstarspartners.com/public/marketing/online/xmlFeeds.html) This is accurate.  I've been working on screenscraping pokerstars to make a kind of "Advisor" app.  The problems I've run into are color fluctuations,  in some cases card positions are "off" by a bit which can mess up OCR, also OCR can be difficult for some cards.  XML is not available, network scraping is locked down so you'll be lucky to get at the data, the only thing left is screen scraping.  Screen scraping is doable, it's just quite difficult when locating elements and hardcoding their positions.  Doable though,  good luck.
maybe I can help.
I play poker, run a HUD, look at the stats and am a software developer. 
I've seen a few posts on this suggesting it's done by OCR software grabbing the screen. Well, that's really difficult and processor hungry, so a programmer wouldn't choose to do that unless there were no other options. 
Also, because you can open multiple windows, the poker window can be hidden or partially obscured by other things on the screen, so you couldn't guarantee to be able to capture the screen.
In short, they read the log files that are output by the poker software. 
When you install your HUD like Sharkscope or Jivaro etc, than they run client software on your PC. It reads the log files and updates its own servers with every hand you play.
Most poker software is similar, but lets start with Pokerstars, as thats where I play. The Poker software outputs to local log files for every action you/it makes. It shows your cards, any opponents cards that you see plus what you do. eg. which button you have pressed, how much you/they bet etc. It posts these updates in near real time and timestamps the log file. 
You can look at your own files to see this in action.
On a PC do this (not sure what you do on a Mac, but will be similar)
1. Load File Explorer
2. Select VIEW from the menu
3. Select HIDDEN ITEMS so that you can see the hidden data files
4. Goto C:\Users\Dave\AppData\Local\PokerStars.UK (you may not be called DAVE...)
5. Open the PokerStars.log.0 file in NOTEPAD
6. In Notepad, SEARCH for updateMyCard
7. It will show your card numerically 
   3c for 3 of Clubs
   14d for Ace of Diamonds
You can see your opponents cards only where you saw them at the table.
Here is a few example lines from the log file.
OnTableData() round -2
:::TableViewImpl::updateMyCard() 8s (0) [2A0498]
:::TableViewImpl::updateMyCard() 13h (1) [2A0498]
:::TableViewImpl::updatePlayerCard() 7s (0) [2A0498]
:::TableViewImpl::updatePlayerCard() 14s (1) [2A0498]
[2015/12/13 12:19:34]

cheers, hope this helps
Dave
 Do users have any unique id or something similar and how can I get their actions i only see my own. This is the correct answer.  After the hand has been played PokerStars writes the hand to a log file.  The key here is they only do it AFTER the hand is played, meaning you can't analyse it in real time.  To do it in real time you'll have to screen scrape.
I've thought about this, and have two theories:
The "sniffer" sites have every table open, AND:

Are able to pull the hand data from the network stream. (or:)
Are obtaining the hand data from the GUI (screen scraping, pulling stuff out via the GUI API).

Alternately, they may have developed/modified clients to log everything for them, but I think one of the above solutions is likely simpler.

Well, they have two choices:

they spider/grab the data without consent. Then they risk being shut down anytime. The poker site can easily detect such monitoring at this scale and block it. And even risk a lawsuit for breach of the terms of service, which probably disallow the use of robots.
they pay for getting the data directly. This saves a lot of bandwidth (e.g. not having to load the full pages, extraction, updates with html changes etc.) and makes their business much less risky (legally and technically).

Guess which one they more likely chose; at least if the site has been around for some time without being shut down every now and then.
 I suspect you don't even have a clue what PTR or Sharkscope is... Why bother answering then ? I need facts not guessing.Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1) 
I have a data table ("norm") containing numeric - at least to what I can see - normalized values of the following form:

When I am executing
k <- kmeans(norm,center=3)

I am receving the following error:
Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1)

Can you help me? Thank you!
 Have you checked if there's NaN/NA/Inf in your data? You can check using is.na() and is.finite() functions Yes, there is plenty of NAs in my file sheet, but I thought that shouldnt be a problem?! is.finite() returns a lot of TRUEs but also some FALSEs.  How can i fix this? You would have to remove the NA/Inf/NaN values from your data. See "missing value imputation" methods for details. One simple method is replacing them by row/column mean values. @UjjwalKumar Thank you!
kmeans cannot handle data that has NA values.
The mean and variance are then no longer well defined, and you don't know anymore which center is closest.


Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1)

This error occurs also due to non numeric values present in the table.

all of you all who are having " Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1)" problem 
instead of 
results <- kmeans(iris.features,3)
results

write the following and please be careful of the case in iris write whatever you have used in the beginning
results <- kmeans(na.omit(irisa.features),3) # this helps in omitting NA 
results


For error stating:

Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1)

the dataset you have posted above contains scaled entries, the reason must be that you have NA values in your dataset, hence omit them by the following code.
km_cluster <- kmeans(na.omit(MyData), 3)
km_cluster
km_cluster$withinss
km_cluster$tot.withinss/km_cluster$betweenss

GBM R function: get variable importance separately for each class 
I am using the gbm function in R (gbm package) to fit stochastic gradient boosting models for multiclass classification. I am simply trying to obtain the importance of each predictor separately for each class, like in this picture from the Hastie book (the Elements of Statistical Learning) (p. 382). 

However, the function summary.gbm only returns the overall importance of the predictors (their importance averaged over all classes). 
Does anyone know how to get the relative importance values? 
 @germcd ?? I don't see how that would change the problem... @germcd Do you advise building a different model for each category of the target variable that needs to be predicted? I don't really understand where this is going. Thanks for the link to the book - seems like an interesting read. it seems this library could provide a workaround (python): github.com/marcotcr/lime
I think the short answer is that on page 379, Hastie mentions that he uses MART, which appears to only be available for Splus.
I agree that the gbm package doesn't seem to allow for seeing the separate relative influence. If that's something you're interested in for a mutliclass problem, you could probably get something pretty similar by building a one-vs-all gbm for each of your classes and then getting the importance measures from each of those models.
So say your classes are a, b, c, & d. You model a vs. the rest and get the importance from that model. Then you model b vs. the rest and get the importance from that model. Etc.
 Almost 3 years have passed but so far there is no answer. Do you have any additional hint besides the effective work-around you proposed in this answer? Unfortunately not - I haven't looked into it much over the years and I've primarily been working in Python Does python offer this kind of solution? @Tchotchke What do you think of my method of using the error reduction for each tree (see answer below)? I am using this in my work and I would really appreciate any thoughts you might have.
Hopefully this function helps you. For the example I used data from the ElemStatLearn package. The function figures out what the classes for a column are, splits the data into these classes, runs the gbm() function on each class and plots the bar plots for these models.
# install.packages("ElemStatLearn"); install.packages("gbm")
library(ElemStatLearn)
library(gbm)

set.seed(137531)

# formula: the formula to pass to gbm()
# data: the data set to use
# column: the class column to use
classPlots <- function (formula, data, column) {

    class_column <- as.character(data[,column])
    class_values <- names(table(class_column))
    class_indexes <- sapply(class_values, function(x) which(class_column == x))
    split_data <- lapply(class_indexes, function(x) marketing[x,])
    object <- lapply(split_data, function(x) gbm(formula, data = x))
    rel.inf <- lapply(object, function(x) summary.gbm(x, plotit=FALSE))

    nobjs <- length(class_values)
    for( i in 1:nobjs ) {
        tmp <- rel.inf[[i]]
        tmp.names <- row.names(tmp)
        tmp <- tmp$rel.inf
        names(tmp) <- tmp.names

        barplot(tmp, horiz=TRUE, col='red',
                xlab="Relative importance", main=paste0("Class = ", class_values[i]))
    }
    rel.inf
}

par(mfrow=c(1,2))
classPlots(Income ~ Marital + Age, data = marketing, column = 2)

`

 The interpretation for this example would be that age greatly effects male income and marital status greatly effects female income thank you very much for this helpful answer. Let me play with your commands in detail before I accept the answer/award the bounty. Also, from a theoretical standpoint, I am wondering whether it is valid to compare the influence that variables have for two separate models... In fact, it is the same model, just on two subsets of the data. Why would this be invalid? we are using the same algorithm in both cases, granted, but in the end we obtain two different models, since the data sets are different. If you compare the final equations (Boosting is similar to a generalized additive model), they won't be the same. So, it's not like we were comparing the relative importance of variables in predicting each class for a given, unique model. Agree - when I proposed this solution above it was an approximation of the solution you were looking for - I don't think it's quite doing the same thing as Hastie did, but it probably gets close enough (and is the easiest thing to do out-of-the-box in R that I could think of)
I did some digging into how the gbm package calculates importance and it is based on the ErrorReduction which is contained in the trees element of the result and can be accessed with pretty.gbm.trees(). Relative influence is obtained by taking the sum of this ErrorReduction over all trees for each variable. For a multiclass problem there are actually n.trees*num.classes trees in the model. So if there are 3 classes you can calculate the sum of the ErrorReduction for each variable over every third tree to get the importance for one class. I have written the following functions to implement this and then plot the results:
Get Variable Importance By Class
RelInf_ByClass <- function(object, n.trees, n.classes, Scale = TRUE){
  library(dplyr)
  library(purrr)
  library(gbm)
  Ext_ErrRed<- function(ptree){
    ErrRed <- ptree %>% filter(SplitVar != -1) %>% group_by(SplitVar) %>% 
      summarise(Sum_ErrRed = sum(ErrorReduction))
  }
  trees_ErrRed <- map(1:n.trees, ~pretty.gbm.tree(object, .)) %>% 
    map(Ext_ErrRed)

  trees_by_class <- split(trees_ErrRed, rep(1:n.classes, n.trees/n.classes)) %>% 
    map(~bind_rows(.) %>% group_by(SplitVar) %>% 
          summarise(rel_inf = sum(Sum_ErrRed)))
  varnames <- data.frame(Num = 0:(length(object$var.names)-1),
                         Name = object$var.names)
  classnames <- data.frame(Num = 1:object$num.classes, 
                           Name = object$classes)
  out <- trees_by_class %>% bind_rows(.id = "Class") %>%  
    mutate(Class = classnames$Name[match(Class,classnames$Num)],
    SplitVar = varnames$Name[match(SplitVar,varnames$Num)]) %>%
    group_by(Class) 
  if(Scale == FALSE){
    return(out)
    } else {
    out <- out %>% mutate(Scaled_inf = rel_inf/max(rel_inf)*100)
    }
}

Plot Variable Importance By Class
In my real use for this I have over 40 features so I give an option to specify the number of features to plot. I also couldn't use faceting if I wanted the plots to be sorted separately for each class, which is why I used gridExtra. 
plot_imp_byclass <- function(df, n) {
  library(ggplot2)
  library(gridExtra)
  plot_imp_class <- function(df){
    df %>% arrange(rel_inf) %>% 
      mutate(SplitVar = factor(SplitVar, levels = .$SplitVar)) %>% 
      ggplot(aes(SplitVar, rel_inf))+
      geom_segment(aes(x = SplitVar, 
                       xend = SplitVar, 
                       y = 0, 
                       yend = rel_inf))+
      geom_point(size=3, col = "cyan") + 
      coord_flip()+
      labs(title = df$Class[[1]], x = "Variable", y = "Importance")+
      theme_classic()+
      theme(plot.title = element_text(hjust = 0.5))
  }

  df %>% top_n(n, rel_inf) %>% split(.$Class) %>% 
    map(plot_imp_class) %>% map(ggplotGrob) %>% 
    {grid.arrange(grobs = .)}
}

Try It
gbm_iris <- gbm(Species~., data = iris)
imp_byclass <- RelInf_ByClass(gbm_iris, length(gbm_iris$trees), 
                              gbm_iris$num.classes, Scale = F)
plot_imp_byclass(imp_byclass, 4)

Seems to give the same results as the built in relative.influence function if you sum the results over all the classes. 
relative.influence(gbm_iris)
# n.trees not given. Using 100 trees.
# Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
# 0.00000     51.88684   2226.88017    868.71085 

imp_byclass %>% group_by(SplitVar) %>% summarise(Overall_rel_inf = sum(rel_inf))
# A tibble: 3 x 2
# SplitVar     Overall_rel_inf
# <fct>                  <dbl>
#   1 Petal.Length          2227. 
# 2 Petal.Width            869. 
# 3 Sepal.Width             51.9

 thanks, I'll take a close look in the weeks to come. In the meantime +1 for sharing your code! The gbm package has been orphaned, and it seems the most recent version lives on GitHub as gbm3:github.com/gbm-developers/gbm3 Great! I have opened a github issue on the gbm3 page so hopefully they will add this functionality to the new version @Antoine have you had a chance to look at this yet? I've started using it in my work so if you see any issues I would be grateful to hear about them! I'm just back from a two-week vacation. I won't have time to look at this in the very short term, but I'll try as soon as I have a chanceBetter text documents clustering than tf/idf and cosine similarity? 
I'm trying to cluster the Twitter stream. I want to put each tweet to a cluster that talk about the same topic. I tried to cluster the stream using an online clustering algorithm with tf/idf and cosine similarity but I found that the results are quite bad.
The main disadvantages of using tf/idf is that it clusters documents that are keyword similar so it's only good to identify near identical documents. For example consider the following sentences:
1- The website Stackoverflow is a nice place.
2- Stackoverflow is a website.
The prevoiuse two sentences will likely by clustered together with a reasonable threshold value since they share a lot of keywords. But now consider the following two sentences:
1- The website Stackoverflow is a nice place.
2- I visit Stackoverflow regularly.
Now by using tf/idf the clustering algorithm will fail miserably because they only share one keyword even tho they both talk about the same topic.
My question: is there better techniques to cluster documents?
 @ThomasJungblut well, TF-IDF is supposed to be a weighting scheme that puts more weight on relevant keywords already. If figure the problem is that tweets are just so tiny text fragments, you can't expect similarity to work very well on them beyond "near identity". Most tweets aren't even complete sentences, so NLP will likely also fail. One thing to watch with LSI / LDA / NMF etc. is topic drift. Training a model on a known dataset will yield good results if your pipeline isn't done correctly. If you then apply your model to a totally unseen dataset you may see significant drop in performance due to fitting the original training data. Because Twitter text is so short the representation will need a bit of fiddling with as there may not be enough text to train a model properly. @steve are there any solutions to this? @guckogucko does the user it comes from have any impact? @steve what do you mean?
In my experience, cosine similarity on latent semantic analysis (LSA/LSI) vectors works a lot better than raw tf-idf for text clustering, though I admit I haven't tried it on Twitter data. In particular, it tends to take care of the sparsity problem that you're encountering, where the documents just don't contain enough common terms.
Topic models such as LDA might work even better.
 are topic models clustering techniques? or features representation? @guckogucko: feature representations.
As mentioned in other comments and answers. Using LDA can give good tweet->topic weights.
If these weights are insufficient clustering for your needs you could look at clustering these topic distributions using a clustering algorithm.
While it is training set dependent LDA could easily bundle tweets with stackoverflow, stack-overflow and stack overflow into the same topic. However "my stack of boxes is about to overflow" might instead go into another topic about boxes.
Another example: A tweet with the word Apple could go into a number of different topics (the company, the fruit, New York and others). LDA would look at the other words in the tweet to determine the applicable topics.

"Steve Jobs was the CEO at Apple" is clearly about the company
"I'm eating the most delicious apple" is clearly about the fruit
"I'm going to the big apple when I travel to the USA" is most likely about visiting New York


Long answer:
TfxIdf is currently one of the most famous search method. What you need are some preprocessing from Natural Langage Processing (NLP). There is a lot of resources that can help you for english (for example the lib 'nltk' in python).
You must use the NLP analysis both on your querys (questions) and on yours documents before indexing.
The point is : while tfxidf (or tfxidf^2 like in lucene)  is good, you should use it on annotated resource with meta-linguistics information. That can be hard and require extensive knowledge about your core search engine, grammar analysis (syntax) and the domain of document.
Short answer : The better technique is to use TFxIDF with light grammar NLP annotations, and both re-write query and indexing.
Can an author's unique “literary style” be used to identify him/her as the author of a text? [closed] 






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                
Closed 4 years ago.




Let's imagine, I have two English language texts written by the same person.
Is it possible to apply some Markov chain algorithm to analyse each: create some kind of fingerprint based on statistical data, and compare fingerprints gotten from different texts?
Let's say, we have a library with 100 texts. Some person wrote text number 1 and some other as well, and we need to guess which one by analyzing his/her writing style.
Is there any known algorithm doing it? Can be Markov chains applied here?
 A famous example is, who wrote which Federalist Papers ? See notes 19 and 20 there. I feel this question shouldn't be closed. In 2017 there was a competition in Kaggle: Spooky Author Identification, which shows the relevance of this question.
Absolutely it is possible, and indeed the record of success in identifying an author given a text or some portion of it, is impressive.
A couple of representative studies (warning: links are to pdf files):

Quantitative Analysis of Literary Styles
Stylogenetics: Clustering-based stylistic analysis of literary coroora

To aid your web-search, this discipline is often called Stylometry (and occasionally, Stylogenetics).
So the two most important questions are i suppose: which classifiers are useful for this purpose and what data is fed to the classifier?
What i still find surprising is how little data is required to achieve very accurate classification. Often the data is just a word frequency list. (A directory of word frequency lists is available online here.)
For instance, one data set widely used in Machine Learning and available from a number of places on the Web, is comprised of data from four authors: Shakespeare, Jane Austen, Jack London, Milton. these works were divided into 872 pieces (corresponding roughly to chapters), in other words, about 220 different substantial pieces of text for each of the four authors; each of these pieces becomes a single data point in the data set. Next a word-frequency scan was performed on each text, and the 70 most common words were used for the study, the remainder of the results of the frequency scan were discarded. Here are the first 20 of that 70-word list.
['a', 'all', 'also', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'been',
  'but', 'by', 'can', 'do', 'down', 'even', 'every', 'for', 'from'] 

Each data point then is just a count of each word of the 70 words in each of the 872 chapters. 
[78, 34, 21, 45, 76, 9, 23, 12, 43, 54, 110, 21, 45, 59, 87, 59, 34, 104, 93, 40]

Each of these data points is one instance of the author's literary fingerprint.
The final item in each data point is an integer (1-4) representing one of the four authors to whom that text belongs.
Recently, I ran this dataset through a simple unsupervised ML algorithm; the results were very good--almost complete separation of the four classes, which you can see in my Answer to a previous Q on StackOverflow related to text classification using ML generally, rather than author identification.
So what other algorithms are used? Apparently, most Machine Learning algorithms in the supervised category can successfully resolve this kind of data. Among these, multi-layer perceptrons (MLP, aka, neural networks) are often used (Author Attribution Using Neural Networks is one such frequently-cited study).
 Is it possible then to trace an anonymous article to its author by analyzing public SNS texts? @FRIdSUN not sure what you mean by "SNS" i'll assume it's a typo and you meant SMS. If so, my answer is no. The reason is that SMS messages have their own (informal) style, syntax, and usage rules, and those rules would. effectively conceal an author's literary prose style. So for instance, stop word frequency, often a strong signature of author style (ie, consistent across many of the author's texts) is probably useless for SMS--eg, SMS texts rarely have any stop words ("a", "a", "the") for brevity sake, often use symbols instead of stop words ("&" for "and"), etc. SNS = Social Network Service. I meant if it is possible to analyze Facebook posts, Twitter tweets, Medium articles and the like to do such identification. Prime reason not to abbreviate something if is not well known.
You might start with a visit to the Apache Mahout web site. There is a giant literature on classification and clustering. Essentially, you want to run a clustering algorithm, and then hope that 'which writer' determines the clusters.
 +1 for the Apach Mahout referenceWhat are some good ways of estimating 'approximate' semantic similarity between sentences? 
I have been looking at the nlp tag on SO for the past couple of hours and am confident I did not miss anything but if I did, please do point me to the question. 
In the mean time though, I will describe what I am trying to do. A common notion that I observed on many posts is that semantic similarity is difficult. For instance, from this post, the accepted solution suggests the following:
First of all, neither from the perspective of computational 
linguistics nor of theoretical linguistics is it clear what 
the term 'semantic similarity' means exactly. .... 
Consider these examples:

Pete and Rob have found a dog near the station.
Pete and Rob have never found a dog near the station.
Pete and Rob both like programming a lot.
Patricia found a dog near the station.
It was a dog who found Pete and Rob under the snow.

Which of the sentences 2-4 are similar to 1? 2 is the exact 
opposite of 1, still it is about Pete and Rob (not) finding a 
dog.

My high-level requirement is to utilize k-means clustering and categorize the text based on semantic similarity so all I need to know is whether they are an approximate match. For instance, in the above example, I am OK with classifying 1,2,4,5 into one category and 3 into another (of course, 3 will be backed up with some more similar sentences). Something like, find related articles, but they don't have to be 100% related. 
I am thinking I need to ultimately construct vector representations of each sentence, sort of like its fingerprint but exactly what this vector should contain is still an open question for me. Is it n-grams, or something from the wordnet or just the individual stemmed words or something else altogether?
This thread did a fantastic job of enumerating all related techniques but unfortunately stopped just when the post got to what I wanted. Any suggestions on what is the latest state-of-the-art in this area?

Latent Semantic Modeling could be useful.  It's basically just yet another application of the Singular Value Decomposition.  The SVDLIBC is a pretty nice C implementation of this approach, which is an oldie but a goodie, and there are even python binding in the form of sparsesvd.  
 A Python implementation of SVD for sparse matrices is also provided in scikit-learn as scikits.learn.utils.extmath.fast_svd.
I suggest you try a topic modelling framework such as Latent Dirichlet Allocation (LDA). The idea there is that documents (in your case sentences, which might prove to be a problem) are generated from a set of latent (hidden) topics; LDA retrieves those topics, representing them by word clusters.
An implementation of LDA in Python is available as part of the free Gensim package. You could try to apply it to your sentences, then run k-means on its output.
 Yes, I think that LDA is all the rage now.Twitter: How to extract tweets containing symbols (!,%,$)? 
For a project, I want to be able to create a dataset of tweets containing some particular string of symbols. Since I would also like to go as far back in time as possible, I tried using the GetOldTweets script ( https://github.com/Jefferson-Henrique/GetOldTweets-python ) mentioned here: https://stackoverflow.com/a/35077920/5858873 .
The issue is, it isn't able to extract tweets containing symbols as input. In fact, one cannot even search directly on Twitter for any tweets consisting of required symbols.
To more clearly explain the problem, consider the following sample case. I would l like to extract all tweets containing the string '!!!' within the last two years.  
What is the best way to do this (if this even is doable)?
 One way is to grab the data (tweets) and manually parse them for your symbols (which will be slow but will get the job done). Another is to look up Twitter's API and see if it support a search function. A quick google yields this. @MooingRawr in which case, however, I would have to first extract ALL tweets (which is impossible).  Also, I don't think the Twitter API allows searching for tweets that only contain symbols. If you want to go official it looks like it will cost you I would be VERY surprise if Twitter didn't support symbol search while they allow normal search... I feel this is important because it will help future posters (these are all relevant clarifications!). What you quoted works if the phrase contains symbols, but does not work for a string comprised entirely of symbols. So, for example, the "Coca-cola!" query would yield results, but not "!!!". No problem. As far as I know, the issue with the Streaming API is that you get access to only the most recent tweets.
You can create your own regular expression on the basis of your requirement and
then hit the twitter data to extract the specific tweets.  
 Could you talk more about "hitting the twitter data"? Are you talking about their API, some particular scraper etc?  Also, I would be grateful if you could provide screenshots of how this worked for you. Yes, check out the link for the API description   marcobonzanini.com/2015/03/02/… That does not seem to work for me. Can you run the code searching for "!!!" and post a screenshot if that worked for you?
I found this interesting ressource : https://webapps.stackexchange.com/questions/92196/search-for-tweets-with-special-characters
It basically says that certain characters cannot be searched because Twitter has blocked their use.
I believe what you should do is search through all the tweets within the range of a certain scope, and then use the string method find on the body message of each tweet. You would then stop when you would have reached a certain run-time or a specific amount of tweets found.
 As stated in the example in the question, I want to find all tweets containing the required string within a time frame of 2 years. That makes extracting ALL tweets infeasible because of the sheer number, even if there were a way to extract all tweets for that duration. You were wondering (if this even is doable).  And I don't think it is, and I provided some sources for it, along with another solution.
You can download and store data from Twitter API using various criteria (search for words in a dictionary, location search, popular Twitter accounts etc) It won't be the whole data for sure but you will have some part of it.
Then search these tweets locally.
These characters are also valid in url's so strip out the url's before searching.
Also don't forget to check whether storing data you got from Twitter is legal.
 Problem is, I have no way of knowing ALL or  the search terms (or even the most common ones) that co-occur with my required string.  Let's say I download all the tweets for the search term "apples" and then search locally among these tweets for "!!!". Sure, I'll get all tweets that have "apples" and "!!!", but what about all the other tweets on twitter that do not have "apples"?   A partial solution is not feasible in this case because there is no frequent itemset for my search query.Datamining open source software alternatives [closed] 









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 7 years ago.



I am evaluating datamining packages.
I have find these two so far:
RapidMiner
Weka
Do you have any experience to share with these two products, or any other product to recommend me?
Thanks

According to the yearly KDnuggets Polls 2007, 2008, and 2009, RapidMiner is the most widely used Open Source Data Mining Solution among data mining experts world-wide:
KDnuggets Data Mining Tool Poll 2009
RapidMiner is open source and 100% Java, RapidMiner is much more flexible and offers significantly more functionality than Weka and KNIME.
Regarding SVM implementations: Weka comes with one such implementation (LibSVM), while RapidMiner provides four SVM implementations (LibSVM, MySVM, EvoSVM, SMO-SVM), some of them with more advanced features.

Another alternative would be Orange. It includes various algorithms and data mining techniques that you can access either directly through Python scripts or through GUI.

Re-invent the wheel and code directly in R !
 +1 for Laziness, Impatience, and Hubris.
Pentaho is a nice suit for Business Intelligence. So maybe you would like to take a look at it. I have some experience in it, mainly for data warehousing and was quite happy.
 It seems like Pentaho is a major sponsor of Weka: weka.sf.net Didn't know that. Maybe I have to re-view Weka.
If you are interested in some Java code related to frequent pattern mining, association rules and sequential pattern mining,  I have a small open-source projects that has 42 algorithms related to these topics: http://www.philippe-fournier-viger.com/spmf/
However, please note that it does not provide any user interface. But it provides some very specialized algorithms that you will not find in other data mining packages.

I have used Weka in a high school course, and it had a nice SVM implementation. This was 4 or 5 years ago.

(KNIME ) is fairly extensive data mining platform.

According to the KDnuggets Poll 2011, RapidMiner once more is the most widely used data mining solution world-wide:
http://www.kdnuggets.com/2011/05/tools-used-analytics-data-mining.html

Have a look at ELKI, which is like WEKA except it is much much stronger on clustering and outlier detection, while WEKA essentially only does classification well.

As said before, Pentaho is a powerful Business Intelligence suite which WEKA belong to.
So I'd also recommand Weka, just for the sake that you have a great solution to extend your application and a great community also.
Ways to calculate similarity 
I am doing a community website that requires me to calculate the similarity between any two users. Each user is described with the following attributes:
age, skin type (oily, dry), hair type (long, short, medium), lifestyle (active outdoor lover, TV junky) and others.
Can anyone tell me how to go about this problem or point me to some resources?
 Related: stackoverflow.com/questions/3007790/finding-the-closest-match/…
Another way of computing (in R) all the pairwise dissimilarities (distances) between observations in the data set. The original variables may be of mixed types. The handling of nominal, ordinal, and (a)symmetric binary data is achieved by using the general dissimilarity coefficient of Gower (Gower, J. C. (1971) A general coefficient of similarity and some of its properties, Biometrics 27, 857–874). For more check out this on page 47. If x contains any columns of these data-types,  Gower's coefficient will be used as the metric.
For example
x1 <- factor(c(10, 12, 25, 14, 29))
x2 <- factor(c("oily", "dry", "dry", "dry", "oily"))
x3 <- factor(c("medium", "short", "medium", "medium", "long"))
x4 <- factor(c("active outdoor lover", "TV junky", "TV junky", "active outdoor lover", "TV junky"))
x <- cbind(x1,x2,x3,x4)

library(cluster)
daisy(x, metric = "euclidean")

you'll get :
Dissimilarities :
         1        2        3        4
2 2.000000                           
3 3.316625 2.236068                  
4 2.236068 1.732051 1.414214         
5 4.242641 3.741657 1.732051 2.645751

If you are interested on a method for dimensionality reduction for categorical data (also a way to arrange variables into homogeneous clusters) check this
 “Gower's distance” is chosen by metric "gower" or automatically if some columns of x are not numeric (?cluster::daisy) so you don't have to pass metric - here this is confusing.
Give each attribute an appropriate weight, and add the differences between values.
enum SkinType
    Dry, Medium, Oily

enum HairLength
    Bald, Short, Medium, Long

UserDifference(user1, user2)
    total := 0
    total += abs(user1.Age - user2.Age) * 0.1
    total += abs((int)user1.Skin - (int)user2.Skin) * 0.5
    total += abs((int)user1.Hair - (int)user2.Hair) * 0.8
    # etc...
    return total

If you really need similarity instead of difference, use 1 / UserDifference(a, b)

You probably should take a look for

Data Mining and Data Warehousing (Essential)
Machine Learning (Extra)
Artificial Neural Networks (Especially SOM)
Pattern Recognition (Related)

These topics will let you your program recognize similarities and clusters in your users collection and try to adapt to them...
You can then know different hidden common groups of related users... (i.e users with green hair usually do not like watching TV..)
As an advice, try to use ready implemented tools for this feature instead of implementing it yourself...  
Take a look at Open Directory Data Mining Projects

Three steps to achieve a simple subjective metric for difference between two datapoints that might work fine in your case:

Capture all your variables in a representative numeric variable, for example: skin type (oily=-1, dry=1), hair type (long=2, short=0, medium=1),lifestyle (active outdoor lover=1, TV junky=-1), age is a number.
Scale all numeric ranges so that they fit the relative importance you give them for indicating difference. For example: An age difference of 10 years is about as different as the difference between long and medium hair, and the difference between oily and dry skin. So 10 on the age scale is as different as 1 on the hair scale is as different as 2 on the skin scale, so scale the difference in age by 0.1, that in hair by 1 and  and that in skin by 0.5
Use an appropriate distance metric to combine the differences between two people on the various scales in one overal difference. The smaller this number, the more similar they are. I'd suggest simple quadratic difference as a first attempt at your distance function. 

Then the difference between two people could be calculated with (I assume Person.age, .skin, .hair, etc. have already gone through step 1 and are numeric):
double Difference(Person p1, Person p2) {

    double agescale=0.1;
    double skinscale=0.5;
    double hairscale=1;
    double lifestylescale=1;

    double agediff = (p1.age-p2.age)*agescale;
    double skindiff = (p1.skin-p2.skin)*skinscale;
    double hairdiff = (p1.hair-p2.hair)*hairscale;
    double lifestylediff = (p1.lifestyle-p2.lifestyle)*lifestylescale;

    double diff = sqrt(agediff^2 + skindiff^2 + hairdiff^2 + lifestylediff^2);
    return diff;
}

Note that diff in this example is not on a nice scale like (0..1). It's value can range from 0 (no difference) to something large (high difference). Also, this method is almost completely unscientific, it is just designed to quickly give you a working difference metric.

Look at algorithms for computing srting difference. Its very similar to what you need. Store your attributes as a bit string and compute the distance between the strings

You should read these two topics.
Most popular clustering algorithm k - means
And similarity matrix are essential in clustering
how to determine the number of topics for LDA? 
I am a freshman in LDA and I want to use it in my work. However, some problems appear. 
In order to get the best performance, I want to estimate the best topic number. After reading "Finding Scientific topics", I know that I can calculate logP(w|z) firstly and then use the harmonic mean of a series of P(w|z) to estimate P(w|T).
My question is what does the "a series of" mean? 

Unfortunately, there is no hard science yielding the correct answer to your question. To the best of my knowledge, hierarchical dirichlet process (HDP) is quite possibly the best way to arrive at the optimal number of topics.
If you are looking for deeper analyses, this paper on HDP reports the advantages of HDP in determining the number of groups.

A reliable way is to compute the topic coherence for different number of topics and choose the model that gives the highest topic coherence. But sometimes, the highest may not always fit the bill. 

See this topic modeling example.

First some people use harmonic mean for finding optimal no.of topics and i also tried but results are unsatisfactory.So as per my suggestion ,if you are using R ,then package"ldatuning" will be useful.It has four metrics for calculating optimal no.of parameters. Again perplexity and log-likelihood based V-fold cross validation are also very good option for best topic modeling.V-Fold cross validation are bit time consuming for large dataset.You can see "A heuristic approach to determine an appropriate no.of topics in topic modeling".
Important links:
https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4597325/

Let k = number of topics
There is no single best way and I am not even sure if there is any standard practices for this.
Method 1:
Try out different values of k, select the one that has the largest likelihood.
Method 2:
Instead of LDA, see if you can use HDP-LDA
Method 3:
If the HDP-LDA is infeasible on your corpus (because of corpus size), then take a uniform sample of your corpus and run HDP-LDA on that, take the value of k as given by HDP-LDA. For a small interval around this k, use Method 1.
How would you group/cluster these three areas in arrays in python? 
So you have an array
1
2
3
60
70
80
100
220
230
250

For a better understanding:

How would you group/cluster the three areas in arrays in python(v2.6), so you get three arrays in this case containing 

[1 2 3]     [60 70 80 100]     [220 230 250]

Background: 
y-axis is frequency, x-axis is number. These numbers are the ten highest amplitudes being represented by their frequencies. I want to create three discrete numbers from them for pattern recognition. There could be many more points but all of them are grouped by a relatively big frequency difference as you can see in this example between about 50 and about 0 and between about 100 and about 220. Note that what is big and what is small changes but the difference between clusters remains significant compared to the difference between elements of a group/cluster.
 This is not specifically a Python problem. You'd first have to choose an appropriate clustering-algorithm and see how you can implement in in Python (or if it is already implemented, for instance in SciPy). If the problem and dataset is always like this, you could use a "home made" heuristic yourself, and fine tune it to work on your data. But if the complexity would be a bit more than this, I think you cannot be spared of studying the many good suggestions and algorithms pointed down in the answers. It is not always 'like this'. Differences are: 1. more numbers. 2. different gaps between the clusters. 3. Different gaps between the elements in clusters. What remains though is that the difference between element gaps and cluster gaps is significant or in other words: Delta(elements) << Delta(cluster) In fact, stats.stackexchange.com would be a better place to ask, and there probaby are already a couple of duplicates there.
This is a simple algorithm implemented in python that check whether or not a value is too far (in terms of standard deviation) from the mean of a cluster:
from math import sqrt

def stat(lst):
    """Calculate mean and std deviation from the input list."""
    n = float(len(lst))
    mean = sum(lst) / n
    stdev = sqrt((sum(x*x for x in lst) / n) - (mean * mean)) 
    return mean, stdev

def parse(lst, n):
    cluster = []
    for i in lst:
        if len(cluster) <= 1:    # the first two values are going directly in
            cluster.append(i)
            continue

        mean,stdev = stat(cluster)
        if abs(mean - i) > n * stdev:    # check the "distance"
            yield cluster
            cluster[:] = []    # reset cluster to the empty list

        cluster.append(i)
    yield cluster           # yield the last cluster

This will return what you expect in your example with 5 < n < 9:
>>> array = [1, 2, 3, 60, 70, 80, 100, 220, 230, 250]
>>> for cluster in parse(array, 7):
...     print(cluster)
[1, 2, 3]
[60, 70, 80, 100]
[220, 230, 250]

 array = [1, 2, 3, 4, 60, 70, 80, 100, 220, 230, 250] makes the code divide into two arrays 1->3 and 4->250. @RichartBremer: The problem was that I tested that in python3, while in python2 sum(lst) / n with n integer gave as a result an integer so mean was 1 instead of 1.5. Converting len(lst) to float resolve the issue (I edited the code). This is probably the most sensible of the methods proposed so far (e.g. run kmeans on range(1,15)). However, you still should spend some thoughts on what you want to achieve. There are many methods that will produce such a split of the array; which one is appropriate depends a lot on what you are using it for and what your real data looks like. +1 for this answer, for not just using kmeans because it is clustering, but actually considering the problem. I is been a long time since last post but can you think of this code using dictionary inside a dictionary instead of array=[1,2,3...] @RikPoggi would you mind taking a look at my question, which uses you code here: stackoverflow.com/questions/18721774/…
Observe that your data points are actually one-dimensional if x just represents an index. You can cluster your points using Scipy's cluster.vq module, which implements the k-means algorithm.
>>> import numpy as np
>>> from scipy.cluster.vq import kmeans, vq
>>> y = np.array([1,2,3,60,70,80,100,220,230,250])
>>> codebook, _ = kmeans(y, 3)  # three clusters
>>> cluster_indices, _ = vq(y, codebook)
>>> cluster_indices
array([1, 1, 1, 0, 0, 0, 0, 2, 2, 2])

The result means: the first three points form cluster 1 (an arbitrary label), the next four form cluster 0 and the last three form cluster 2. Grouping the original points according to the indices is left as an exercise for the reader.
For more clustering algorithms in Python, check out scikit-learn.
 I don't like the phrase "is left as an exercise for the reader." due to it's arrogance. @RichartBremer: the phrase just indicates that I'm too busy/lazy to solve the list-processing drudgery and I trust that you can solve that yourself. It would also distract from the core of the answer. I don't see what's arrogant about it, I certainly didn't mean to be arrogant. Ok, it seems to be a simple misunderstanding. Try this on the data set array=list(range(15)) if you find the result very convincing. K-means is NOT a good choice for one-dimensional data, in particular when you do not know k. In fact, the only good thing to say about k-means is that it is very simple to implement. I'm not trying to make your answer appear bad. k-means doesn't make that much sense on 1-dimensional data, but my points are that a) k-means makes a lot of implicit assumptions on the data: clusters being equal in numerical size and k being known; b) clustering is not just about grouping objects, but actually about grouping objects in a way that makes sense for the particular task to solve, and this cannot be answered by the algorithm, but by the domain expert.
I assume you want a pretty-good-but-simple algorithim here.  
If you know you want N clusters, then you can take the differences (deltas) between consecutive members of the (sorted) input list.  E.g. in numpy:
 deltas = diff( sorted(input) )

Then you can place your cuttoffs where you find the N-2 biggest differences.   
Things are trickier if you don't know what N is.  Here you might place the cuttoffs whenever you see a delta greater than a certain size.  This will then be a hand-tuned parameter, which is not great, but might be good enough for you.

You can solve this in various ways. One of the obvious ones when you throw the keyword "clustering" is to use kmeans (see other replies).
However, you might want to first understand more closely what you are actually doing or attempting to do. Instead of just throwing a random function on your data.
As far as I can tell from your question, you have a number of 1-dimensional values, and you want to separate them into an unknown number of groups, right? Well, k-means might do the trick, but in fact, you could just look for the k largest differences in your data set then.
I.e. for any index i > 0, compute k[i] - k[i-1], and choose the k indexes where this is larger than for the rest. Most likely, your result will actually be better and faster than using k-means.
In python code:
k = 2
a = [1, 2, 3, 60, 70, 80, 100, 220, 230, 250]
a.sort()
b=[] # A *heap* would be faster
for i in range(1, len(a)):
  b.append( (a[i]-a[i-1], i) )
b.sort()
# b now is [... (20, 6), (20, 9), (57, 3), (120, 7)]
# and the last ones are the best split points.
b = map(lambda p: p[1], b[-k:])
b.sort()
# b now is: [3, 7]
b.insert(0, 0)
b.append(len(a) + 1)
for i in range(1, len(b)):
  print a[b[i-1]:b[i]],
# Prints [1, 2, 3] [60, 70, 80, 100] [220, 230, 250]

(This can btw. be seen as a simple single-link clustering!)
A more advanced method, that actually gets rid of the parameter k, computes the mean and standard deviation of b[*][1], and splits whereever the value is larger than say mean+2*stddev. Still this is a rather crude heuristic. Another option would be to actually assume a value distribution such as k normal distributions, and then use e.g. Levenberg-Marquardt to fit the distributions to your data.
But is that really what you want to do?
First try to define what should be a cluster, and what not. The second part is much more important.
 I think my definition is exhaustive. If not be specific what you are missing. insert() takes exactly 2 arguments (1 given) @RichartBremer: I think he/she meant b.insert(0,0) because b holds the "breaker" indexes so it also needs the first one (0) to start.
You could use nearest neighbor clustering. For a point to belong to one of the clusters, its nearest neighbor must also belong to the cluster. With the case you've shown, you'd just need to iterate along the x-axis and compare the differences to the adjacent points. When the difference to the previous point is greater than the difference to the next point, it indicates the start of a new cluster.  
Clustering cosine similarity matrix 
A few questions on stackoverflow mention this problem, but I haven't found a concrete solution.
I have a square matrix which consists of cosine similarities (values between 0 and 1), for example:
  |  A  |  B  |  C  |  D
A | 1.0 | 0.1 | 0.6 |  0.4
B | 0.1 | 1.0 | 0.1 |  0.2
C | 0.6 | 0.1 | 1.0 |  0.7
D | 0.4 | 0.2 | 0.7 |  1.0

The square matrix can be of any size. I want to get clusters (I don't know how many) which maximize the values between the elements in the cluster. I.e. for the above example I should get two clusters:

B
A, C, D

The reason being because C & D have the highest value between them, and A & C also have the highest value between them.
An item can be in only one cluster.
Recall is not that important for this problem, but precision is very important. It is acceptable to output three clusters: 1) B, 2) A, 3) C, D . But it is not acceptable to output any solution where B is in a cluster with another element.
I think the diagonal (1.0) is confusing me. My data is guaranteed to have at least one cluster of 2+ elements, and I want to find as many clusters as possible without sacrificing precision.
I will have to implement this in Python.
 Have you tried hierarchical clustering? This sounds exactly what you are attempting, hierarchical agglomerative clustering.
You can easily do this using spectral clustering. You can use the ready implementations such as the one in sklearn or implement it yourself. It is rather an easy algorithm.
Here is a piece of code doing it in python using sklearn:
import numpy as np
from sklearn.cluster import SpectralClustering
mat = np.matrix([[1.,.1,.6,.4],[.1,1.,.1,.2],[.6,.1,1.,.7],[.4,.2,.7,1.]])
SpectralClustering(2).fit_predict(mat)
>>> array([0, 1, 0, 0], dtype=int32)

As you can see it returns the clustering you have mentioned.
The algorithm takes the top k eigenvectors of the input matrix corresponding to the largest eigenvalues, then runs the k-mean algorithm on the new matrix. Here is a simple code that does this for your matrix:
from sklearn.cluster import KMeans
eigen_values, eigen_vectors = np.linalg.eigh(mat)
KMeans(n_clusters=2, init='k-means++').fit_predict(eigen_vectors[:, 2:4])
>>> array([0, 1, 0, 0], dtype=int32)

Note that the implementation of the algorithm in the sklearn library may differ from mine. The example I gave is the simplest way of doing it. There are some good tutorial available online describing the spectral clustering algorithm in depth.
For the cases you want the algorithm to figure out the number of clusters by itself, you can use Density Based Clustering Algorithms like DBSCAN:
from sklearn.cluster import DBSCAN
DBSCAN(min_samples=1).fit_predict(mat)
array([0, 1, 2, 2])

 Both the KMeans algorithm and SpectralClustering assume that the number of clusters is known. In my problem the number of clusters is not known, and cannot be estimated reliably. But thanks for pointing me to the sklearn clustering algorithms. I tried them all, Affinity Propagation gives the best results. I may try to optimize it or try to create a Python module for FLAME clustering: en.wikipedia.org/wiki/FLAME_clustering I see. you want to do the clustering without specifying the number of clusters. I will add one other example of such clustering algorithms to my answer now other than the Affinity Propagation your are using just in case. Also, could you tell me why the diagonal is confusing you? It is reasonable that the similarity of an element to itself be the maximum value 1. Thank you! This worked, although not as straightforward. DBSCAN assumes distance between items, while cosine similarity is the exact opposite. To make it work I had to convert my cosine similarity matrix to distances (i.e. subtract from 1.00). Then I had to tweak the eps parameter. It achieves OK results now. @Leo-T Just a small correction to your post concerning how spectral clustering works. It is stated that "The algorithm takes the top k eigenvectors of the input matrix corresponding to the largest eigenvalues ...", however, it does just the opposite. It returns the first k eigenvectors of the Laplacian corresponding to the k smallest eigenvalues.Use feedback or reinforcement in machine learning? 
I am trying to solve some classification problem. It seems many classical approaches follow a similar paradigm. That is, train a model with some training set and than use it to predict the class labels for new instances.
I am wondering if it is possible to introduce some feedback mechanism into the paradigm. In control theory, introducing a feedback loop is an effective way to improve system performance.
Currently a straight forward approach on my mind is, first we start with a initial set of instances and train a model with them. Then each time the model makes a wrong prediction, we add the wrong instance into the training set. This is different from blindly enlarge the training set because it is more targeting. This can be seen as some kind of negative feedback in the language of control theory.
Is there any research going on with the feedback approach? Could anyone shed some light?
 Look up boosting, this is basically what you are describing. Should it be migrated to stats.stackexchange.com? smwikipedia: I am after exactly the same problem. stackoverflow.com/questions/36068292/….      Would you like to share your findings ? @AnujGupta My question was inspired the negative feedback theory in control theory. This question has been a while but I didn't dig into it much due to project shift. I suggest you reading the replies below. Especially the one I granted bounty. Sorry for not being able to help you much.
There are two areas of research that spring to mind.
The first is Reinforcement Learning. This is an online learning paradigm that allows you to get feedback and update your policy (in this instance, your classifier) as you observe the results.
The second is active learning, where the classifier gets to select examples from a pool of unclassified examples to get labelled. The key is to have the classifier choose the examples for labelling which best improve its accuracy by choosing difficult examples under the current  classifier hypothesis.

I have used such feedback for every machine-learning project I worked on. It allows to train on less data (thus training is faster) than by selecting data randomly. The model accuracy is also improved faster than by using randomly selected training data. I'm working on  image processing (computer vision) data so one other type of selection I'm doing is to add clustered false (wrong) data instead of adding every single false data. This is because I assume I will always have some fails, so my definition for positive data is when it is clustered in the same area of the image.
 I do not think this will work for every machine learning method. The fact that training it on failed datapoints makes it better is not obvious (may be, it is starting to fail on all the points which are outside the set). Your experience is just one data point, please support it by some academic research references. Also mention which exact methods you used, since the behaviour could be strikingly different. Otherwise I find it doubtful as my experience tells me this works less often than it doesn't, but I'm just another data point... If every answer on SO had to be supported by academic research references there would only be a handful of accepted answers. I applied this technique to OCR, image similarity and pedestrian detection. I used GentleBoost. When working on images, the number of negative samples is almost infinite while the number of positive samples is quite limited, so using randomly selected data out of an infinity of possibilities is inefficient. The training time will be longer and the accuracy will be lower. Not every answer on SO should be backed by a reference. This should. Otherwise it is merely an opinion of one person. So, you used this technique for three projects, mention this in the answer, which exact problems it helped with. "every machine-learning project I worked on" is very broad. If you worked on three, that is very little data for such a claim. I worked on >20, but I simply don't know what the answer is because it is always different for me. Yes, I might be doing something wrong which is even better reason for you to specify what exactly you did so that the answer becomes useful. Also, "Is there any research going on with the feedback approach?" is in the question, so OP expects some references to literature on this topic. Mention how this method is called, give some links for further reading, etc. At the moment this answer is equivalent to "yes, it helped me a few times" which is rather a comment.
I saw this paper some time ago, which seems to be what you are looking for.
They are basically modeling classification problems as Markov decision processes and solving using the ACLA algorithm. The paper is much more detailed than what I could write here, but ultimately they are getting results that outperform the multilayer perceptron, so this looks like a pretty efficient method.
 it looks like the link to the paper is broken. Could you please provide a different working link, or a citation to the paper in text as a replacement? And the ACLA link that searches DeepDyve also does not have any useful results. Please do look into that as well. Thank you. The links are broken and the citation is always better or immutableWriting rules generated by Apriori 
I'm working with some large transactions data. I've been using read.transactions and apriori (parts of the arules package) to mine for frequent item pairings.
My problem is this: when rules are generated (using "inspect()") I can easily view them in the R console. Right now I'm manually copying the results into a text file, then saving and opening in excel. I'd like to just save the generated rules using write.csv, or something similar, but when I try, I receive an error that the data cannot be coerced into data.frame.
Does anyone have experience doing this successfully in R?

I know I'm answering my own question, but I found out that the solution is to use as() to convert the rules into a data frame. [I'm new to R, so I missed this my first time searching for a solution.] From there, it can easily be manipulated in any way you'd like (sub setting, sorting, exporting, etc.).
> mba = read.transactions(file="Book2.csv",rm.duplicates=FALSE, format="single", sep=",",cols=c(1,2));

> rules_1 <- apriori(mba,parameter = list(sup = 0.001, conf = 0.01, target="rules"));

> as(rules_1, "data.frame");


Another way to achieve that would be:
write(rules_1,
      file = "association_rules.csv",
      sep = ",",
      quote = TRUE,
      row.names = FALSE)

 This is a much better solution than the one above as the as function can lead to errors using Rscript. Furthermore, it uses the write function of the package.Best DataMining Database 
I am an occasional Python programer who only have worked so far with MYSQL or SQLITE databases. I am the computer person for everything in a small company and I have been started a new project where I think it is about time to try new databases. 
Sales department makes a CSV dump every week and I need to make a small scripting application that allow people form other departments mixing the information, mostly linking the records. I have all this solved, my problem is the speed, I am using just plain text files for all this and unsurprisingly it is very slow.
I thought about using mysql, but then I need installing mysql in every desktop, sqlite is easier, but it is very slow. I do not need a full relational database, just some way of play with big amounts of data in a decent time.
Update: I think I was not being very detailed about my database usage thus explaining my problem badly. I am working reading all the data ~900 Megas or more from a csv into a Python dictionary then working with it. My problem is storing and mostly reading the data quickly.
Many thanks!
 "I am using just plain text files for all this and unsurprisingly it is very slow."  That's shocking.  Flat files are the fastest possible way to process data.  What causes you to think your files are the culprit?  What measurements have you made?  Have you profiled? I did not have time to make tests yet, I will report here to improve the question and its solution when i do. What makes you say SQLite is very slow? At least as of 16 nov '13, I can say that SQLite is not at all slow.
Quick Summary

You need enough memory(RAM) to solve your problem efficiently. I think you should upgrade memory?? When reading the excellent High Scalability Blog you will notice that for big sites to solve there problem efficiently they store the complete problem set in memory.
You do need a central database solution. I don't think hand doing this with python dictionary's only will get the job done.
How to solve "your problem" depends on your "query's". What I would try to do first is put your data in elastic-search(see below) and query the database(see how it performs). I think this is the easiest way to tackle your problem. But as you can read below there are a lot of ways to tackle your problem.

We know:

You used python as your program language.
Your database is ~900MB (I think that's pretty large, but absolute manageable).
You have loaded all the data in a python dictionary. Here I am assume the problem lays. Python tries to store the dictionary(also python dictionary's aren't the most memory friendly) in your memory, but you don't have enough memory(How much memory do you have????). When that happens you are going to have a lot of Virtual Memory. When you attempt to read the dictionary you are constantly swapping data from you disc into memory. This swapping causes "Trashing". I am assuming that your computer does not have enough Ram. If true then I would first upgrade your memory with at least 2 Gigabytes extra RAM. When your problem set is able to fit in memory solving the problem is going to be a lot faster. I opened my computer architecture book where it(The memory hierarchy) says that main memory access time is about 40-80ns while disc memory access time is 5 ms. That is a BIG difference.

Missing information

Do you have a central server. You should use/have a server.
What kind of architecture does your server have? Linux/Unix/Windows/Mac OSX? In my opinion your server should have linux/Unix/Mac OSX architecture.
How much memory does your server have?
Could you specify your data set(CSV) a little better.
What kind of data mining are you doing? Do you need full-text-search capabilities? I am not assuming you are doing any complicated (SQL) query's. Performing that task with only python dictionary's will be a complicated problem. Could you formalize the query's that you would like to perform? For example:


"get all users who work for departement x"
"get all sales from user x"


Database needed

I am the computer person for
  everything in a small company and I
  have been started a new project where
  I think it is about time to try new
  databases.

You are sure right that you need a database to solve your problem. Doing that yourself only using python dictionary's is difficult. Especially when your problem set can't fit in memory.
MySQL

I thought about using mysql, but then
  I need installing mysql in every
  desktop, sqlite is easier, but it is
  very slow. I do not need a full
  relational database, just some way of
  play with big amounts of data in a
  decent time.

A centralized(Client-server architecture) database is exactly what you need to solve your problem. Let all the users access the database from 1 PC which you manage. You can use MySQL to solve your problem.
Tokyo Tyrant
You could also use Tokyo Tyrant to store all your data. Tokyo Tyrant is pretty fast and it does not have to be stored in RAM. It handles getting data a more efficient(instead of using python dictionary's). However if your problem can completely fit in Memory I think you should have look at Redis(below).
Redis:
You could for example use Redis(quick start in 5 minutes)(Redis is extremely fast) to store all sales in memory. Redis is extremely powerful and can do this kind of queries insanely fast. The only problem with Redis is that it has to fit completely in RAM, but I believe he is working on that(nightly build already supports it). Also like I already said previously solving your problem set completely from memory is how big sites solve there problem in a timely manner.
Document stores
This article tries to evaluate kv-stores with document stores like couchdb/riak/mongodb. These stores are better capable of searching(a little slower then KV stores), but aren't good at full-text-search.
Full-text-search
If you want to do full-text-search queries you could like at: 

elasticsearch(videos): When I saw the video demonstration of elasticsearch it looked pretty cool. You could try put(post simple json) your data in elasticsearch and see how fast it is. I am following elastissearch on github and the author is commiting a lot of new code to it.
solr(tutorial): A lot of big companies are using solr(github, digg) to power there search. They got a big boost going from MySQL full-text search to solr.

 First of all, thanks a lot Arthur for this elaborated answer. Your pointers to Tokyo Tyrant, Redis and to the document-store link were exactly what I was hoping for when I launched my question. However, I was not expecting be told MySQL can also do the job.  The part of the virtual memory and thrashing was something I knew was happening but I did not understand exactly at tech level, thanks a lot for the explanation!   My application is very simple and it works exactly as my users want, the problem is sometimes they need to use it with a big files and that made my question here. Answering your questions: - Most of the desktop computers have 4 GB RAM. - Mixed environment GNU/Linux and Windows. - very simple queries, it is mostly compare this and this column with this other and this other column of the second file. It would be pretty much:  + "GET row1 from DATABASE1" (gives a csv line)  + "GET row2 from DATABASE2" (gives a csv line)  + compare 2nd element from row1.dabatase1 with 3rd element from row1.database1,  + if the match, output the whole 2 rows to final report. - I do not need any full-text-search capability MySQL also is a pretty fast KV-store, but not as fast as redis and I with the limited search you have to perform you can do it. A lot of heavy users of MySQL say "“Normalization is for sissies.” Okay actually this is a quote from a blog post @code.flickr.com/blog/2010/02/08/….
You probably do need a full relational DBMS, if not right now, very soon.  If you start now while your problems and data are simple and straightforward then when they become complex and difficult you will have plenty of experience with at least one DBMS to help you.  You probably don't need MySQL on all desktops, you might install it on a server for example and feed data out over your network, but you perhaps need to provide more information about your requirements, toolset and equipment to get better suggestions.
And, while the other DBMSes have their strengths and weaknesses too, there's nothing wrong with MySQL for large and complex databases.  I don't know enough about SQLite to comment knowledgeably about it.
EDIT: @Eric from your comments to my answer and the other answers I form even more strongly the view that it is time you moved to a database.  I'm not surprised that trying to do database operations on a 900MB Python dictionary is slow.  I think you have to first convince yourself, then your management, that you have reached the limits of what your current toolset can cope with, and that future developments are threatened unless you rethink matters.
If your network really can't support a server-based database than (a) you really need to make your network robust, reliable and performant enough for such a purpose, but (b) if that is not an option, or not an early option, you should be thinking along the lines of a central database server passing out digests/extracts/reports to other users, rather than simultaneous, full RDBMS working in a client-server configuration.
The problems you are currently experiencing are problems of not having the right tools for the job.  They are only going to get worse.  I wish I could suggest a magic way in which this is not the case, but I can't and I don't think anyone else will.
 Sadly, the server installation option is not available, the network via wifi is quite flaky here.  Thanks for your reply!
Have you done any bench marking to confirm that it is the text files that are slowing you down? If you haven't, there's a good chance that tweaking some other part of the code will speed things up so that it's fast enough.
 Fair question. In some moment, I need to read a 900Megas csv file that I need in a Python dictionary, it takes ages a lot of time (~hours) and hangs up the whole system.  Thanks for your answer! @Eric: That's not sensible.  It should read in minutes.  Can you please provide profiling data? @S.Lott: @jinka already told me in his|her answer to look at profiling. I have not idea about how to do it, but I am looking some documentation about it atm. Any tips welcome! Thanks!
It sounds like each department has their own feudal database, and this implies a lot of unnecessary redundancy and inefficiency.
Instead of transferring hundreds of megabytes to everyone across your network, why not keep your data in MySQL and have the departments upload their data to the database, where it can be normalized and accessible by everyone?
As your organization grows, having completely different departmental databases that are unaware of each other, and contain potentially redundant or conflicting data, is going to become very painful.
 Hi Chris,  "It sounds like each department has their own feudal database, and this implies a lot of unnecessary redundancy and inefficiency."  You exactly got it here, sadly I can not change this without upper management support. So I still have this problem to solve :) @Eric -- your comment on @Chris's answer makes it clear that you are looking for a technical solution to what is fundamentally an organisational or management problem.  Only broken dreams and bitter tears lie ahead. Dear @High Performance Mark: I am aware of the organisational problems, sadly I still need to get the best approach to this with the tools I have. @Eric, I feel your pain. A bad solution now is often better then the perfect solution never.
Does the machine this process runs on have sufficient memory and bandwidth to handle this efficiently?  Putting MySQL on a slow machine and recoding the tool to use MySQL rather than text files could potentially be far more costly than simply adding memory or upgrading the machine.

Here is a performance benchmark of different database suits ->
Database Speed Comparison
I'm not sure how objective the above comparison is though, seeing as it's hosted on sqlite.org. Sqlite only seems to be a bit slower when dropping tables, otherwise you shouldn't have any problems using it. Both sqlite and mysql seem to have their own strengths and weaknesses, in some tests the one is faster then the other, in other tests, the reverse is true.
If you've been experiencing lower then expected performance, perhaps it is not sqlite that is the causing this, have you done any profiling or otherwise to make sure nothing else is causing your program to misbehave?
EDIT: Updated with a link to a slightly more recent speed comparison.
 No I have not done any profiling, I am an occasional programmer, will look at it and at the database speed comparison.  Thanks for your answer! That comparison page is useless. They even start off with the disclaimer "The numbers here are old enough to be nearly meaningless"
It has been a couple of months since I posted this question and I wanted to let you all know how I solved this problem. I am using Berkeley DB with the module bsddb instead loading all the data in a Python dictionary. I am not fully happy, but my users are.
My next step is trying to get a shared server with redis, but unless users starts complaining about speed, I doubt I will get it.
Many thanks everybody who helped here, and I hope this question and answers are useful to somebody else.
 Thanks for giving us feedback. If your users are happy you should be happy I guess  :)
If you have that problem with a CSV file, maybe you can just pickle the dictionary and generate a pickle "binary" file with pickle.HIGHEST_PROTOCOL option. It can be faster to read and you get a smaller file. You can load the CSV file once and then generate the pickled file, allowing faster load in next accesses.
Anyway, with 900 Mb of information, you're going to deal with some time loading it in memory. Another approach is not loading it on one step on memory, but load only the information when needed, maybe making different files by date, or any other category (company, type, etc..)
 thanks! It can be an interesting workaroud easy to do while I get a better solution! I'm not sure this is such a good idea. Correct me if I'm wrong, but Pickle isn't a streaming format, so in order to generate a Pickle file, you'd first have to load all 900 megs into Python's memory before saving the Pickle file. Instead, I'd recommend making a CSV dump, which is natively supported by the mysqldump tool and the "SELECT * INTO OUTFILE" query syntax, and then running this output through gzip to compress it. I liked from this idea that none of my desktop users would dare trying to open the 900Megas CSV file with Excel anymore (They love trying this!) It was easy and quick to test, but I did not get any improvement.
Take a look at mongodb.
 Thanks was the kind of answer I was waiting for, however it would be nice if could you elaborate why mongodb is a good idea?What are data requirements for FP-Growth in Weka? 
I'd like to use FP-Growth association rule algorithm on my dataset (model) in Weka. 
Unfortunately, this algorithm is greyed out. What are preconditions I have to meet in order to make use of it?
 Why people are downgrading? C'mon, at least leave a comment...
The answer/solution:

Each algorithm that Weka implements has some sort of a summary info associated with it. In order to see it from the GUI, one has to click on algorithm (or filter) options and then click once more on Capabilities button. Then a small popup will show up containing some info regarding particular algorithm.
In case of FPGrowth - model attributes needs to be of binary type. In my case I had a mix od nominal and numeric parameters. I had to apply NominalToBinary filter which converted my nominal attributes to binary values. Then I had to apply flter NumericToBinary with selected option ignoreClass set to true.

This has helped me to "unlock" FPGrowth in Weka.
 Here's a slight longer explanation: "FP-Growth algorithm works for boolean values only. Hence, the attributes of the dataset can have only true or false values. If you are using different type of attributes (numeric, string etc.), it looks disabled." weka.8497.n7.nabble.com/FP-GROWTH-Algorithm-td23554.html
Adding to @ŁukaszBachman answer: 
You need to set class to "No Class" before applying filter operation. If you are using weka java api, then you need to add data.setClassIndex(-1) to your java code.
For example: To perform Nominal To Binary in Java: 
        NominalToBinary nn = new NominalToBinary();
        nn.setInputFormat(Data);
        Data.setClassIndex(-1);
        Data = Filter.useFilter(Data, nn);

java framework for image pattern recognition? 
I'm looking for a Java framework to help with some data mining specific to images.  We have a set of historical images that I would like to categorize and classify.  I'm was hoping to find something like weka http://www.cs.waikato.ac.nz/ml/weka/ or Marsyas http://marsyas.sness.net but more specific to sifting through image data to find patterns.  Any suggestions?

What about using the OpenCV  library for Processing?  Technically, Processing is not Java, but it runs on the JVM and shouldn't be difficult to get working.  OpenCV is the standard choice for computer vision, which is what you're trying to do.

I played around with Neuroph (http://neuroph.sourceforge.net) which is a neural network framework for Java, and they have a very nice image recognition tutorial that I think everybody should read, but specially if you're dealing with image recognition: http://neuroph.sourceforge.net/image_recognition.html

Have a look at OpenCV with JavaCV libraries from Google. Simply use function from that library to find whatever patterns you want.
 from 2.4.4 onwards, there's even native wrappers for java, so javacv might get superseeded/deprecated by that
It'll be a great surprise for me if you can find such library. 
I can advice you to find some library with ART neural network implementation. Then you can add some photos to each category you want to recognize and ART algorithm will try to recognize all other photos.
Load MIT-BIH Arrhythmia ECG database onto MATLAB 
I am working on ECG signal processing using neural network which involves pattern recognition. 
As I need to collect all the data from Matlab to use it as test signal, I am finding it difficult to load it on to the Matlab.
I am using MIT Arrhythmia database here.
The signal needs to be indexed and stored as data structure in Matlab compatible format.
Presently, the signal is in .atr and .dat format.
How can you load MIT-BIH Arrhythmia database onto Matlab?
 I removed you email address. It's better to put such info into your profile page rather than in your post as plain text, even if Gmail spam filtering does quite a good job. I don't know what exactly you want to detect on the ECG signal but in my opinion the MIT-BIH database has a poor quality and I'd probably recommend you to find a different one.  From historical reasons it is some kind of academic standard, but if you don't need to compare your results with previous publications I'd use a different one. For example PTB is a good one (2 minute strips of 500 different patients, 12 lead ECG) @Polda How can you describe PTB's quality? Can you confirm it to be AAMI standard? MIT-BIH is AAMI standard in ambulatory setting.
You can use physionet ATM to get .mat files which are easier to work with.
In the input part select the desired leads, length, database and sample.
In the toolbox select export as .mat:
 
Then download the '.mat' file,
 
In order to open the file in MATLAB, here is a sample code:
load ('100m.mat')          % the signal will be loaded to "val" matrix
val = (val - 1024)/200;    % you have to remove "base" and "gain"
ECGsignal = val(1,1:1000); % select the lead (Lead I)
Fs = 360;                  % sampling frequecy
t = (0:length(ECGsignal)-1)/Fs;  % time
plot(t,ECGsignal)

and you will get,

However, If you were to read annotation files for arrhythmia or QRS complexes that would be another problem. 
Edit
The base and gain come from the info file (second picture).
This file gives you various information regarding the ECG signal.

In the last sentence it says: To convert from raw units to the physical units shown above, subtract 'base' and divide by 'gain'.
 Can you please explain your "base" and "gain" point more thoroughly. I added my answer below where I show the difference between your and the method where you do not those conversions. Hi @Masi, thanks for your attention, as you want to download the .mat file, there is another file named .info in which it indicates: To convert from raw units to the physical units shown above, subtract 'base' and divide by 'gain'`. Can you, please, include the link, citation and reference where they exactly say something about base and gain. Through the ATM tool, I cannot extract any info file. There may be a case that they have depreciated the info file here. I think this is the source what are you recalling physionet.org/physiotools/matlab/rddata.m It does this base and gain removals in this code. I cannot find any other source for this thing. @Masi, on my second picture in the post it says: download these files: there is an .info file, please check that file.
You need the program rddata.m (MATLab script) from this website. The program can be found here. rddata.m is probably the only program you will need to read the ecg signals. I remember having used this program and database myself not too long ago.

So I read this answer 3 months ago and removed the base and gain. It turns out , i completely shifted my R-peaks in various directions, screwing up all my results. While I am not sure if doing this is necessary in matlab or not, DO NOT DO THIS if you are not preprocessing your signal in matlab. I was preprocessing my signal in python, and all I did to normalizae it was 
val = val/2047  % (2047 is the max volt range of signals)

and used butterworth filters to remove artifacts  (range 0.5hz-45hz)
CORRECTION
The cutoff i selected is 0.5 to 45 not 5-15 as I previously reported. This cutoff preserves the QRS for various arrhythmias without adding too much noise
# baseline correction and bandpass filter of signals 
lowpass = scipy.signal.butter(1, highfreq/(rate/2.0), 'low') 
highpass = scipy.signal.butter(1, lowfreq/(rate/2.0), 'high') 

# TODO: Could use an actual bandpass filter 
ecg_low = scipy.signal.filtfilt(*lowpass, x=ecg) 
ecg_band = scipy.signal.filtfilt(*highpass, x=ecg_low)

 Can you give your command about butterworth to remove artifacts in the range [0.5-45 Hz]`, please. I want to study it because the implementation varies in applications. I do not trust that the filtering is necessary here. If you were doing HRV analysis of the signals, should you use the normalization val=val/2047;? I am using sig=(sig-1024)/200; at the moment, and really worrying that this is wrong. Your val=val/2047 makes sense because max volt is 2^11 -1. I would like to understand why Rashid claims the other thing. I opened a new ticket about the thing in Silva's tree because I really need an authoritative answer about the confusion. github.com/ikarosilva/wfdb-app-toolbox/issues/119 Hi Sorry i just saw this today See the problem is that  if you are detecting R-peaks in your HRV analysis you don't wanna divide it by 200 because then each beat's R-peak position will be shifted and not synced. I used the scipy,signal lib in python for butterworth. I am pasting my function in the next comment for your reference
There is a tutorial for using matlab to read the data. tutorial for matlab user

install "The WFDB Toolbox for Matlab" from the link above. Add the folder of the toolbox to the path in matlab.
Download the ECG signal. Be sure to download '.atr', '.dat' and '.hea' together for the signal you are to deal with.
Command in matlab is as follows :

[tm,signal,Fs]=rdsamp( filename , 1 ) ; 
[ann,type]=rdann( filename , 'atr' ) ;

Note: for signal '101', its name is '101'. And you can check the detail information about rdsamp and rdann from the tutorial. 

 Here about how to download those files locally stackoverflow.com/a/36706214/54964 so use physionetdb("mitdb", 1); before running rdsamp(...) to reduce network load and speed up your system. I am studying dbpath because there seem to be changes in different trees. I want that many projects can use local files to speed up the system. Here, the ticket github.com/ikarosilva/wfdb-app-toolbox/issues/117 in Ikaro's tree.
just use it
A=input('Enter Variable: ','s');
load(A);
a=(val(1,:));
b=fir1(100,[0.1,0.25],'stop');
y2=filter(b,1,a);
figure;
plot(y2);

 When giving an answer it is preferable to give some explanation as to WHY your answer is the one.
Use ATM to extract .mat as described by Kamtal (now known Rashid). 
However, note that to see the .info file in some cases, you need to click the arrow

After I pushed this forward to developers here, we got improvements in the documentation here in Section 4. 

If they are all integers in the range [-2^N, 2^N-1 ] or [ 0, 2^N ], they are probably digital. Compare the values to see if they are in the expected physiological range of the signal you are analyzing. For example, if the header states that the signal is an ECG stored in milivolts, which typically has an amplitude of about 2mV, a signal of integers ranging from -32000 to 32000 probably isn't giving you the physical ECG in milivolts...
If they are not integers then they are physical. Once again you can quickly compare the values to see if they are in the expected physiological range of the signal you are analyzing.

0-9-10 wfdb - physical units

We say that signals are in 'physical units' when the values are used to represent the actual real life values as closely as possible, although obviously everything on the computer is digital and discrete rather than analogue and continuous. This includes our precious 64 bit double precision floating point values, but this is as close as we can get and already very close to the actual physical values, so we refer to them as 'physical'.
-
For example, if a 15 bit signal is collected via a capturing device, Physionet will likely store it as a 16 bit signal. Each 16 bit block stores an integer value between -2^15 and 2^15-1, and using the gain and offset stated in the header for each channel, the original physical signal can be mapped out for processing. 

The default units are now physical units where base and gain should be added stated in the header for each channel, so the physical signal can be mapped out for processing. 
% rawUnits
%       A 1x1 integer (default: 0). Returns tm and signal as vectors
%       according to the following values:
%               rawUnits=0 - Uses Java Native Interface to directly fetch  data, returning signal in physical units with double precision.
%               rawUnits=1 -returns tm ( millisecond precision only! ) and signal in physical units with 64 bit (double) floating point precision
%               rawUnits=2 -returns tm ( millisecond precision only! ) and signal in physical units with 32 bit (single) floating point  precision
%               rawUnits=3 -returns both tm and signal as 16 bit integers (short). Use Fs to convert tm to seconds.
%               rawUnits=4 -returns both tm and signal as 64 bit integers (long). Use Fs to convert tm to seconds.

rawUnits=1, rawUnits=2 use also physical units. 
rawUnits=3, rawUnits=4 use then again analog/digital units where you need to remove base and gain. 
If you use rawUnits=1 or rawUnits=2, you need to adjust for base and gain where base = 1024 and gain = 200
# Kamtal's method in considering base and gain
load('201m.mat');
val = (val - 1024)/200;    % you have to remove "base" and "gain"
ECGsignal = val(1,16:950); % select the lead (Lead I)

See the .info file below where you can get the base and gain. There is also the unit mV which suggests the values should be near 2 after the base-gain operations. 
<0-9-9 wfdb - analog/digital units so base and gain by default; now only rawUnits=3,4 for analog units
After selection ATM, you should be able to see the list where you can select .info file after the export as described in Kamtal's answer. 
The .info file instructs to remove so-called base and gain from the data before use
Source: record mitdb/201  Start: [00:02:10.000]
val has 2 rows (signals) and 3600 columns (samples/signal)
Duration:     0:10
Sampling frequency: 360 Hz  Sampling interval: 0.002777777778 sec
Row     Signal  Gain    Base    Units
1       MLII    200     1024    mV
2       V1      200     1024    mV

To convert from raw units to the physical units shown
above, subtract 'base' and divide by 'gain'.

Comparing wrong answers here! [Deprecated]
Kamtal (now called Rashid) answer is about the old wfdb system which used digital units without removal of base and gain
# Kamtal's method in considering base and gain
load('201m.mat');
val = (val - 1024)/200;    % you have to remove "base" and "gain"
ECGsignal = val(1,16:950); % select the lead (Lead I)

# Method without considering base and gain
load('201m.mat');
ECGsignal2 = val(1,16:950); 

# http://www.mathworks.com/matlabcentral/fileexchange/10502-image-overlay
imshow(imoverlay(ECGsignal, ECGsignal2, uint8([255,0,0])))

and you get the difference between my method and his method

Retrieving population density data 
I need to figure out whether not a given location is considered urban or rural. I take it that the best way to do this is by looking at the population density of the city/state or province/country combination.
The kicker is that we're using this for data mining. Generally, mapping APIs that could do this have a requirement that each request must be in response to a single user action. This doesn't fit that criteria...using a web service, we would be making hundreds of web service calls for any single user action. So I think we can't really use something like the Google Maps API.
The problem is, what is available? Are there any databases ready to download which I can use to retrieve this data, or web services that actually allow data mining? I am using PHP, though the programming language doesn't really matter. I'm sure if I can get the data, I can get it to work with PHP.
 How current does your data need to be and what County is it for? It doesn't have to be super current, I don't think towns go from rural to urban or back that quickly.  These would primarily be for United States addresses, although we do have some international users and it would be great to be able to look them up too. another thing is a lot of times census zones do not include, grade or water bodies. You'll have to subtract both areas to get an actual one blog.splitwise.com/2014/01/06/…
I don't know if there are any freely available ready-made databases providing this information out there.
You could download a DBpedia dataset, specifically the infobox dataset, and extract population/location data from that.
 Thanks! I think I'll use this one. I've downloaded the mapping properties dataset, grepped out the population density lines, and will write a Python script to parse out the excess stuff to squash the filesize. A lot of useful (and accessible) info in those files :)

Are there any databases ready to download which I can use to retrieve this data, or web services that actually allow data mining?

For the US:
You might want to take a look at the gridded 1 km population estimates for the conterminous United States by decade from 1930 - 2000. (some more info)

For the World:
It looks like you want something like the Gridded Population of the World, version 3 (GPWv3), and the Global Rural-Urban Mapping Project (GRUMP) datasets.
There's a stand alone SEDAC Map Client and data downloads (here's some urban rural estimates data in Excel format).
You can obtain population estimates within a defined region using the Population Estimation Service

Population Estimation Service Features:
The service is accessible through three standard protocols used by many online map tools and clients: the Open Geospatial Consortium (OGC) Web Processing Service (WPS) standard, a Representational State Transfer (REST) interface, and a Simple Object Access Protocol (SOAP) interface. Standards-based clients such as uDig are able to submit requests using the OGC WPS. Users of ArcGIS software from ESRI can submit requests through SOAP. The REST interface is intended for use with lightweight javascript clients.
The parametric statistics returned for each supplied polygon include the count (number of grid cells used in the analysis), minimum population count, maximum population count, range of population counts, mean population counts, and standard deviation of population counts.
  Two measures of data quality are included in the service results. The first measure reflects the precision of the input data and the second indicates when the requested polygons are too small in area compared with the underlying input data to produce reliable population statistics.
Access:
To access the Population Estimation Service, users need to work with an online map client or Geographic Information System (GIS) software package that supports spatial queries through one of the three supported protocols. The service interfaces are available at:
Web Processing Service (WPS)
http://sedac.ciesin.columbia.edu/wps/WebProcessingService?Request=GetCapabilities&Service=WPS 
REST/SOAP Services
     http://sedac.ciesin.columbia.edu/mapservices/arcgis/rest/services/sedac/GPW/GPServer


Once I was browsing Google using query "city filetype:sql" and found very interesting database containing about 4000 largest cities and other cool geographical data.
Have a look here: http://www.dbis.informatik.uni-goettingen.de/Mondial/

This is not free, but take a look at Rural Urban Suburban. At a minimum you will find a good brief explanation and what factors to consider.

This could be useful. It worked for me. http://www.geonames.org/export/wikipedia-webservice.html
What does dimensionality reduction mean? 
What does dimensionality reduction mean exactly?
I searched for its meaning, I just found that it means the transformation of raw data into a more useful form.  So what is the benefit of having data in useful form, I mean how can I use it in a practical life (application)?
 are you talking about 'dimensionality reduction'? Changed title to match OP's clarification. Perhaps you mean "Dimensional Analysis"?
Dimensionality Reduction is about converting data of very high dimensionality into data of much lower dimensionality such that each of the lower dimensions convey much more information.   
This is typically done while solving machine learning problems to get better features for a classification or regression task.  
Heres a contrived example - Suppose you have a list of 100 movies and 1000 people and for each person, you know whether they like or dislike each of the 100 movies. So for each instance (which in this case means each person) you have a binary vector of length 100 [position i is 0 if that person dislikes the i'th movie, 1 otherwise ].
You can perform your machine learning task on these vectors directly.. but instead you could decide upon 5 genres of movies and using the data you already have, figure out whether the person likes or dislikes the entire genre and, in this way reduce your data from a vector of size 100 into a vector of size 5 [position i is 1 if the person likes genre i]  
The vector of length 5 can be thought of as a good representative of the vector of length 100 because most people might be liking movies only in their preferred genres.
However its not going to be an exact representative because there might be cases where a person hates all movies of a genre except one. 
The point is, that the reduced vector conveys most of the information in the larger one while consuming a lot less space and being faster to compute with.
 Maybe worth noting that this is 'contrived' in part because the genres we're used to, are a bit different to the patterns that matrix decomposition will find. So instead of 'comedy', 'thriller', 'cartoon', we get the kinds of result seen in timelydevelopment.com/demos/NetflixPrize.aspx --- dimensions that don't come with an official label, but are something like a scale from "What a 10 year old boy would watch" to "What a liberal woman would watch", or (their first dimension) "Offbeat / Dark-Comedy" to "Mass-Market / 'Beniffer' Movies".
You're question is a little vague, but there's an interesting statistical technique that may be what you're thinking off called Principal Component Analysis which does something similar (and incidentally plotting the results from which was my first real world programming task)
It's a neat, but clever technique which is remarkably widely applicable.  I applied it to similarities between protein amino acid sequences, but I've seen it used for analysis everything from relationships between bacteria to malt whisky.
Consider a graph of some attributes of a collection of things where one has two independent variables - to analyse the relationship on these one obviously plots on two dimensions and you might see a scatter of points.  if you've three variable you can use a 3D graph, but after that one starts to run out of dimensions.
In PCA one might have dozens or even a hundred or more independent factors, all of which need to be plotted on perpendicular axis. Using PCA one does this, then analyses the resultant multidimensional graph to find the set of two or three axis within the graph which contain the largest amount of information.  For example the first Principal Coordinate will be a composite axis (i.e. at some angle through n-dimensional space) which has the most information when the points are plotted along it.  The second axis is perpendicular to this (remember this is n-dimensional space, so there's a lot of perpendiculars) which contains the second largest amount of information etc.
Plotting the resultant graph in 2D or 3D will typically give you a visualization of the data which contains a significant amount of the information in the original dataset.  It's usual for the technique to be considered valid to be looking for a representation that contains around 70% of the original data - enough to visualize relationships with some confidence that would otherwise not be apparent in the raw statistics.  Notice that the technique requires that all factors have the same weight, but given that it's an extremely widely applicable method that deserves to be more widely know and is available in most statistical packages (I did my work on an ICL 2700 in 1980 - which is about as powerful as an iPhone)

http://en.wikipedia.org/wiki/Dimension_reduction
maybe you have heard of PCA (principle component analysis), which is a Dimension reduction algorithm.
Others include LDA, matrix factorization based methods, etc.
Here's a simple example. You have a lot of text files and each file consists some words. There files can be classified into two categories. You want to visualize a file as a point in a 2D/3D space so that you can see the distribution clearly. So you need to do dimension reduction to transfer a file containing a lot of words into only 2 or 3 dimensions. 
 A very nice introduction to PCA with the "just right" background of Eigen Values/Vectors: cs.otago.ac.nz/cosc453/student_tutorials/…
The dimensionality of a measurement of something, is the number of numbers required to describe it. So for example the number of numbers needed to describe the location of a point in space will be 3 (x,y and z).
Now lets consider the location of a train along a long but winding track through the mountains. At first glance this may appear to be a 3 dimensional problem, requiring a longitude, latitude and height measurement to specify. But this 3 dimensions can be reduced to one if you just take the distance travelled along the track from the start instead.
If you were given the task of using a neural network or some statistical technique to predict how far a train could get given a certain quantity of fuel, then it will be far easier to work with the 1 dimensional data than the 3 dimensional version.

It's a technique of data mining. Its main benefit is that it allows you to produce a visual representation of many-dimensional data. The human brain is peerless at spotting and analyzing patterns in visual data, but can process a maximum of three dimensions (four if you use time, i.e. animated displays) - so any data with more than 3 dimensions needs to somehow compressed down to 3 (or 2, since plotting data in 3D can often be technically difficult).
BTW, a very simple form of dimensionality reduction is the use of color to represent an additional dimension, for example in heat maps.

Suppose you're building a database of information about a large collection of adult human beings. It's also going to be quite detailed. So we could say that the database is going to have large dimensions.
AAMOF each database record will actually include a measure of the person's IQ and shoe size. Now let's pretend that these two characteristics are quite highly correlated. Compared to IQs shoe sizes may be easy to measure and we want to populate the database with useful data as quickly as possible. One thing we could do would be to forge ahead and record shoe sizes for new database records, postponing the task of collecting IQ data for later. We would still be able to estimate IQs using shoe sizes because the two measures are correlated.
We would be using a very simple form of practical dimension reduction by leaving IQ out of records initially. Principal components analysis, various forms of factor analysis and other methods are extensions of this simple idea.
Download link for Ta Feng Grocery dataset [closed] 






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 2 years ago.




I am desperately trying to download the Ta-Feng grocery dataset for few days but appears that all links are broken. I needed for data mining / machine learning research for my msc thesis. I also have the Microsoft grocery database, the Belgian store and Supermarket.arff from Weka. However in the research they say Ta Feng is largest and most interesting from all public available data sets.
http://recsyswiki.com/wiki/Grocery_shopping_datasets
I will be super thankful for any help :) Cheers!

The person that down voted doesn't understand the difficulty to find this valuable piece of information for machine learning related to supermarket scenarios. It is the biggest publicly available dataset containing 4 months of shopping transactions of the Ta-Feng supermarket. I got it from Prof. Chun Nan who was very kind to send it to me because the servers of his previous institute in Taiwan were not supporting it anymore. Here is a link for everybody that needs it: https://sites.google.com/site/dataminingcourse2009/spring2016/annoucement2016/assignment3/D11-02.ZIP
 I also added small ruby script converting the file into the WEKA readable .arff file as well as the file itself Dear Dragan, first, thank you very much, this is very valuable information. I'm trying to make sense of the data, to understand what each field means, but if I open it on a text editor I can see the data itself but not the headers... would you have this information available as well? Thanks a lot!
If anyone who uses this "Ta Feng" data set will encounter a major problem when it comes to the column names. So I thought of sharing this. Hope this would help someone immensely.
It contains these files
D11: Transaction data collected in November, 2000
D12: Transaction data collected in December, 2000
D01: Transaction data collected in January, 2001
D02: Transaction data collected in February, 2001
Format of Transaction Data
First line: Column definition in Traditional Chinese
Second line and the rest: data columns separated by ";"
Column definition
Transaction date and time (time invalid and useless)
Customer ID
Age: 10 possible values, 
A <25,B 25-29,C 30-34,D 35-39,E 40-44,F 45-49,G 50-54,H 55-59,I 60-64,J >65
Residence Area: 8 possible values, A-F: zipcode area: 105,106,110,114,115,221,G: others, H: Unknown Distance to store, from the closest: 115,221,114,105,106,110
Product subclass
Product ID
Amount
Asset
Sales price
 this is really helpful, thanks a lot I am glad to hear that from you. Cheers. @CharlesChow
The dropbox link seems to be broken. You can still download the dataset at the following link:
https://sites.google.com/site/dataminingcourse2009/spring2016/annoucement2016/assignment3/D11-02.ZIP
Can k-means clustering do classification? 
I want to know whether the k-means clustering algorithm can do classification?
If I have done a simple k-means clustering .
Assume I have many data , I use k-means clusterings, then get 2 clusters A, B. and the centroid calculating method is Euclidean distance.
Cluster A at left side.
Cluster B at right side.
So, if I have one new data. What should I do?

Run k-means clustering algorithm again, and can get which cluster does the new data belong to?
Record the last centroid and use Euclidean distance to calculating to decide the new data belong to?
other method?


The simplest method of course is 2., assign each object to the closest centroid (technically, use sum-of-squares, not Euclidean distance; this is more correct for k-means, and saves you a sqrt computation).
Method 1. is fragile, as k-means may give you a completely different solution; in particular if it didn't fit your data well in the first place (e.g. too high dimensional, clusters of too different size, too many clusters, ...)
However, the following method may be even more reasonable:
3. Train an actual classifier.
Yes, you can use k-means to produce an initial partitioning, then assume that the k-means partitions could be reasonable classes (you really should validate this at some point though), and then continue as you would if the data would have been user-labeled.
I.e. run k-means, train a SVM on the resulting clusters. Then use SVM for classification.
k-NN classification, or even assigning each object to the nearest cluster center (option 1) can be seen as very simple classifiers. The latter is a 1NN classifier, "trained" on the cluster centroids only.
 Why is sum of squares more correct than Euclidean distance? Isn't the difference only the square root, as you have mentioned? The least squares optimum found by k-means is not the least non-squares solution. so to be formally consistent with the k-means optimization problem use the squares.
Yes, we can do classification.
I wouldn't say the algorithm itself (like #1) is particularly well-suited to classifying points, as incorporating data to be classified into your training data tends to be frowned upon (unless you have a real-time system, but I think elaborating on this would get a bit far from the point).
To classify a new point, simply calculate the Euclidean distance to each cluster centroid to determine the closest one, then classify it under that cluster.
There are data structures that allows you to more efficiently determine the closest centroid (like a kd-tree), but the above is the basic idea.
 so , is my option (2) right ? I need to record the last cluster centroid to determine the closet cluster the new data belong to ? If by recording the last cluster centroid, you mean you'll remember the most recent centroid for the each cluster, then yes, #2 is correct. You don't benefit much from a kd-tree, unless you have a very large k - you store the centroids only, not the full data set. @Anony-Mousse Not just if you have a very large k, but also just if you have many points to classify (assuming k is fairly large, of course).
If you've already done k-means clustering on your data to get two clusters, then you could use k Nearest Neighbors on the new data point to find out which class it belongs to.
 I agree with Dukeling. After k-means Clustering algorithm converges, it can be used for classification, with few labeled exemplars. After finding the closest centroid to the new point/sample to be classified, you only know which cluster it belongs to. Here you need a supervisory step to label each cluster. Suppose you label each cluster as C1,C2 and C3 for example. This requires few samples with known labels from these clusters. Once this is done, the closet cluster label becomes the label of the new point/sample and thus it is classified as C1, C2 or C3.
If you are doing real-time analysis where you want to recognize new conditions during use (or adapt to a changing system), then you can choose some radius around the centroids to decide whether a new point starts a new cluster or should be included in an existing one. (That's a common need in monitoring of plant data, for instance, where it may take years after installation before some operating conditions occur.)  If real-time monitoring is your case, check RTEFC or RTMAC, which are efficient, simple real-time variants of K-means. RTEFC in particular, which is non-iterative. See http://gregstanleyandassociates.com/whitepapers/BDAC/Clustering/clustering.htm
Yes, you can use that for classification.  If you've decided you have collected enough data for all possible cases, you can stop updating the clusters, and just classify new points based on the nearest centroid.  As in any real-time method, there will be sensitivity to outliers - e.g., a caused by sensor error or failure when using sensor data.  If you create new clusters, outliers could be considered legitimate if one purpose of the clustering is identify faults in the sensors, although that the most useful when you can do some labeling of clusters. 

Here another method:
I saw it on "The Elements of Statistical Learning". I'll change the notation a little bit. Let C be the number of classes and K the number of clusters. Now, follow these steps:

Apply K-means clustering to the training data in each class seperately, using K clusters per class.
Assign a class label to each of the C*K clusters.
Classify observation x to the class of the closest cluster.

It seems like a nice approach for classification that reduces data observations by using clusters.
 But how can we know the label of cluster?
You are confusing the concepts of clustering and classification. When you have labeled data, you already know how the data is clustered according to the labels and there is no point in clustering the data unless if you want to find out how well your features can discriminate the classes.
If you run the k-means algorithm to find the centroid of each class and then use the distances from the centroids to classify a new data point, you in fact implement a form of the linear discriminant analysis algorithm assuming the same multiple-of-identity covariance matrix for all classes.

After k-means Clustering algorithm converges, it can be used for classification, with few labeled exemplars/samples.
Trajectory Clustering: Which Clustering Method? 
As a newbie in Machine Learning, I have a set of trajectories that may be of different lengths. I wish to cluster them, because some of them are actually the same path and they just SEEM different due to the noise. 
In addition, not all of them are of the same lengths. So maybe although Trajectory A is not the same as Trajectory B, yet it is part of Trajectory B. I wish to present this property after the clustering as well.
I have only a bit knowledge of K-means Clustering and Fuzzy N-means Clustering. How may I choose between them two? Or should I adopt other methods?
Any method that takes the "belongness" into consideration? 
(e.g. After the clustering, I have 3 clusters A, B and C. One particular trajectory X belongs to cluster A. And a shorter trajectory Y, although is not clustered in A, is identified as part of trajectory B.)
=================== UPDATE ======================
The aforementioned trajectories are the pedestrians' trajectories. They can be either presented as a series of (x, y) points or a series of step vectors (length, direction). The presentation form is under my control.
 could you elaborate a bit more on what a trajectory is in your problem and how it is represented? @rano thanks for the advice. Please see the updated question.:) I guess your problem now falls into the Sequential Pattern Matching/Mining more than into the simple clustering one @rano     can kindly direct me to the right direction further?
It might be a little late but I am also working on the same problem.
I suggest you take a look at TRACLUS, an algorithm created by Jae-Gil Lee, Jiawei Han and Kyu-Young Wang, published on SIGMOD’07.
http://web.engr.illinois.edu/~hanj/pdf/sigmod07_jglee.pdf
This is so far the best approach I have seen for clustering trajectories because:

Can discover common sub-trajectories.
Focuses on Segments instead of points (so it filters out noise-outliers).
It works over trajectories of different length.

Basically is a 2 phase approach:

Phase one - Partition: Divide trajectories into segments, this is done using MDL Optimization with complexity of O(n) where n is the numbers of points in a given trajectory. Here the input is a set of trajectories and output is a set of segments.

Complexity: O(n) where n is number of points on a trajectory
Input: Set of trajectories.
Output: Set D of segments

Phase two - Group: This phase discovers the clusters using some version of density-based clustering like in DBSCAN. Input in this phase is the set of segments obtained from phase one and some parameters of what constitutes a neighborhood and the minimum amount of lines that can constitute a cluster. Output is a set of clusters. Clustering is done over segments. They define their own distance measure made of 3 components: Parallel distance, perpendicular distance and angular distance. This phase has a complexity of O(n log n) where n is the number of segments.

Complexity: O(n log n) where n is number of segments on set D
Input: Set D of segments, parameter E that sets neighborhood treshold and parameter MinLns that is the minimun number of lines.
Output: Set C of Cluster, that is a Cluster of segments (trajectories clustered).


Finally they calculate a for each cluster a representative trajectory, which is nothing else that a discovered common sub-trajectory in each cluster.
They have pretty cool examples and the paper is very well explained. Once again this is not my algorithm, so don't forget to cite them if you are doing research.
PS: I made some slides based on their work, just for educational purposes:
http://www.slideshare.net/ivansanchez1988/trajectory-clustering-traclus-algorithm
 Great answer, this is exactly what  ended up using (1 year ago, lol). Cool @SibbsGambling, Where your results close to what you expected. In case you are interested I can post the results of my attempt to approximate trajectory clustering when I finish my research. My plan is to use an approximation to reduce time complexity via Locality sensitive Hashing. Just in case someone is interested, this is my implementation of the traclus algorithm (in Java). Needs some refactoring and tuning but is working. github.com/ivansanchezvera/TrajectoryClustering
Every clustering algorithm needs a metric. You need to define distance between your samples. In your case simple Euclidean distance is not a good idea, especially if the trajectories can have different lengths.
If you define a metric, than you can use any clustering algorithm that allows for custom metric. Probably you do not know the correct number of clusters beforehand, then hierarchical clustering is a good option. K-means doesn't allow for custom metric, but there are modifications of K-means that do (like K-medoids)
The hard part is defining distance between two trajectories (time series). Common approach is DTW (Dynamic Time Warping). To improve performance you can approximate your trajectory by smaller amount of points (many algorithms for that).
 -1 : you cannot use K-means if you define a custom metric. K-means is well defined only for the eucliean metric. K-medoids or kernelized k-means can overcome this limitation but at the cost of additional computational resources. now the answer is inconsistent, simply remove the part saying that "If you define a metric, than every clustering algorithm is viable" instead of just adding clarification on the end. It will be much more valuable for the reader. Actually, no. There are clustering algorithms that do not need a metric. E.g. COOLCAT. It's just that those that can work with arbitrary distances (most do not need a metric, just a dissimilarity) are more flexible, if the user cares to choose a good distance for his problem. @Anony Mousse has a good point about making clear distinction between metric and dissimilarity. This can also be a case for clustering method usind pseudo-kernels (which do not induce the kernel-metric)  As well as clusterings based on some model/distribution fitting, where we do not care about any concept of distance between data, we rather measure probability of coming from some parametrized model (which is not just a point in the input space).
Neither will work. Because what is a proper mean here?
Have a look at distance based clustering methods, such as hierarchical clustering (for small data sets, but you probably don't have thousands of trajectories) and DBSCAN.
Then you only need to choose an appropriate distance function that allows e.g. differences in time and spatial resolution of trajectories.
Distance functions such as dynamic time warping (DTW) distance can accomodate this.
 You could use K-Means with DTW as a distance measure (to determine similarity) and calculate the centroid in each iteration using DBA, which is a new method to average different time series when DTW is used as a metric. cs.ucr.edu/~eamonn/ICDM_2014_DTW_average.pdf
This is good concept and having possibility for real-time applications. In my view, one can adopt any clustering but need to select appropriate dissimilarity measure, later need to think about computational complexity. 
Paper (http://link.springer.com/chapter/10.1007/978-81-8489-203-1_15) used Hausdorff and suggest the technique for reducing complexity, and paper (http://www.cit.iit.bas.bg/CIT_2015/v-15-2/4-5-TCMVS%20A-edited-md-Gotovop.pdf) described the use of "Trajectory Clustering Technique Based on Multi-View Similarity"
What is the difference between a Confusion Matrix and Contingency Table? 
I'm writting a piece of code to evaluate my Clustering Algorithm and I find that every kind of evaluation method needs the basic data from a m*n matrix like A = {aij} where aij is the number of data points that are members of class ci and elements of cluster kj.
But there appear to be two of this type of matrix in Introduction to Data Mining (Pang-Ning Tan et al.), one is the Confusion Matrix, the other is the Contingency Table. I do not fully understand the difference between the two. Which best describes the matrix I want to use?
 Just as a side note: I know from a different context that (in many settings, including the popular data sets that you run into everywhere) the geometrical configurations of points and their classes do not correlate all that well. I'm not sure that this gives you a good tool to measure the quality of a clustering, unless of course you make/can make the assumption that classes and point locations are well-correlated.
Wikipedia's definition:

In the field of artificial intelligence, a confusion matrix is a
  visualization tool typically used in supervised learning (in
  unsupervised learning it is typically called a matching matrix). Each
  column of the matrix represents the instances in a predicted class,
  while each row represents the instances in an actual class.

Confusion matrix should be clear, it basically tells how many actual results match the predicted results. For example, see this confusion matrix
                 predicted class
                        c1  -  c2
  Actual class   c1     15  -   3
                ___________________
                 c2     0   -   2

It tells that:

Column1, row 1 means that the classifier has predicted 15 items as belonging to class c1, and actually 15 items belong to class c1 (which is a correct prediction)
the second column row 1 tells that the classifier has predicted that 3 items belong to class c2, but they actually belong to class c1 (which is a wrong prediction)
Column 1 row 2 means that none of the items that actually belong to class c2 have been predicted to belong to class c1 (which is a wrong prediction)
Column 2 row 2 tells that 2 items that belong to class c2 have been predicted to belong to class c2 (which is a correct prediction)

Now see the formula of Accuracy and Error Rate from your book (Chapter 4, 4.2), and you should be able to clearly understand what is a confusion matrix. It is used to test the accuracy of a classifier using data with known results. The K-Fold method (also mentioned in the book) is one of the methods to calculate the accuracy of a classifier that has also been mentioned in your book.
Now, for Contingency table:
Wikipedia's definition:

In statistics, a contingency table (also referred to as cross
  tabulation or cross tab) is a type of table in a matrix format that
  displays the (multivariate) frequency distribution of the variables.
  It is often used to record and analyze the relation between two or
  more categorical variables.

In data mining, contingency tables are used to show what items appeared in a reading together, like in a transaction or in the shopping-cart of a sales analysis. For example (this is the example from the book you have mentioned):
       Coffee  !coffee
tea    150       50      200
!tea   650       150     800
       800       200    1000   

It tells that in 1000 responses (responses about do they like Coffee and tea or both or one of them, results of a survey):

150 people like both tea and coffee
50 people like tea but do not like coffee
650 people do not like tea but like coffee
150 people like neither tea nor coffee

Contingency tables are used to find the Support and Confidence of association rules, basically to evaluate association rules (read Chapter 6, 6.7.1).
Now the difference is that Confusion Matrix is used to evaluate the performance of a classifier, and it tells how accurate a classifier is in making predictions about classification, and contingency table is used to evaluate association rules.
Now after reading the answer, google a bit (always use google while you are reading your book), read what is in the book, see a few examples, and don't forget to solve a few exercises given in the book, and you should have a clear concept about both of them, and also what to use in a certain situation and why.
Hope this helps.

In short, contingency table is used to describe data. and confusion matrix is, as others have pointed out, often used when comparing two hypothesis. One can think of predicted vs actual classification/categorization as two hypothesis, with the ground truth being the null and the model output being the alternative.
An understandable clusterization 
I have a dataset. Each element of this set consists of numerical and categorical variables. Categorical variables are nominal and ordinal. 
There is some natural structure in this dataset. Commonly, experts clusterize datasets such as mine using their 'expert knowledge', but I want to automate this process of clusterization.
Most algorithms for clusterization use distance (Euclidean, Mahalanobdis and so on) between objects to group them in clusters. But it is hard to find some reasonable metrics for mixed data types, i.e. we can't find a distance between 'glass' and 'steel'. So I came to the conclusion that I have to use conditional probabilities P(feature = 'something' | Class) and some utility function that depends on them. It is reasonable for categorical variables, and it works fine with numeric variables assuming they are distributed normally.
So it became clear to me that algorithms like K-means will not produce good results. 
At this time I try to work with COBWEB algorithm, that fully matches my ideas of using conditional probabilities. But I faced another obsacles: results of clusterization are really hard to interpret, if not impossible. As a result I wanted to get something like a set of rules that describes each cluster (e.g. if feature1 = 'a' and feature2 in [30, 60], it is cluster1), like descision trees for classification.
So, my question is: 
Is there any existing clusterization algorithm that works with mixed data type and produces an understandable (and reasonable for humans) description of clusters.
Additional info:
As I understand my task is in the field of conceptual clustering. I can't define a similarity function as it was suggested (it as an ultimate goal of the whoal project), because of the field of study - it is very complicated and mercyless in terms of formalization. As far as I understand the most reasonable approach is the one used in COBWEB, but I'm not sure how to adapt it, so I can get an undestandable description of clusters.
Decision Tree
As it was suggested, I tried to train a decision tree on the clustering output, thus getting a description of clusters as a set of rules. But unfortunately interpretation of this rules is almost as hard as with the raw clustering output. First of only a few first levels of rules from the root node do make any sense: closer to the leaf - less sense we have. Secondly, these rules doesn't match any expert knowledge.
So, I came to the conclusion that clustering is a black-box, and it worth not trying to interpret its results.
Also
I had an interesting idea to modify a 'decision tree for regression' algorithm in a certain way: istead of calculating an intra-group variance calcualte a category utility function and use it as a split criterion. As a result we should have a decision tree with leafs-clusters and clusters description out of the box. But I haven't tried to do so, and I am not sure about accuracy and everything else.
 Why can't you use decision trees where class=cluster? I assume you already have some labeled examples you can use... @amit that's the point that I don't have labeled examples, and I don't have any existing classes. Ideally I want to achieve the following: input dataset -> clusterization algorithm -> clusters description, and when an expert looks at the description, he says: "Yes, that's it. I understand it, and I would do the same". Do you know the number of categories or member of categories a priori?  Without a distance metric there is really not a good way to determine how good your algorithm is. Sorry, I cannot give you much more hints than below. I believe training a decision tree is the best idea, as decision trees are one of the few methods around that really give you an explanation of their decision process. Related to Anony-Mousse's suggestion, random forests can be run in an "unsupervised" mode that leads to a clustering algorithm, and since it's tree based it side-steps the metric definition problem.
For most algorithms, you will need to define similarity. It doesn't need to be a proper distance function (e.g. satisfy triangle inequality).
K-means is particularly bad, because it also needs to compute means. So it's better to stay away from it if you cannot compute means, or are using a different distance function than Euclidean.
However, consider defining a distance function that captures your domain knowledge of similarity. It can be composed of other distance functions, say you use the harmonic mean of the Euclidean distance (maybe weighted with some scaling factor) and a categorial similarity function.
Once you have a decent similarity function, a whole bunch of algorithms will become available to you. e.g. DBSCAN (Wikipedia) or OPTICS (Wikipedia). ELKI may be of interest to you, they have a Tutorial on writing custom distance functions.
Interpretation is a separate thing. Unfortunately, few clustering algorithms will give you a human-readable interpretation of what they found. They may give you things such as a representative (e.g. the mean of a cluster in k-means), but little more. But of course you could next train a decision tree on the clustering output and try to interpret the decision tree learned from the clustering. Because the one really nice feature about decision trees, is that they are somewhat human understandable. But just like a Support Vector Machine will not give you an explanation, most (if not all) clustering algorithms will not do that either, sorry, unless you do this kind of post-processing. Plus, it will actually work with any clustering algorithm, which is a nice property if you want to compare multiple algorithms.
There was a related publication last year. It is a bit obscure and experimental (on a workshop at ECML-PKDD), and requires the data set to have a quite extensive ground truth in form of rankings. In the example, they used color similarity rankings and some labels. The key idea is to analyze the cluster and find the best explanation using the given ground truth(s). They were trying to use it to e.g. say "this cluster found is largely based on this particular shade of green, so it is not very interesting, but the other cluster cannot be explained very well, you need to investigate it closer - maybe the algorithm discovered something new here". But it was very experimental (Workshops are for work-in-progress type of research). You might be able to use this, by just using your features as ground truth. It should then detect if a cluster can be easily explained by things such as "attribute5 is approx. 0.4 with low variance". But it will not forcibly create such an explanation!

H.-P. Kriegel, E. Schubert, A. Zimek
Evaluation of Multiple Clustering Solutions 
In 2nd MultiClust Workshop: Discovering, Summarizing and Using Multiple Clusterings Held in Conjunction with ECML PKDD 2011. http://dme.rwth-aachen.de/en/MultiClust2011

 Please, look at the additional info to the question. You can do the bottom suggestion with COBWEB, too, I guess. Cluster, then learn a decision tree to get a description for the clusters. If you make a decision tree from the clustering algorithm - make sure you set minimum number of instances per leaf as 1, and no prunning - this will ensure the resulting tree is consistent with the clustering algorithm. Thanks for your reply! I will surely try the latter approach when I have enough time, and will update answer on any results.
A common approach to solve this type of clustering problem is to define a statistical model that captures relevant characteristics of your data. Cluster assignments can be derived by using a mixture model (as in the Gaussian Mixture Model) then finding the mixture component with the highest probability for a particular data point.
In your case, each example is a vector has both real and categorical components. A simple approach is to model each component of the vector separately.
I generated a small example dataset where each example is a vector of two dimensions. The first dimension is a normally distributed variable and the second is a choice of five categories (see graph):

There are a number of frameworks that are available to run monte carlo inference for statistical models. BUGS is probably the most popular (http://www.mrc-bsu.cam.ac.uk/bugs/). I created this model in Stan (http://mc-stan.org/), which uses a different sampling technique than BUGs and is more efficient for many problems:
data {
  int<lower=0>  N; //number of data points
  int<lower=0>  C; //number of categories

  real x[N]; // normally distributed component data
  int y[N];  // categorical component data
}
parameters {
  real<lower=0,upper=1> theta; // mixture probability
  real mu[2]; // means for the normal component
  simplex[C] phi[2]; // categorical distributions for the categorical component
}

transformed parameters {
  real log_theta;
  real log_one_minus_theta;
  vector[C] log_phi[2];
  vector[C] alpha;

  log_theta <- log(theta);
  log_one_minus_theta <- log(1.0 - theta);

  for( c in 1:C)
    alpha[c] <- .5;

  for( k in 1:2)
    for( c in 1:C)
        log_phi[k,c] <- log(phi[k,c]);
}
model {
  theta ~ uniform(0,1); // equivalently, ~ beta(1,1);
  for (k in 1:2){
    mu[k] ~ normal(0,10);
    phi[k] ~ dirichlet(alpha);
  }

  for (n in 1:N) {
    lp__ <- lp__ + log_sum_exp(log_theta + normal_log(x[n],mu[1],1) + log_phi[1,y[n]],
                               log_one_minus_theta + normal_log(x[n],mu[2],1) + log_phi[2,y[n]]);
  }
}

I compiled and ran the Stan model and used the parameters from the final sample to compute the probability of each datapoint under each mixture component. I then assigned each datapoint to the mixture component (cluster) with higher probability to recover the cluster assignments below:

Basically, the parameters for each mixture component will give you the core characteristics of each cluster if you have created a model appropriate for your dataset.
 The main problem here is not clusterization of data set, but acceptable interpratetion of given clusters. Of course clusterization algorithm can drastically change clusters structure, and suddenly this structure will become plain and simple. But it is too good to be truth. I haven't used BUGS or Stan, but if I understand it right, these are using a Gaussian Mixture Model, so the result of its work is similar to the one EM-algorithm produces. Is it so? You can extract an interpretation of each cluster from the distribution of each mixture component. In the GMM, each mixture is (multivariate) gaussian and can be described by a mean vector and covariance matrix. This description is probably not ideal for data with categorical components. So you probably want to define custom distributions over mixture components to do this - NOT a gaussian mixture. BUGS and Stan can automate this inference process for custom mixtures so you don't have to derive EM or sampling updates yourself.
For heterogenous, non-Euclidean data vectors as you describe, hierarchical clustering algorithms often work best.  The conditional probability condition you describe can be incorporated as an ordering of attributes used to perform cluster agglomeration or division.  The semantics of the resulting clusters are easy to describe.
Is there a good way to do this type of mining? 
I am trying to find points that are closest in space in X and Y directions (sample dataset given at the end) and am looking to see if there are smarter approaches to do this than my trivial (and untested) approach. The plot of these points in space looks something like the following and am trying to find sets of points marked inside the boxes i.e. the output I am looking for is a set of groups:
Group 1: (1,23), (2,23), (3,23)...
Group 2: (68,200), (68,201), (68,203), (68,204), (68,100), (68,101), (68,101)...


For the horizontal bands, I am thinking I could just go ahead and use small sliding windows of size say, 5 or 10 (which should really be determined from the global information of which size will give the maximum grouped points but I am still exploring a good approach) and search for contiguous points because a break would not be considered a horizontal band anymore. 
I am guessing the same approach works for the vertical bands as well but not in all cases because there is a subtle difference in horizontal and vertical bands: points should appear close to be considered a group horizontally but they can appear anywhere to be considered part of a vertical band. Observe the large vertical band in the figure. So I am guessing I could just look for points that have the same x-coordinate (in this case, x=68) should give me a lot of points. 
Other than this trivial solution, I can't think of anything smart that can be done here as this problem appears deceptively simple to me. Am I missing something here? Does this fall into some known class of problems and if so, is there a good and scalable approach to achieve this?
Sample Dataset:
1,23
1,23
2,23
3,23
4,23
5,23
6,23
7,23
8,23
9,23
10,23
11,23
12,23
13,23
14,23
15,23
16,23
10,33
11,33
12,33
13,33
14,33
15,33
16,33
17,33
18,33
19,33
2,28
2,28
3,28
34,75
34,76
34,76
34,77
34,78
34,79
34,80
34,81
34,82
34,83
34,75
34,76
34,76
34,77
34,78
34,79
34,80
34,81
400,28
400,28
400,28
68,200
68,201
68,203
68,204
68,100
68,101
68,103
68,104

 Clustering algorithm could be interesting ? You could try looking at the scipy.cluster module, though it looks like your system is a bit non-standard. Must the clusters be exclusively 1 row, 1 column, or are regions, e.g. (1,1), (1,2), (2,1), (2,2) allowed? Look at R-tree algorithm @FredL: Actually, regions are allowed too except that it would be hard in my case to find out exactly how big a region is. I was trying to first get one-dimensional bands before heading the direction you pointed out. @dziobas: Would you care to elaborate a bit on that? Do you mean, build an R-tree and then query it using all points to get spatially close neighbors?
This is a little late, but this problem has been worrying me for some time.  I
was sure it could be solved with mixed integer / linear programming techniques
and asked for help in this question: Identifying column and row clusters with linear programming
However, after getting a reply there, I had the insight that your problem, at
least as I understand it, is so simple (when framed as a constraint program)
that you can solve it trivially with a simple program (which you already
knew).  In other words, constraint programming would be a cool way to solve
this, but, at least with the approach I found, would give you the same answer
as something much simpler.
I'll explain below my reasoning, how I would implement it with a constraint
solving package, and then give the final, trivial, algorithm.
Mixed integer programming solution
The most important detail is the difference between horizontal and vertical
groups.  As far as i can see, anything that aligns vertically can be in the
same group.  But horizontal groups are different - components have to be close
together.
The hardest part of solving a problem with constraints seems to be finding a
way to describe the limits in a way that the solver can understand.  I won't
go into the details here, but solvers are frustratingly limited.  Luckily I
think there is a way to do this here, and it is to consider horizontal
neighbours:  if there are N points in a row then we have N-1 sets of
neighbours (for example, with 4 points A B C and D there are the three pairs
AB, BC, and CD).
For each pair, we can give a score, which is the number of spaces between them
(S_i) scaled by some factor K, and a flag (F_i) which is 0 or 1.  If the
pair are in the same horizontal group then we set the flag to 1, otherwise it
is zero.
It is critical to see that the set of flags for all the pairs completely
defines a solution.  We can run across any row, placing pairs with a flag
of 1 in the same horizontal group, and starting a new horizontal group each
time the flag is 0.  Then, we can take all horizontal groups of size 1 and
convert them into vertical groups: any point that is not in a horizontal group
must be in a vertical group (even if it is a vertical group of just one).
So all we need now is a way to express an optimal solution in terms of the
flags.  I suggest that we want to minimise:
sum(1 - F_i) + sum(K * S_i * F_i)

This has two terms.  The first is the sum of "one minus the flag" for each
pair.  The flag is 1 when the points are in the same horizontal group and 0
otherwise.  So minimising this value is the same as saying that we want as
few horizontal groups as possible.  If this was the only constraint then we
could set it to zero by making all the F_i 1 - by making all pairs on a row
members of the same group.
But the second term stops us from choosing such an extreme solution.  It
penalises groups with gaps.  If a pair are in the same group, but are
separated by S_i spaces, then we have a "penalty" of K * S_i.
So we have a trade-off.  We want horizontal groups, but we don't want gaps.
The final solution will depend on K - if it is large then we won't include
any spaces in horizontal groups.  But as it is decreased we will start to do
so, until when it is very small (tends to zero) we place everything in a row
in a single group.
To use this you would choose some K, calculate the S_i, and enter the expression above into a constraint system.  The system would then choose F_i to minimise the expression.  Finally you would convert the F_i into a pattern of groups by scanning each row as described above and then grouping singletons vertically.
Analytic solution
OK, cool.  At this point we have a way to express the problem that we can give
to a constraint engine.
But it's trivial to solve!  We don't need no stinkin' constraint engine to
solve this - we can just look at the expression:
sum(1 - F_i) + sum(K * S_i * F_i)

The two sums are over the same pairs, so we can move everything into the sum:
sum(1 - F_i + K * S_i * F_i)
sum(1 + F_i * (K * S_i - 1))

And then extract the constant (N here is the total number of pairs):
N + sum(F_i * (K * S_i - 1))

Now note that each term in the sum is independent (and additive).  So for each
term, we want the minimum value.  We have two options:

if F_i is 0 then the entire term is 0.  
otherwise, F_i is 1 and the term is K * S_i - 1.

So the best choice depends on whether K * S_i is greater than 1.  If K *
S_i is greater than 1 then the smallest value of the term is 0, and F_i
should be 0.  Otherwise the second choice above is negative, and F_i should
be one.
Trivial algorithm
What does this mean?  It means that for each pair we can simply look at the
number of spaces, S_i.  If that is greater than 1 / K then the two points
should be in separate groups.  Otherwise they should be in the same group.
So all this fancy maths and optimisation and constraints and bullshitting
comes down to: how far apart are two points in neighbouring pairs?  If they
are closer than some cut-off, put them in the same horizontal group.
Otherwise, put them in separate groups.
So here, finally, is your algorithm:
choose some cut-off value, X
place each point in its own, singleton, horizontal group
for each row with more than one point:
    for each neighbouring pair in the row:
        if the space between the pair is less than X:
            join into a single horizontal group
for each column:
    join any remaining singleton groups into a single vertical group

Conclusion

You can use constraint programming techniques to solve this problem, but such techniques are restricted to solutions that describe the system in the "correct" (typically, linear) way.
The simplest such approach I can find is equivalent to a trivial, direct
algorithm that divides points in a row into horizontal groups depending on
the number of spaces between them.
This all depends on a whole pile of assumptions about what you wanted which may, of course,
be over-simplifications, or just plain wrong.


You can try using cluster module. It contains implementation of K-means clustering algorithm. You can adjust argument of getclusters function to change the number of clusters you want. 
s = '''
1,23
1,23
2,23
...
68,101
68,103
68,104
'''

from cluster import *

ll = [tuple(map(int,each.split(','))) for each in s.split()]

#horizontal 
cl = HierarchicalClustering(ll, lambda x,y: abs(x[0]-y[0]))

for c in cl.getlevel(1):
    print c

#vertical
cl = HierarchicalClustering(ll, lambda x,y: abs(x[1]-y[1]))

for c in cl.getlevel(1):
    print c

 I don't think k-means clustering will return 1xn and nx1 clusters only which as I understand is OP's requirement. @Abhinav I don't think that is his requirement. But let me read the question again. Carefully this time! :) OP mentions horizontal bands and vertical bands so it looks like he is looking for a way to cluster in the points in bands only. @Abhinav I have updated the answer according to your suggestions. Thanks! :) Wonderful! As of now, it does give me what I want and have accepted it as a solution. I will provide an update if I find something else :) Thank you very much.Architecture for database analytics 
We have an architecture where we provide each customer Business Intelligence-like services for their website (internet merchant). Now, I need to analyze those data internally (for algorithmic improvement, performance tracking, etc...) and those are potentially quite heavy: we have up to millions of rows / customer / day, and I may want to know how many queries we had in the last month, weekly compared, etc... that is the order of billions entries if not more.
The way it is currently done is quite standard: daily scripts which scan the databases, and generate big CSV files. I don't like this solutions for several reasons:

as typical with those kinds of scripts, they fall into the write-once and never-touched-again category
tracking things in "real-time" is necessary (we have separate toolset to query the last few hours ATM).
this is slow and non-"agile"

Although I have some experience in dealing with huge datasets for scientific usage, I am a complete beginner as far as traditional RDBM go. It seems that using column-oriented database for analytics could be a solution (the analytics don't need most of the data we have in the app database), but I would like to know what other options are available for this kind of issues.

You will want to google Star Schema. The basic idea is to model a special data warehouse / OLAP instance of your existing OLTP system in a way that is optimized to provided the type of aggregations you describe. This instance will be comprised of facts and dimensions.
In the example below, sales 'facts' are modeled to provide analytics based on customer, store, product, time and other 'dimensions'. 

You will find Microsoft's Adventure Works sample databases instructive, in that they provide both the OLTP and OLAP schemas along with representative data. 
 thanks - would you have any suggestion for a good introduction (book ?) to this kind of stuff (I am more interested in principles than actual vendor specific solutions) the MS example is complete, and well documented, thus useful even if you are deploying on a different architecture. in terms of a good book, I have enjoyed Joe Celko's SQL books. Looks like he has one with an OLAP slant that contains relevant details. Reviews on amazon are a bit lumpy amazon.com/Celkos-Analytics-Kaufmann-Management-Systems/dp/… also, as an aside, I was curious what the web analytics project piwik used in terms of a schema; thinking it might also be a useful example. i was surprised to see that they are not doing any type of rollup / fact / dimension schema   see: dev.piwik.org/trac/browser/trunk/misc/db-schema.png
There are special db's for analytics like Greenplum, Aster data, Vertica, Netezza, Infobright and others. You can read about those db's on this site: http://www.dbms2.com/ 

The canonical handbook on Star-Schema style data warehouses is Raplh Kimball's "The Data Warehouse Toolkit" (there's also the "Clickstream Data Warehousing" in the same series, but this is from 2002 I think, and somewhat dated, I think that if there's a new version of the Kimball book it might serve you better. If you google for "web analytics data warehouse" there are a bunch of sample schema available to download & study.
On the other hand, a lot of the no-sql that happens in real life is based around mining clickstream data, so it might be worth see what the Hadoop/Cassandra/[latest-cool-thing] community has in the way of case studies to see if your use case matches well with what they can do.
Python tools for out-of-core computation/data mining 
I am interested in python mining data sets too big to sit in RAM but sitting within a single HD. 
I understand that I can export the data as hdf5 files, using pytables. Also the numexpr allows for some basic out-of-core computation.
What would come next? Mini-batching when possible, and relying on linear algebra results to decompose the computation when mini-batching cannot be used?
Or are there some higher level tools I have missed?
Thanks for insights,

In sklearn 0.14 (to be released in the coming days) there is a full-fledged example of out-of-core classification of text documents.
I think it could be a great example to start with :
http://scikit-learn.org/dev/auto_examples/applications/plot_out_of_core_classification.html
In the next release we'll extend this example with more classifiers and add documentation in the user guide.
NB: you can reproduce this example with 0.13 too, all the building blocks were already there.

What exactly do you want to do — can you give an example or two please ?
numpy.memmap is easy —

Create a memory-map to an array stored in a binary file on disk.
  Memory-mapped files are used for accessing small segments of large
  files on disk, without reading the entire file into memory.  Numpy's
  memmap's are array-like objects ...

see also numpy+memmap on SO.  
The scikit-learn people are very knowledgeable, but prefer specific questions.
 Thanks for answer Denis. It appears skilearn has mini-batching facilities. Actually I am looking for the most rational way to deal with out-of-the-core learning of a sub map-reduce size. Particularly I am striving to understand the relative strengths of hdf5, sql, nosql. Zelazny7's large-data-work-flows question is better because concrete, and gets better answers
I have a similar need to work on sub map-reduce sized datasets.  I posed this question on SO when I started to investigate python pandas as a serious alternative to SAS: "Large data" work flows using pandas
The answer presented there suggests using the HDF5 interface from pandas to store pandas data structures directly on disk.  Once stored, you could access the data in batches and train a model incrementally.  For, example, scikit-learn has several classes that can be trained on incremental pieces of a dataset.  One such example is found here:
http://scikit-learn.org/0.13/modules/generated/sklearn.linear_model.SGDClassifier.html
Any class that implements the partial_fit method can be trained incrementally.  I am still trying to get a viable workflow for these kinds of problems and would be interested in discussing possible solutions. 
Find substring in text which has the highest similarity to a given keyword 
Say I have this text = I love apples, kiwis, oranges and bananas and the searchString = kiwis and bananas and a similarity algorithm say Jaccard index. How can I efficiently find the substring in text which has the highest similarity to searchString.
Basically I am trying to find portions of text (text has high errors, misspellings, extra symbols and spaces) which match a list of keywords I have.
 i dont know much about this but this link might help ...    stackoverflow.com/questions/5859561/… en.wikipedia.org/wiki/… @Dandy: I know about edit distance. This question is asking given strings S and T, find a substring of S that has the smallest edit distance (or any other similarity metric) to T. web.stanford.edu/class/cs124/lec/med.pdf @Dandy: Thanks for the link but I know the min edit distance problem. Not sure how it applied to my question? Can you provide an answer below detailing what you are trying to say?
Jaccard index is "lucky" similarity algorithm, because you can update it's value for new symbol without recalculating all previous stuff. So, you can view text as a sequence of diffs for resulting index value. After that, problem can be reduced to https://en.wikipedia.org/wiki/Maximum_subarray_problem.
What about your second paragraph, if you are doing some NLP-like research, I'd suggest to clean your data (remove those extra symbols and spaces, whenever that's possible) before further processing. That's known as "spelling correction", and there are tons of different algorithms and libraries. To choose appropriate one, extra information about your domain is needed.
 > Jaccard index is "lucky" similarity algorithm, because you can update it's value for new symbol without recalculating all previous stuff.  Can you explain or provide a link regarding what you mean above? Assume sets are A = {1, 2, 3} and B = {1}, JI is 1/3. If you add new value to B,  you can simply update numerator and denominator -- B += {2}, JI becomes (1 + 1) / (3 + 0) = 2/3, values 1 and 0 can be calculated without rest of B. As long as we work with sets, contains operation is pretty efficient. (This example can be easily extended to strings) (For previous comment -- to check if, for example, 1 already was calculated in numerator, we need additional check, that can be done in amortized constant time too (however, required max space is space of A). Actually interesting question -- what would be more efficient, to find diffs for searchString and check contains for text, or visa verse. I bet that doesn't matter, and going to write proof.
Take a look at shingling technique, and try to find the similarity.
you can follow this link: http://nlp.stanford.edu/IR-book/html/htmledition/near-duplicates-and-shingling-1.html
For example use 9 shingle and compare each subset with your specific keyword
 I believe the algorithm I am using (Jacard index) is based on k-shingling anyway
I Use Stemming
and Levenshtein distance
This is the algorithm in action: https://wizsearch.wizsoft.com/index.php/demo/
This demo searches all wiki titles, try the "show search terms" option to see the Levenshtein distance and error correction algorithm in action.

Every query term is checked against a dictionary.
If a term is not found in the dictionary, then those words from the dictionary are shown as spelling suggestions, which are most similar to the query term in question.
Similarity / Edit distance
As similarity measure between two words serves usually the Damerau-Levenshtein distance https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance
Few other references

http://www.joyofdata.de/blog/comparison-of-string-distance-algorithms/
http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/

NLP and Machine learning for sentiment analysis [closed] 






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 2 years ago.




I'm trying to write a program that takes text(article) as input and outputs the polarity of this text, weather its a positive or a negative sentiment. I've read extensively about different approaches but i am still confused. I read about many techniques like classifiers and machine learning. I would like direction and clear instructions on where to start. For example, i have a classifier which requires a dataset but how do i convert the text(article) into a dataset for the classifier. If anyone can tell me the logical sequence to approach this problem that would be greet. Thanks in advance!
PS: please mention any related algorithms or open-source implementation
Regards,
Mike
 There are myriad choices for turning text into classifier input, depending o.a. factors on the ML framework used. Read up on ML first.
If you're using Python, I'd suggest you have a look at NLTK and the NLTK book.
This blog: streamhacker.com has some very good articles to get you started.
There's been lots of research in this area in the since the late 2000's.
UPDATE (Oct 2013):
Stanford researches made a breakthrough in sentiment analysis that has achieved more than 85% accuracy on average. (http://gigaom.com/2013/10/03/stanford-researchers-to-open-source-model-they-say-has-nailed-sentiment-analysis/)
 start with NLTK.  NLP and machine learning are not trivial and it's good to work through some examples. the solution you'll use is probably some word-based sentiment classifier with stemming and maybe n-grams of words with your ML algorithm of choice. Naive Bayes is not bad for this and is easy to implement yourself. See many similar questions on here for more in depth explanations. im looking for something that has great performance and good accuracy. I read that random forests have good performance. Is it practical to use random forests for finding document’s polarity ? performance depends on the problem, data and features. algorithm is not so important. especially if you haven't done this before, it's important to work through the simple stuff so you get a feel for what features actually help you. +1 for NLTK. Look here for a sentiment analysis demo, btw: text-processing.com/demo/sentiment
Before starting from scratch, you can have a look at existing NLP frameworks.

Gate
OpenNLP
...


You can look at the software WEKA. It has many built-in machine learning classifiers which you can use for sentiment classification.
It requires you to convert the input data to ARFF format.

You could find some interesting datasets from NLP, NER to Image Classification, Bounding here: https://dataturks.com/projects/trending

If You are a completely beginner for nlp and python , then you can try some good api for sentiment analysis.
Here are some Api that you can use for your task
1.) sentiment analysis api
2.) Monkey Learn api for sentiment analysis
For reading Purpose 
Great info on Sentiment Analysis:
Weka simple K-means clustering assignments 
I have what feels like a simple problem, but I can't seem to find an answer. I'm pretty new to Weka, but I feel like I've done a bit of research on this (at least read through the first couple of pages of Google results) and come up dry.
I am using Weka to run clustering using Simple K-Means. In the results list I have no problem visualizing my output ("Visualize cluster assignments") and it is clear both from my understanding of the K-Means algorithm and the output of Weka that each of my instances is ending up as a member of a different cluster (centered around a particular centroid, if you will).
I can see something of the cluster composition from the text output. However Weka provides me with no explicit "mapping" from instance number to cluster number. I would like something like:
instance 1 --> cluster 0
instance 2 --> cluster 0
instance 3 --> cluster 2
instance 4 --> cluster 1
... etc.

How do I obtain these results without calculating the distance from each item to each centroid on my own?

I had the same problem and figured it out. I am posting the method here if anyone needs to know :
Its actually quite simple, you have to use Weka's java api.
SimpleKMeans kmeans = new SimpleKMeans();

kmeans.setSeed(10);

// This is the important parameter to set
kmeans.setPreserveInstancesOrder(true);
kmeans.setNumClusters(numberOfClusters);
kmeans.buildClusterer(instances);

// This array returns the cluster number (starting with 0) for each instance
// The array has as many elements as the number of instances
int[] assignments = kmeans.getAssignments();

int i=0;
for(int clusterNum : assignments) {
    System.out.printf("Instance %d -> Cluster %d", i, clusterNum);
    i++;
}

 Where can one find the up-to-date api in which the documentation of the methods setPreserveInstancesOrder, getAssignments etc. appear? hi @amon.gammon how if i want to show specific attributes from instance? stackoverflow.com/questions/21014916/…
Aha, I think I found what I was looking for.
Under the cluster visualizer, click "Save". This saves the whole data set as an ARFF file almost identical to the input file I provided, but with 2 new attributes: the first attribute is the index of the instance, while the last attribute is the cluster assignment. Now I just have to parse the crap out of it!
 Better yet: interface using the Java API. In a similar manner, can we get the cluster probabilities in EM algorithm of weka? Turns out, we can also use the -p command line parameter to make Weka output the predictions.Cosine distance as vector distance function for k-means 
I have a graph of N vertices where each vertex represents a place. Also I have vectors, one per user, each one of N coefficients where the coefficient's value is the duration in seconds spent at the corresponding place or 0 if that place was not visited.
E.g. for the graph:

the vector:
v1 = {100, 50, 0 30, 0}

would mean that we spent:
100secs at vertex 1
50secs at vertex 2 and 
30secs at vertex 4 

(vertices 3 & 5 where not visited, thus the 0s).
I want to run a k-means clustering and I've chosen cosine_distance = 1 - cosine_similarity as the metric for the distances, where the formula for cosine_similarity is:

as described here.
But I noticed the following. Assume k=2 and one of the vectors is:
v1 = {90,0,0,0,0}

In the process of solving the optimization problem of minimizing the total distance from candidate centroids, assume that at some point, 2 candidate centroids are:
c1 = {90,90,90,90,90}
c2 = {1000, 1000, 1000, 1000, 1000}

Running the cosine_distance formula for (v1, c1) and (v1, c2) we get exactly the same distance of 0.5527864045 for both.
I would assume that v1 is more similar (closer) to c1 than c2. Apparently this is not the case.
Q1. Why is this assumption wrong? 
Q2. Is the cosine distance a correct distance function for this case? 
Q3. What would be a better one given the nature of the problem?

Let's divide cosine similarity into parts and see how and why it works. 
Cosine between 2 vectors - a and b - is defined as: 
cos(a, b) = sum(a .* b) / (length(a) * length(b))

where .* is an element-wise multiplication. Denominator is here just for normalization, so let's simply call it L. With it our functions turns into: 
cos(a, b) = sum(a .* b) / L

which, in its turn, may be rewritten as: 
cos(a, b) = (a[1]*b[1] + a[2]*b[2] + ... + a[k]*b[k]) / L = 
          = a[1]*b[1]/L + a[2]*b[2]/L + ... + a[k]*b[k]/L

Let's get a bit more abstract and replace x * y / L  with function g(x, y) (L here is constant, so we don't put it as function argument). Our cosine function thus becomes: 
cos(a, b) = g(a[1], b[1]) + g(a[2], b[2]) + ... + g(a[n], b[n]) 

That is, each pair of elements (a[i], b[i]) is treated separately, and result is simply sum of all treatments. And this is good for your case, because you don't want different pairs (different vertices) to mess with each other: if user1 visited only vertex2 and user2 - only vertex1, then they have nothing in common, and similarity between them should be zero. What you actually don't like is how similarity between individual pairs - i.e. function g() - is calculated. 
With cosine function similarity between individual pairs looks like this: 
g(x, y) = x * y / L

where x and y represent time users spent on the vertex. And here's the main question: does multiplication represent similarity between individual pairs well? I don't think so. User who spent 90 seconds on some vertex should be close to user who spent there, say, 70 or 110 seconds, but much more far from users who spend there 1000 or 0 seconds. Multiplication (even normalized by L) is totally misleading here. What it even means to multiply 2 time periods? 
Good news is that this is you who design similarity function. We have already decided that we are satisfied with independent treatment of pairs (vertices), and we only want individual similarity function g(x, y) to make something reasonable with its arguments. And what is reasonable function to compare time periods? I'd say subtraction is a good candidate: 
g(x, y) = abs(x - y)

This is not similarity function, but instead distance function - the closer are values to each other, the smaller is result of g() - but eventually idea is the same, so we can interchange them when we need. 
We may also want to increase impact of large mismatches by squaring the difference: 
g(x, y) = (x - y)^2 

Hey! We've just reinvented (mean) squared error! We can now stick to MSE to calculate distance, or we can proceed finding good g() function. 
Sometimes we may want not increase, but instead smooth the difference. In this case we can use log: 
g(x, y) = log(abs(x - y))

We can use special treatment for zeros like this: 
g(x, y) = sign(x)*sign(y)*abs(x - y)   # sign(0) will turn whole expression to 0

Or we can go back from distance to similarity by inversing the difference: 
g(x, y) = 1 / abs(x - y)

Note, that in recent options we haven't used normalization factor. In fact, you can come up with some good normalization for each case, or just omit it - normalization is not always needed or good. For example, in cosine similarity formula if you change normalization constant L=length(a) * length(b) to L=1, you will get different, but still reasonable results. E.g. 
cos([90, 90, 90]) == cos(1000, 1000, 1000)  # measuring angle only
cos_no_norm([90, 90, 90]) < cos_no_norm([1000, 1000, 1000])  # measuring both - angle and magnitude

Summarizing this long and mostly boring story, I would suggest rewriting cosine similarity/distance to use some kind of difference between variables in two vectors.  

Cosine similarity is meant for the case where you do not want to take length into accoun, but the angle only.
If you want to also include length, choose a different distance function.
Cosine distance is closely related to squared Euclidean distance (the only distance for which k-means is really defined); which is why spherical k-means works.
The relationship is quite simple:
squared Euclidean distance sum_i (x_i-y_i)^2 can be factored into sum_i x_i^2 + sum_i y_i^2 - 2 * sum_i x_i*y_i. If both vectors are normalized, i.e. length does not matter, then the first two terms are 1. In this case, squared Euclidean distance is 2 - 2 * cos(x,y)!
In other words: Cosine distance is squared Euclidean distance with the data normalized to unit length.
If you don't want to normalize your data, don't use Cosine.

Q1. Why is this assumption wrong?
As we see from the definition, cosine similarity measures angle between 2 vectors. 
In your case, vector v1 lies flat on the first dimension, while c1 and c2 both are equally aligned from the axes, and thus, cosine similarity has to be same. 
Note that the trouble lies with c1 and c2 pointing in the same direction. Any v1 will have the same cosine similarity with both of them. For illustration : 

Q2. Is the cosine distance a correct distance function for this case?
As we see from the example in hand, probably not.
Q3. What would be a better one given the nature of the problem?
Consider Euclidean Distance.
 Given the nature of the problem I would be more inclined to use an asymmetrical function.   I mean that two users who visit a vertex, even if the time they stayed may not be exactly the same, should be considered closer than two users who didn't visit a vertex at all i.e. have matching 0s. To my understanding Euclidean Distance is symmetrical. Right? The problem is not with the vector having a lot of zeros. The problem is with each of the centroids having all the coefficient equal. I just 0'ed all the coefficients of the user vector to show more clearly that it should be closer to the one center than the other. You can replace the 0's of the user vector with non-zero values and you will still get the same distance from both centers. That is why I'm not 100% convinced cosine-distance is inappropriate for this problem after all.Latent Semantic Analysis concepts 
I've read about using Singular Value Decomposition (SVD) to do Latent Semantic Analysis (LSA) in corpus of texts. I've understood how to do that, also I understand mathematical concepts of SVD. 
But I don't understand why does it works applying to corpuses of texts (I believe - there must be linguistical explanation). Could anybody explain me this with linguistic point of view?
Thanks
 This might be a better fit at cstheory.stackexchange.com. Have you read the introductory paragraph of en.wikipedia.org/wiki/Latent_semantic_analysis? Hi , i have also had the same doubt ! is it mandatory to reduce the dimensions ? why cant we just use the v matrix to find the similarity between documents and the u matrix to find the similarity between terms ?
There is no linguistic interpretation, there is no syntax involved, no handling of equivalence classes, synonyms, homonyms, stemming etc. Neither are any semantics involved, it is just words-occuring-together.
Consider a "document" as a shopping cart: it contains a combination of words (purchases). And words tend to occur together with "related" words. 
For instance: The word "drug" can occur together with either of {love, doctor, medicine, sports, crime}; each will point you in a different direction. But combined with many other words in the document, your query will probably find  documents from a similar field.
 Your answer is a lot better than mine. And the drug example was a home run!
Words occurring together (i.e. nearby or in the same document in a corpus) contribute to context. Latent Semantic Analysis basically groups similar documents in a corpus based on how similar they are to each other in terms of context. 
I think the example and the word-document plot on this page will help in understanding.

Suppose we have the following set of five documents

d1 : Romeo and Juliet. 
d2 : Juliet: O happy dagger! 
d3 : Romeo died by dagger.
d4 : “Live free or die”, that’s the New-Hampshire’s motto. 
d5 : Did you know, New-Hampshire is in New-England.

and a search query: dies, dagger.
Clearly, d3 should be ranked top of the list since it contains both dies, dagger. Then, d2 and d4
should follow, each containing a word of the query. However, what about d1 and d5? Should they be
returned as possibly interesting results to this query? As humans we know that d1 is quite related
to the query. On the other hand, d5 is not so much related to the query. Thus, we would like d1 but
not d5, or differently said, we want d1 to be ranked higher than d5.
The question is: Can the machine deduce this? The answer is yes, LSI does exactly that. In this
example, LSI will be able to see that term dagger is related to d1 because it occurs together with
the d1’s terms Romeo and Juliet, in d2 and d3, respectively. Also, term dies is related to d1 and d5
because it occurs together with the d1’s term Romeo and d5’s term New-Hampshire in d3 and d4,
respectively. LSI will also weigh properly the discovered connections; d1 more is related to the query
than d5 since d1 is “doubly” connected to dagger through Romeo and Juliet, and also connected to
die through Romeo, whereas d5 has only a single connection to the query through New-Hampshire.
Reference: Latent Semantic Analysis (Alex Thomo)
Comparing a large number of graphs for isomorphism 
I am comparing a large set of networkx graphs for isomorphism, where most of the graphs should not be isomorphic (Lets say 0-20% are isomorphic to something in the list, for example).
I have tried the following approach.
graphs = [] # A list of networkx graphs
unique = [] # A list of unique graphs

for new in graphs:
    for old in unique:
        if nx.is_isomorphic(new, old[0]):
            break
    else:
        unique.append([new])

This let me get a much faster reduced set, but I still find it too slow for ideal use.  Is there some faster algorithm to handle this type of problem (comparing pairs of transitive commutative properties) or a way to extend this algorithm to a multicore setup (running on a 20 core machine).
I am already filtering these sets of data based on the number of nodes / edges, we can assume that the nx.is_isomorphic function cannot be made faster by any filtering types of operations.  I also cannot change tools easily right now, so using a compiled package is not an option.
Additional Information:
Graphs tend to be roughly 16-20 nodes with 24-48 edges total,  there is a lot of interconnection so each node has roughly 8 edges.  Each edge is labeled as well, but there are only 2-3 types of edges ever used.
 If you can think about some normalized way to represent a graph, you can compute a hash of that representation and compare hashes. How large are your graphs? Have you tried using could_be_isomorphic? 16-20 nodes with 24-48 edges - 2 or 3 types of edges... Can you post an example of a graph? it would be helpful to understand how the set of graphs can be efficiently partitioned for processing. Thanks You could group the graphs by certain features. Maybe group by tuple(sorted(edge counts)), and then only compare the graphs that could possibly be isomorphic. This would change your initial search from O(N^2) to O(N).
As others have mentioned, if you want to stay in Python + Networkx, you could use could_be_isomorphic to filter your graphs.
The problem is that this method expects 2 graphs as an input, not millions. If you compare every pair of graphs with this method, it would take an awfully long time.
Looking at the sourcecode of could_be_isomorphic, it compares degree, triangle, and number of cliques sequences for both graphs. If they're not equal, the graphs cannot be isomorphic.
You could pack this fingerprint in a function, sort your graphs according to this fingerprint and group them with itertools.groupby. There will be a huge majority of lone graphs. The few graphs that have the same fingerprints can then be checked for isomorphism.
Using a list of 100 000 random graphs:
many_graphs = [nx.fast_gnp_random_graph(random.randint(16, 22), 0.2) for _ in range(100000)]

There were only 500 fingerprints that were shared by at least 2 graphs. If you add edge types information, there will be even fewer common fingerprints.
Here's an example with 3000 graphs, each having between 10 and 14 nodes:
import networkx as nx
from itertools import groupby
import random

many_graphs = [nx.fast_gnp_random_graph(
    random.randint(10, 14), 0.3) for _ in range(3000)]


def graph_fingerprint(g):
    order = g.order()
    d = g.degree()
    t = nx.triangles(g)
    c = nx.number_of_cliques(g) 
    props = [[d[v], t[v], c[v]] for v in d]
    props.sort()
    # TODO: Add count of edge types.
    return(props)


sorted_graphs = sorted(many_graphs, key=graph_fingerprint)

for f, g in groupby(sorted_graphs, key=graph_fingerprint):
    similar_graphs = list(g)
    n = len(similar_graphs)
    if n > 1:
        print("Found %d graphs which could be isomorphic." % n)
        for i in range(n):
            for j in range(i + 1, n):
                g1, g2 = similar_graphs[i], similar_graphs[j]
                if g1 != g2 and nx.is_isomorphic(g1, g2):
                    print(" %s and %s are isomorphic!" %
                          (nx.generate_graph6(g1,header=False), nx.generate_graph6(g2,header=False)))

It finds 4 isomorphic pairs in less than 1s:
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
 Id?OGG_C? and IaKO_@?@? are isomorphic!
Found 6 graphs which could be isomorphic.
 I?OWcGG?G and I?OCSa?@_ are isomorphic!
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
 I_uI???JG and II??QDNA? are isomorphic!
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
 IDOCCY@GG and IOGC@`dS? are isomorphic!
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 3 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.
Found 2 graphs which could be isomorphic.

Here are the 2 last isomorphic graphs. "IDOCCY@GG":

and "IOGC@\dS?":

Here are 2 graphs which have the same fingerprint but aren't isomorphic:


The fingerprinting could be done in parallel. Sorting and grouping would have to happen on 1 CPU, but the isomorphism check for each group could be done in distinct CPUs.
 I am getting the following error --------------------------------------------------------------------------- KeyError                                  Traceback (most recent call last) <ipython-input-17-33a5378cd66e> in <module> ----> 1 sorted_graphs = sorted(many_graphs, key=graph_fingerprint)  <ipython-input-14-e93190098d6e> in graph_fingerprint(g)       4     t = nx.triangles(g)       5     c = nx.number_of_cliques(g)   KeyError: (0, 4) 
Can you use nauty (http://users.cecs.anu.edu.au/~bdm/nauty/, available in linux distributions)?  That has a canonical label algorithm that is fast and might work for your problem.  A canonical labeling makes isomorphic graphs identical (canonization).  For example using graph6 format output from a set of random graphs gives the following count of isomorphic graphs
$ cat g6.py
import networkx as nx
for i in range(100000):
    print(nx.generate_graph6(nx.fast_gnp_random_graph(4,0.2),header=False))


$ python g6.py  |nauty-labelg  |sort |uniq -c 
>A labelg
>Z 100000 graphs labelled from stdin to stdout in 0.21 sec.
   4898 C`
    167 C^
     10 C~
  26408 C?
  39392 C@
  19684 CB
   1575 CF
   1608 CJ
   1170 CN
    288 Cr
   4800 CR

Those are the 11 graphs of 4 nodes -
$ cat atlas.py 
import networkx as nx
for g in  nx.atlas.graph_atlas_g()[8:19]:
     print(nx.generate_graph6(g,header=False))
$ python atlas.py  |nauty-labelg  |sort |uniq -c 
>A labelg
>Z 11 graphs labelled from stdin to stdout in 0.00 sec.
      1 C`
      1 C^
      1 C~
      1 C?
      1 C@
      1 CB
      1 CF
      1 CJ
      1 CN
      1 Cr
      1 CR

It would be pretty easy to parallelize this approach if it runs too slow.

You can try your code on PyPy which provides just-in-time compilation for pure Python code. For possible performance boost they say it...

...depends greatly on the type of task being performed. The geometric average of all benchmarks is 0.13 or 7.5 times faster than CPython  

If your workload is CPU-bound (which seems like so) and Python process is long-running (so JIT compilation can be performed) then performance boost can be significant. NetworkX is pure Python (it has optional dependencies like numpy, but they are needed for extra functionality) and specifically isomorph module. I tried PyPy 5.7.1 and networkx/algorithms/isomorphism/tests/test_isomorphism.py passes. The the suite in general has a few failures:
Ran 2952 tests in 51.311s

FAILED (failures=3, skipped=54)
Test failed: <unittest.runner.TextTestResult run=2952 errors=0 failures=3>

On Python 2.7.12 it's:
Ran 2952 tests in 88.915s

OK (skipped=54)

 I will give this a try,  easier than switching to a different library but still gives performance ontop of any improvements.Hierarchical Clustering: Determine optimal number of cluster and statistically describe Clusters 
I could use some advice on methods in R to determine the optimal number of clusters and later on describe the clusters with different statistical criteria. I’m new to R with basic knowledge about the statistical foundations of cluster analysis. 

Methods to determine the number of clusters: In the literature one common method to do so is the so called "Elbow-criterion" which compares the Sum of Squared Differences (SSD) for different cluster solutions. Therefore the SSD is plotted against the numbers of Cluster in the analysis and an optimal number of clusters is determined by identifying the “elbow” in the plot (e.g. here: https://en.wikipedia.org/wiki/File:DataClustering_ElbowCriterion.JPG)
This method is a first approach to get a subjective impression. Therefore I’d like to implement it in R. The information on the internet on this is sparse. There is one good example here: http://www.mattpeeples.net/kmeans.html where the author also did an interesting iterative approach to see if the elbow is somehow stable after several repetitions of the clustering process (nevertheless it is for partitioning cluster methods not for hierarchical). 
Other methods in Literature comprise the so called “stopping rules”. MILLIGAN & COOPER compared 30 of these stopping rules in their paper “An examination of procedures for determining the number of clusters in a data set” (available here: http://link.springer.com/article/10.1007%2FBF02294245) finding that the Stopping Rule from Calinski and Harabasz provided the best results in a Monte Carlo evaluation. Information on implementing this in R is even sparser. 
So if anyone has ever implemented this or another Stopping rule (or other method) some advice would be very helpful.
Statistically describe the clusters:For describing the clusters I thought of using the mean and some sort of Variance Criterion. My data is on agricultural land-use and shows the production numbers of different crops per Municipality. My aim is to find similar patterns of land-use in my dataset. 

I produced a script for a subset of objects to do a first test-run. It looks like this (explanations on the steps within the script, sources below).  
    #Clusteranalysis agriculture

    #Load data
    agriculture <-read.table ("C:\\Users\\etc...", header=T,sep=";")
    attach(agriculture)

    #Define Dataframe to work with
    df<-data.frame(agriculture)

    #Define a Subset of objects to first test the script
    a<-df[1,]
    b<-df[2,]
    c<-df[3,]
    d<-df[4,]
    e<-df[5,]
    f<-df[6,]
    g<-df[7,]
    h<-df[8,]
    i<-df[9,]
    j<-df[10,]
    k<-df[11,]
    #Bind the objects
    aTOk<-rbind(a,b,c,d,e,f,g,h,i,j,k)

    #Calculate euclidian distances including only the columns 4 to 24
    dist.euklid<-dist(aTOk[,4:24],method="euclidean",diag=TRUE,upper=FALSE, p=2)
    print(dist.euklid)

    #Cluster with Ward
    cluster.ward<-hclust(dist.euklid,method="ward")

    #Plot the dendogramm. define Labels with labels=df$Geocode didn't work
    plot(cluster.ward, hang = -0.01, cex = 0.7)

    #here are missing methods to determine the optimal number of clusters

    #Calculate different solutions with different number of clusters
    n.cluster<-sapply(2:5, function(n.cluster)table(cutree(cluster.ward,n.cluster)))
    n.cluster

    #Show the objects within clusters for the three cluster solution
    three.cluster<-cutree(cluster.ward,3)
    sapply(unique(three.cluster), function(g)aTOk$Geocode[three.cluster==g])

    #Calculate some statistics to describe the clusters
    three.cluster.median<-aggregate(aTOk[,4:24],list(three.cluster),median)
    three.cluster.median
    three.cluster.min<-aggregate(aTOk[,4:24],list(three.cluster),min)
    three.cluster.min
    three.cluster.max<-aggregate(aTOk[,4:24],list(three.cluster),max)
    three.cluster.max
    #Summary statistics for one variable
    three.cluster.summary<-aggregate(aTOk[,4],list(three.cluster),summary)
    three.cluster.summary

    detach(agriculture)

Sources:

http://www.r-tutor.com/gpu-computing/clustering/distance-matrix
How to apply a hierarchical or k-means cluster analysis using R?
http://statistics.berkeley.edu/classes/s133/Cluster2a.html

 You may want to take a look at Numerical Ecology with R, by Borcard, Gillet, and Legendre, which has a good chapter on cluster analysis: springer.com/statistics/life+sciences,+medicine+%26+health/book/… I just odered a copy of the book fom our library and will have a look on it. Thank you for the advice!... i must admit that I find it quite strange that there are a lot of manuals on how to perform a Cluster Analysis and only few on how to actually deal with the results :/ In my opinion that's because there are many more people who know how to perform CA than understand the results! If you like the book you may also want to check out Legendre and Legendre Numerical Ecology, which is not R-specific but is very general and authoritative. @Drew Steen thanks once more for the advice on the literature. I just recieved the book and it comes with quite a lot of interesting methods to deepen the cluster analysis and plot the the dissimilartities. As I get trough it i'll try to post a solution. I've answered a Q elsewhere that addresses part 1 of the above: stackoverflow.com/a/15376462/1036500
The elbow criterion as your links indicated is for k-means. Also the cluster mean is obviously related to k-means, and is not appropriate for linkage clustering (in particular not for single-linkage, see single-link-effect).
Your question title however mentions hierarchical clustering, and so does your code?
Note that the elbow criterion does not choose the optimal number of clusters. It chooses the optimal number of k-means clusters. If you use a different clustering method, it may need a different number of clusters.
There is no such thing as the objectively best clustering. Thus, there also is no objectively best number of clusters. There is a rule of thumb for k-means that chooses a (maybe best) tradeoff between number of clusters and minimizing the target function (because increasing the number of clusters always can improve the target function); but that is mostly to counter a deficit of k-means. It is by no means objective.
Cluster analysis in itself is not an objective task. A clustering may be mathematically good, but useless. A clustering may score much worse mathematically, but it may provide you insight to your data that cannot be measured mathematically.
 Thank you for your answer. I think it highlights some important points in cluster analysis. I agree with you completely that there is no such thing like an objectively best clustering. Clustering methods are to a good degree subjective and in fact I wasn't searching for an objective method to interpret the results of the cluster method. I was/am searching for a robust method to determine the best number of cluster in hierarchical clustering in R that represents best my data structure. I think this is a tricky point in cluster analysis because as you mentioned there are always a bunch... ...of possible solutions. So besides the empirical interpretation some statistic indicators can be used to determine a good number of clusters based on homogeneity inside the clusters and heterogeneity between them. The Elbow criterion based on SSD is not necessarily linked to the k-means algorithm. Ward- Clustering is also based on minimizing the SSD within Clusters (with the difference that this task is executed in a hierarchical way). Therefore the elbow in SSD can indicate a good number of homogenous clusters where the SSD is still low inside clusters and high between them. For hierarchical clustering, the common approach is to look at the dendrogram. Just fixing the target number of clusters doesn't give you the option of cutting at different depth. A visual check helps a lot here. If  I understood it right than looking at the dendogramm and plotting SSDs against the number of clusters is quite the same isn't it? the problem with locking at my dendogramm is, that I have so many objects that my dendogramm is too growded to see anything. Maybe you have an advice how to plot it with higer resolution (i'm quite new to R so i get stuck in this basic stuff)? Maybe it would be interesting to plot both, the dendogramm and the SSDs against number of clusters... Only look at the top part of the dendrogram. The point is, you want to see if there is a clear threshold. If the dendrogram doesn't have big steps at the top, it's not significant. SSD can't capture this, because it tests one particular horizontal cut, not whether there is a good reason to choose this cut.
This is a very late answer and probably not useful for the asker anymore - but maybe for others. Check out the package NbClust. It contains 26 indices that give you a recommended number of clusters (and you can also choose your type of clustering). You can run it in such a way that you get the results for all the indices and then you can basically go with the number of clusters recommended by most indices. And yes, I think the basic statistics are the best way to describe clusters.

You can also try the R-NN Curves method.
http://rguha.net/writing/pres/rnn.pdf
 Thanks for your advice and the link! have you ever done this in R? When working on segmentation I was handling time series of 48 points, so the R-NN curves method didn't fit my needs cause decreasing the dimensionality removed the differences I was trying to highlight ... But I could probably still help you. I must have, somewhere, a document much more detailed (including some scripts) than the simple one I've posted. I'll be back as soon as I find it.
K means Clustering is highly sensitive to the scale of data e.g. for a person's age and salary, if not normalized, K means would consider salary more important variable for clustering rather than age, which you do not want. So before applying the Clustering Algorithm, it is always a good practice to normalize the scale of data, bring them to the same level and then apply the CA.
Monitor brands with common words 
Let's say you should monitor the brand "ONE" online. What algorithms can be used to separate pages about the brand ONE from pages containing the common word ONE?
I'm thinking maybe Bayes could work, but are there other ways to do this?

If it's not really unique word then I would suggest the next approach.
Let's imagine that our key-word is Java. Then there are at least 2 categories: about programming and about tourism in Indonesia. We are interested in the first one.
Lets take a small text about Java (maybe from books or from wikipedia). Then lets assume some threshold (for example, 0.7). Then let's compare our text with different pages (one of the fastest ways is using Classic Vector Space Model algorithm, you can implement it yourself or find it's implementation in google). Then compare results with your threshold and filter weak results.

About using Bayes algorithm: it's not bad approach imo. But you should 'teach' your algorithm very carefully because several bad inputs can spoil the whole work. 
Let me explain. Input for your Bayes algorithm is text with your brand-word. Output is probability [0 .. 1] that your text is about your brand but not about something else. In practice this algorithm very often gives you results near 0 or near 1 and it rare returns values between 0.2 and 0.8. It means that the algorithm is very sensitive to small variations and 1 or 2 words in text of 100 words can seriously affect the result.
 I still don't see how VSM is any better than Bayes. Convince me, please? Actually, it's nice reason for some experiments. I've implemented both algorithms before and it's not difficult at all (you can also download some existing implementations). Prepare test input data (but it shouldn't be small), and verify which algorithm satisfy your requirements better.
You may want to associate brand ONE with its products, its executive officers or its challengers in your monitoring.

The term you're looking for is Concept learning or Concept extraction. The word One appears in many pages, but most often it refers to the concept of one as a quantity. Only rarely it refers to the concept of ONE the brand. (Another frequently used example is SUN as in the astral object sun, or the company named Sun).
I know Ari Rappoport has a lot of research on this topic. Practically this boils down to something like 
mouviciel's answer, but Ari's research is also about how you can automatically infer what related words you need to look for in order to distinguish one-as-number from one-the-brand.

I've done approaching things by seeing Wikipedia as a giant ontology (where each hyperlink is a relation between source node and end node).
EDIT : One very rough algo, with the "Java" example :

Query "Java" in wikipedia. Among
others, this should give you (at
least) the island and the programming
language.
Get the in / out nodes of these base
pages (from the base pages hyperlinks).
You have now small sets of correlated words.
Compute a "distance" of each set to the page and find the minimum of these distances.

The distance you'll use is very subjective and must be tweaked a bit to match your needs. You might have trouble getting the "core" of each page too, as parsing HTML will be a major pain.
 Could you please expand a bit? I don't understand what you mean I should do.
I would suggest an unsupervised approach to the problem:

Get as many possible documents which describe the "ONE" in correct context and create a corpus.
Find Statistically improbable phrases in that corpus against a standard english corpus.

this website gives a good example 
http://sip.s-anand.net/?url=http://en.wikipedia.org/wiki/Apple_Inc.

As you can see the brand specific terms such as ipod, powerpc etc are easily filtered out.
Once you have extracted those you can create a Google alert or similar equivalent (if google alerts are too simplistic) with Queries like  "SIP" AND "ONE" to monitor new articles.
Of course given this approach is unsupervised it might not be very efficient but should do the work.
 you can find the code for SIP using google app engine here: code.google.com/p/statistically-improbable-phrases/source/…
A different approach could be to look the page up in Google Directory, which has 'the web organized by topic into categories'. You could potentially use the category information for each page to decide what it is about.
Hadoop Machine learning/Data mining project idea? [closed] 









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,   or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question   can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 7 years ago.



I am a graduate CS student (Data mining and machine learning) and have a good exposure to core Java (>4 years). I have read up a bunch of stuff on Hadoop and Map/Reduce
I would now like to do a project on this stuff (over my free time of corse) to get a better understanding.
Any good project ideas would be really appreciated. I just wanna do this to learn, so I dont really mind re-inventing the wheel. Also, anything related to data mining/machine learning would be an added bonus (fits with my research) but absolutely not necessary.
 What do you mean by graph mining? what are your interests you would like to learn/build/improve on? I have some suggestions but want to first await your personal interest-fields... By graph mining, I meant I have worked on optimization problems on large graphs (Flicker, DBLP) and am currently working on some social network graph analysis including Topic modelling in Twitter Data and also on low rank matrix approximations of large graphs. Even otherwise, I am interested in machine learning and data mining problems in particular. However, for Hadoop I am looking for any non-trivial but non-research project I can do in my free time, just to have a better understanding.
You haven't written anything about your interest.
I know algorithms in graph mining has been implemented over hadoop framework. This software http://www.cs.cmu.edu/~pegasus/ and paper : "PEGASUS: A Peta-Scale Graph Mining System - Implementation and Observations" may give you starting point.
Further, this link discusses something similar to your question: http://atbrox.com/2010/02/08/parallel-machine-learning-for-hadoopmapreduce-a-python-example/ but it is in python.
And, there is a very good paper by Andrew Ng "Map-Reduce for Machine Learning on Multicore".
There was a NIPS 2009  workshop on similar topic "Large-Scale Machine Learning: Parallelism and Massive Datasets". You can browse some of the paper and get an idea. 
Edit : Also there is Apache Mahout http://mahout.apache.org/   -->" Our core algorithms for clustering, classfication and batch based collaborative filtering are implemented on top of Apache Hadoop using the map/reduce paradigm"
 Thank you so much for the pointers. I appreciate it. I have basically concentrated on machine learning, graph mining stuff in my masters and am looking to learn hadoop as an additional skill-set. I have gone through projects like Mahout, but I would like to build something on my own to gain a deeper understanding as well as some large-scale software development.
See http://www.quora.com/Machine-Learning/What-are-some-good-class-projects-for-machine-learning-using-MapReduce
and some good toy projects to start with: http://www.quora.com/Programming-Challenges-1/What-are-some-good-toy-problems-in-data-science

Why don't you contribute to Apache Hadoop/Mahout by helping them implement additional algorithms?
https://cwiki.apache.org/confluence/display/MAHOUT/Algorithms
Has a number of algorithms marked as "open". To my understanding, they could use help with implementing these? And there are hundreds of algorithms even missing from this list.
By any means, since you want to do something with Hadoop, why don't you ask them what they need instead of asking on some random internet site?

Trying to think of an efficient way to implement Hierarchical Agglomerative Clustering on Hadoop is a nice project to work on. It not only involves algorithmic aspects but also had hadoop core framework related optimizations.
What method do you use for selecting the optimum number of clusters in k-means and EM? 
Many algorithms for clustering are available. A popular algorithm is the K-means where, based on a given number of clusters, the algorithm iterates to find best clusters for the objects. 
What method do you use to determine the number of clusters in the data in k-means clustering?
Does any package available in R contain the V-fold cross-validation method for determining the right number of clusters?
Another well used approach is Expectation Maximization (EM) algorithm which assigns a probability distribution to each instance which indicates the probability of it belonging to each of the clusters.
Is this algorithm implemented in R?
If it is, does it have the option to automatically select the optimum number of clusters by cross validation?
Do you prefer some other clustering method instead?
 I intentionally left out hierarchical clustering because hclust is a rather memory hungry method, not suitable for large datasets in which i'm actually mostly interested. Please define what you mean by "optimal" Great question @Svante, I've been thinking a lot on that one. I even intended to write a package with several algorithms for optimal number of clusters (hclust methods only).  @hadley, I've acquainted with: C-H index (Calinsky & Harabasz), C-index,  Goodman-Kruskal gamma coef. and there's a way to "pick an optimal cluster solution" by utilizing F-test. Here's a ref: Miligan, G.W. & Cooper, M.C. (1985). An Examination of Procedures for Determining the Number of Clusters in a Data Set, Psychometrika, 50, 159-179 Though I assume that you prefer "graph-based" decision on optimal solution... @hadley, in the sense of maximizing some score function having as arguments perhaps the between class distance and the within class distance. See, for example the method described in paragraph Optimal Number of Clusters here: sandro.saitta.googlepages.com/… This may also come in handy: stats.stackexchange.com/questions/723/…
For large "sparse" datasets i would seriously recommend "Affinity propagation" method.
It has superior performance compared to k means and it is deterministic in nature.
http://www.psi.toronto.edu/affinitypropagation/
  It was published in journal "Science".
However the choice of optimal clustering algorithm depends on the data set under consideration. K Means is a text book method and it is very likely that some one has developed a better algorithm more suitable for your type of dataset/
This is a good tutorial by Prof. Andrew Moore (CMU, Google) on K Means and Hierarchical Clustering.
http://www.autonlab.org/tutorials/kmeans.html

Last week I coded up such an estimate-the-number-of-clusters algorithm for a K-Means clustering program. I used the method outlined in:
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9687&rep=rep1&type=pdf
My biggest implementation problem was that I had to find a suitable Cluster Validation Index (ie error metric) that would work. Now it is a matter of processing speed, but the results currently look reasonable.
Clustering values by their proximity in python (machine learning?) [duplicate] 






This question already has answers here:
                        
                    



Cluster one-dimensional data optimally? [closed]

                                (1 answer)
                            


1D Number Array Clustering

                                (2 answers)
                            

Closed 6 years ago.



I have an algorithm that is running on a set of objects. This algorithm produces a score value that dictates the differences between the elements in the set.
The sorted output is something like this:
[1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]
If you lay these values down on a spreadsheet you see that they make up groups
[1,1,5,6,1,5] [10,22,23,23] [50,51,51,52] [100,112,130] [500,512,600] [12000,12230]
Is there a way to programatically get those groupings?
Maybe some clustering algorithm using a machine learning library? Or am I overthinking this?
I've looked at scikit but their examples are way too advanced for my problem...

Don't use clustering for 1-dimensional data
Clustering algorithms are designed for multivariate data. When you have 1-dimensional data, sort it, and look for the largest gaps. This is trivial and fast in 1d, and not possible in 2d. If you want something more advanced, use Kernel Density Estimation (KDE) and look for local minima to split the data set.
There are a number of duplicates of this question:

1D Number Array Clustering
Cluster one-dimensional data optimally?

 This approach might be sensitive to noise. On the contrary. KDE is smooth and thus not too sensitive to noise. Much less so than k-means which is known to be very sensitive due to squared error terms. Interesting, thanks for pointing this out. What does "largest" mean in this context? If I have a list of gaps, how do I find those that constitute a large enough gap to separate "clusters"?
A good option if you don't know the number of clusters is MeanShift:
import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth

x = [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]

X = np.array(zip(x,np.zeros(len(x))), dtype=np.int)
bandwidth = estimate_bandwidth(X, quantile=0.1)
ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)
labels = ms.labels_
cluster_centers = ms.cluster_centers_

labels_unique = np.unique(labels)
n_clusters_ = len(labels_unique)

for k in range(n_clusters_):
    my_members = labels == k
    print "cluster {0}: {1}".format(k, X[my_members, 0])

Output for this algorithm:
cluster 0: [ 1  1  5  6  1  5 10 22 23 23 50 51 51 52]
cluster 1: [100 112 130]
cluster 2: [500 512]
cluster 3: [12000]
cluster 4: [12230]
cluster 5: [600]

Modifying quantilevariable you can change the clustering number selection criteria
 The first argument of np.array needs to be list(zip(x,np.zeros(len(x)))). Otherwise, Python throws an error: TypeError: int() argument must be a string, a bytes-like object or a number, not 'zip' This approach may not work very well for some inputs that aren't easily "clusterable", e.g. x = [90, 100, 110]. It will then fail with ValueError: Expected n_neighbors > 0. Got 0 (which can be avoided with parameter tuning). For such inputs stackoverflow.com/a/18385795/942774 is probably the much simpler and better answer.
You can use clustering to group these. The trick is to understand that there are two dimensions to your data: the dimension you can see, and the "spatial" dimension that looks like [1, 2, 3... 22]. You can create this matrix in numpy like so:
import numpy as np

y = [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]
x = range(len(y))
m = np.matrix([x, y]).transpose()

Then you can perform clustering on the matrix, with:
from scipy.cluster.vq import kmeans
kclust = kmeans(m, 5)

kclust's output will look like this:
(array([[   11,    51],
       [   15,   114],
       [   20, 12115],
       [    4,     9],
       [   18,   537]]), 21.545126372346271)

For you, the most interesting part is the first column of the matrix, which says what the centers are along that x dimension:
kclust[0][:, 0]
# [20 18 15  4 11]

You can then assign your points to a cluster based on which of the five centers they are closest to:
assigned_clusters = [abs(cluster_indices - e).argmin() for e in x]
# [3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 2, 2, 2, 2, 1, 1, 0, 0, 0]

 an updated function kmeans2 (in scipy.cluster.vq) now outputs both centroid and label, e.g. kclust, label = kmeans(m, 5) Hi, The code does not work. Error in first line for obvious reasons. The last line also produces an error, 'cluster_indices' not defined. Can you please help to get this code running ? @gprakhar Use cluster_indices = kclust[0][:, 0].Outlier detection in data mining [closed] 






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 7 years ago.




I have a few sets of questions regarding outlier detection:

Can we find outliers using k-means and is this a good approach?
Is there any clustering algorithm which does not accept any input from the user?
Can we use support vector machine or any other supervised learning algorithm for outlier detection?
What are the pros and cons of each approach?

 This question would better fit on stats.stackexchange.com, IMO. Great contribution to SO community! These are very important topics that every programmer must deal with! can't believe this question was closed! machinelearningstories.blogspot.com/2018/07/…
I will limit myself to what I think is essential to give some clues about all of your questions, because this is the topic of a lot of textbooks and they might probably be better addressed in separate questions.

I wouldn't use k-means for spotting outliers in a multivariate dataset, for the simple reason that the k-means algorithm is not built for that purpose: You will always end up with a solution that minimizes the total within-cluster sum of squares (and hence maximizes the between-cluster SS because the total variance is fixed), and the outlier(s) will not necessarily define their own cluster. Consider the following example in R:
set.seed(123)
sim.xy <- function(n, mean, sd) cbind(rnorm(n, mean[1], sd[1]),
                                      rnorm(n, mean[2],sd[2]))
# generate three clouds of points, well separated in the 2D plane
xy <- rbind(sim.xy(100, c(0,0), c(.2,.2)),
            sim.xy(100, c(2.5,0), c(.4,.2)),
            sim.xy(100, c(1.25,.5), c(.3,.2)))
xy[1,] <- c(0,2)     # convert 1st obs. to an outlying value
km3 <- kmeans(xy, 3) # ask for three clusters
km4 <- kmeans(xy, 4) # ask for four clusters

As can be seen in the next figure, the outlying value is never recovered as such: It will always belong to one of the other clusters.

One possibility, however, would be to use a two-stage approach where one's removing extremal points (here defined as vector far away from their cluster centroids) in an iterative manner, as described in the following paper: Improving K-Means by Outlier Removal (Hautamäki, et al.).
This bears some resemblance with what is done in genetic studies to detect and remove individuals which exhibit genotyping error, or individuals that are siblings/twins (or when we want to identify population substructure), while we only want to keep unrelated individuals; in this case, we use multidimensional scaling (which is equivalent to PCA, up to a constant for the first two axes) and remove observations above or below 6 SD on any one of say the top 10 or 20 axes (see for example, Population Structure and Eigenanalysis, Patterson et al., PLoS Genetics 2006 2(12)).
A common alternative is to use ordered robust mahalanobis distances that can be plotted (in a QQ plot) against the expected quantiles of a Chi-squared distribution, as discussed in the following paper:

R.G. Garrett (1989). The chi-square plot: a tools for multivariate outlier recognition. Journal of Geochemical Exploration 32(1/3): 319-341.

(It is available in the mvoutlier R package.)
It depends on what you call user input. I interpret your question as whether some algorithm can process automatically a distance matrix or raw data and stop on an optimal number of clusters. If this is the case, and for any distance-based partitioning algorithm, then you can use any of the available validity indices for cluster analysis; a good overview is given in 

Handl, J., Knowles, J., and Kell, D.B.
  (2005). Computational cluster validation in post-genomic data analysis. 
  Bioinformatics 21(15): 3201-3212.

that I discussed on Cross Validated. You can for instance run several instances of the algorithm on different random samples (using bootstrap) of the data, for a range of cluster numbers (say, k=1 to 20) and select k according to the optimized criteria taht was considered (average silhouette width, cophenetic correlation, etc.); it can be fully automated, no need for user input.
There exist other forms of clustering, based on density (clusters are seen as regions where objects are unusually common) or distribution (clusters are sets of objects that follow a given probability distribution). Model-based clustering, as it is implemented in Mclust, for example, allows to identify clusters in a multivariate dataset by spanning a range of shape for the variance-covariance matrix for a varying number of clusters and to choose the best model according to the BIC criterion.
This is a hot topic in classification, and some studies focused on SVM to detect outliers especially when they are misclassified. A simple Google query will return a lot of hits, e.g. Support Vector Machine for Outlier Detection in Breast Cancer Survivability Prediction by Thongkam et al. (Lecture Notes in Computer Science 2008 4977/2008 99-109; this article includes comparison to ensemble methods). The very basic idea is to use a one-class SVM to capture the main structure of the data by fitting a multivariate (e.g., gaussian) distribution to it; objects that on or just outside the boundary might be regarded as potential outliers. (In a certain sense, density-based clustering would perform equally well as defining what an outlier really is is more straightforward given an expected distribution.)  
Other approaches for unsupervised, semi-supervised, or supervised learning are readily found on Google, e.g.

Hodge, V.J. and Austin, J. A Survey of Outlier Detection Methodologies.
Vinueza, A. and Grudic, G.Z. Unsupervised Outlier Detection and Semi-Supervised Learning.
Escalante, H.J. A Comparison of Outlier Detection Algorithms for Machine Learning.


A related topic is anomaly detection, about which you will find a lot of papers.
That really deserves a new (and probably more focused) question :-)


1) Can we find outliers using k-means, is it a good approach?
Cluster-based approaches are optimal to find clusters, and can be used to detect outliers as
by-products. In the clustering processes, outliers can affect the locations of the cluster centers, even aggregating as a micro-cluster. These characteristics make the cluster-based approaches infeasible to complicated databases.
2) Is there any clustering algorithm which does not accept any input from the user?
Maybe you can achieve some valuable knowledge on this topic:
Dirichlet Process Clustering
Dirichlet-based clustering algorithm can adaptively determine the number of clusters according to the distribution of observation data.
3) Can we use support vector machine or any other supervised learning algorithm for outlier detection?
Any Supervised learning algorithm needs enough labeled training data to construct classifiers. However, a balanced training dataset is not always available for real world problem, such as intrusion detection, medical diagnostics. According to the definition of Hawkins Outlier("Identification of Outliers". Chapman and Hall, London, 1980), the number of normal data is much larger than that of outliers. Most supervised learning algorithms can't achieve an efficient classifier on the above unbalanced dataset.
4) What is the pros and cons of each approach?
Over the past several decades, the research on outlier detection varies from the global computation to the local analysis, and the descriptions of outliers vary from the binary interpretations to probabilistic representations. According to hypotheses of outlier detection models, outlier detection algorithms can be divided into four kinds: Statistic-based algorithms, Cluster-based algorithms, Nearest Neighborhood based algorithms, and Classifier-based algorithms. There are several valuable surveys on outlier detection:

Hodge, V. and Austin, J. "A survey of outlier detection methodologies", Journal of Artificial Intelligence Review, 2004.
Chandola, V. and Banerjee, A. and Kumar, V. "Outlier detection: A survey", ACM Computing Surveys, 2007.



k-means is rather sensitive to noise in the data set. It works best when you remove the outliers beforehand.
No. Any cluster analysis algorithm that claims to be parameter-free usually is heavily restricted, and often has hidden parameters - a common parameter is the distance function, for example. Any flexible cluster analysis algorithm will at least accept a custom distance function.
one-class classifiers are a popular machine-learning approach to outlier detection. However, supervised approaches aren't always appropriate for detecting _previously_unseen_ objects. Plus, they can overfit when the data already contains outliers.
Every approach has its pros and cons, that is why they exist. In a real setting, you will have to try most of them to see what works for your data and setting. It's why outlier detection is called knowledge discovery - you have to explore if you want to discover something new ...


You may want to have a look at the ELKI data mining framework. It is supposedly the largest collection of outlier detection data mining algorithms. It's open source software, implemented in Java, and includes some 20+ outlier detection algorithms. See the list of available algorithms.
Note that most of these algorithms are not based on clustering. Many clustering algorithms (in particular k-means) will try to cluster instances "no matter what". Only few clustering algorithms (e.g. DBSCAN) actually consider the case that maybe not all instance belong into clusters! So for some algorithms, outliers will actually prevent a good clustering!
Naive Bayesian for Topic detection using “Bag of Words” approach 
I am trying to implement a naive bayseian approach to find the topic of a given document or stream of words. Is there are Naive Bayesian approach that i might be able to look up for this ? 
Also, i am trying to improve my dictionary as i go along. Initially, i have a bunch of words that map to a topics (hard-coded). Depending on the occurrence of the words other than the ones that are already mapped. And depending on the occurrences of these words i want to add them to the mappings, hence improving and learning about new words that map to topic. And also changing the probabilities of words.
How should i go about doing this ? Is my approach the right one ? 
Which programming language would be best suited for the implementation ?  

Existing Implementations of Naive Bayes
You would probably be better off just using one of the existing packages that supports document classification using naive Bayes, e.g.:
Python - To do this using the Python based Natural Language Toolkit (NLTK), see the Document Classification section in the freely available NLTK book.
Ruby - If Ruby is more of your thing, you can use the Classifier gem. Here's sample code that detects  whether Family Guy quotes are funny or not-funny.
Perl - Perl has the Algorithm::NaiveBayes module, complete with a sample usage snippet in the package synopsis. 
C# - C# programmers can use nBayes. The project's home page has sample code for a simple spam/not-spam classifier.
Java - Java folks have Classifier4J. You can see a training and scoring code snippet here.  
Bootstrapping Classification from Keywords
It sounds like you want to start with a set of keywords that are known to cue for certain topics and then use those keywords to bootstrap a classifier. 
This is a reasonably clever idea. Take a look at the paper Text Classication by Bootstrapping with Keywords, EM and Shrinkage by McCallum and Nigam (1999). By following this approach, they were able to improve classification accuracy from the 45% they got by using hard-coded keywords alone to 66% using a bootstrapped Naive Bayes classifier. For their data, the latter is close to human levels of agreement, as people agreed with each other about document labels 72% of the time.
 The C# developers also can use Accord Framework.NET. Naive Bayes documentation hereWhat is data mining from a developer's perspective? 
I can find the technical explanation of what data mining is in a book or on Wikipedia, but I'm wondering what sort of development does it exactly involve? Is it more about using tools or more about writing tools? Is it really any much different from other domains when it comes to R&D?
 Isn't StackOverflow a data mine? :-) In a way, yes. You could try to analyse the interest in specific tags over time, to see which is a future trend. Actually, you can even measure the knowledge level of the visitors, which -combined with the answers they've provided- could be used to find the best experts in a certain topic. Very practical for headhunters, if only they could collect enough information about all the high-reputation visitors from this site. I recommend you change your accepted answer. spark.apache.org  ;)
Data Mining is the process of discovering interesting patterns in large amounts of data. It is not querying data, which is just what user Treb describes (sorry Treb).
To understand DM from a developer's perspective, you should read the book Programming Collective Intelligence by Toby Segaran.
 Can't say that I agree with you - How would you discover any pattern in your data without querying first? Querying is the first step, therefore it's the first thing a developer has to think about. I admit that I completely forgot to mention any data analysis - statistics are certainly a must for any data mining application, as well as visual representation of large data sets. But performing an analysis is done by a data miner, not the developer. The OP was asking about data mining from a dev's POV, so that's what I tried to answer. "How would you discover any pattern in your data without querying first?" you ask. You discover patterns in your data by programmatic implementation, not by fishing with queries. This is the whole point -- getting the machine to detect the patterns in the data. And in order to detect pattern programmatically, you first need to look at the data. So in the end it comes down to queries, no matter if who is doing the querying. @Treb Yes, but querying alone will do nothing.  The data needs to be analyzed for patterns which is what data mining is.
In my experience (I'm a former data miner :-)), it's a mixture of using tools and writing tools. A lot of the time, the tools you need to analyse the particular data set don't exist, so you have to write them yourself first. It can be very interesting but you often need quite a different approach to the sort of programming I do now (embedded wireless), for example.

You really ought to change the accepted answer on this question so it doesn't mislead those who come across it.  
Saying that querying a database IS data mining because "[h]ow would you discover any pattern in your data without querying first?" is like saying opening your car door is driving because "how else would you be able to drive somewhere without opening the car door first."  
You can read your data out of a text file if you want.  My first data mining assignment used data sets from the UCI repository and those are almost all text files.
If you want to learn about data mining start by looking up clustering and classification.  Learn about decision trees and rule based classification.  Then look at k-nearest-neighbor and k-means.  After that if you really want to see what data mining is all about look at Chameleon, DBScan, and Support Vector Machines.  Don't necessarily learn the minutiae of the last three (they're pretty complex and math heavy) but understanding the abstract idea of what happens will tell you all you need to know in order to use the many tools and libraries that are available for each strategy.
These are only the algorithms that popped into my head just now.  There are so many others that I don't recall or don't even know yet. 

Data mining is about searching large quantities of data for hidden patterns.  Web 2.0 example: News corp uses its site myspace.com as a large data mine to determine what movies and products to promote.  They write software to identify trends in the data that it's users post to the site. News corp does this to gather information useful for advertising campaigns and market predictions.  It's different from other domains of R&D in that from a data givers perspective its passive.  Rather than going out on the street and asking people in person what movies they are likely to see this summer and other such questions, the data mining tools sort out these things by analyzing data given by users voluntarily.
Wikipedia actually does have a pretty good article on it:
 - http://en.wikipedia.org/wiki/Data_mining

Data Mining as I say is finding patterns or trends from given data. A developer perspective might be in applications like Anti Money Laundring... Where given a pattern you will search data for that given pattern. One other use is in Projection Softwares... where you project a result or outcome in future against a heuristic by studying recognizing the current trend from data.

I think it's more about using off the shelf tools rather than developing your own. An academic example of that kind of tools might be WEKA. Of course, you still have to know what algorithms use, how to preprocess data (very important this part), etc.
In R&D I don't have much idea, but it should be like almost everything: maths, statistics, more maths...
 I disagree. It is a lot about implementing and adjusting algorithms for your particular use case, as off the shelf solutions fail to cover everything and are only good for prototyping.
On the development level, data mining is just another database application, but with a huge amount of data. 
The mining itself is done by running specific queries on the database. It's in the creation of the queries where the important work is done. They of course depend on the  data model, and on the  hypotheses, what sort of trends the customer  expects to find.
Therefore, the fine tuning of the queries usually can't be done in development, but only once the system is live and you have live data. Then the user can test his hypotheses and adapt the queries to show him the trends he is looking for.
So from a dev point of view, data maining is about

Managing large sets of data in your client (one query may return 100.000 rows of data)
Providing the user (who may know nothing about SQL or relational databases in general) with an effective way to modify his queries and view the results.

 +1 That's what I'm actually doing, and couldn't have said this was data mining. Good explanation! Thanks! Clustering, Classification, Anomaly Detection, Similarity Measurement, etc aren't done by just "querying" the data and "adapting" those queries.  I disagree. @colithium: By which other means are they done, then? As stated in my response to ybakos' answer, my answer lacks any reference to data analysis methods, true. But I don't see how the first step in data mining can be anything else but accessing the data, which is usually done through queries. And this is where I see potential technical difficulties that the developer of a DM app should keep in mind. Sure to be used it must be accessed, I agree with you. But that's not the essence of data mining.  That's like prefacing every answer on SO with "you need to access RAM to solve your problem".  I'm not trying to be glib; data mining is about developing and/or choosing techniques to identify patterns in your vast data set.  It's not about querying for summary stats or interesting joins.WEKA Tutorials / Examples for a Newbie [closed] 






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                
Closed 6 years ago.




In a follow-up to this answer I want to ask if any of you know any good (and more importantly easy to understand) tutorials and / or examples of data mining with the Weka toolkit.
I've been very interested in Data Mining ever since I've first heard of it and the things it can do, I've also have some experiments I'd like to do with some of my data and I've already bought four books and I found specially interesting the following two:

Data Mining http://ecx.images-amazon.com/images/I/61DhYb1Z6QL._BO2,204,203,200_PIsitb-sticker-arrow-click,TopRight,35,-76_AA240_SH20_OU01_.jpg
The last one is written by the same authors of Weka and contains a lot of examples but still, I found it a little hard to understand the logic and specially the math. My math skills are currently very rough, I plan to go to the University this year and hopefully I'll learn and be able to better understand the math involved, but until then I want to gain some practice in Data Mining.
Is there any step-by-step tutorial with example data I can read to get me started with the Weka toolkit?
 You can take a look at my channel. Dedicated fully for Weka video tutorials. youtube.com/user/rushdishams
When it comes to "applied" data mining, for the starters, you may not need to think about math at all. Weka is product of a university machine-learning project and offers 100+ algorithms. Contrast that with Microsoft SQL server SSAS which offers nine algorithms -- and they do not even bother to explain the math.
They both offer association, clustering, attribute selection, some kind of neural network.
So, the trick is to understand what you are trying to achieve, not necessarily the math below. Try reading about Microsoft algorithms (good documentation) and see if you can figure out principles that SSAS and Weka have in common -- this should help you focus on basic principles and get you started.  
There is a list of a few  Weka tutorials here.

Personally, I would use RapidMiner5, as it's a super easy GUI environment (much better than v4). It has all of the WEKA functions available.
Get rapidminer here: http://www.rapid-i.com 
Super easy intro videos to RapidMiner here:
https://dspace.ndlr.ie/jspui/handle/10633/2353
here:
http://www.neuralmarkettrends.com/
and here:
http://www.youtube.com/user/VancouverData
If you want to learn more about the statistics behind data mining, see this Standford prof's lecture series at Google:
http://www.youtube.com/watch?v=zRsMEl6PHhM&feature=related

Here are links to a collection of videos and a collection of tutorials.
 broken links on "collection of videos" page above nobody cares about videos, reading's much better use of time one of those videos (the first one in the series, I think) is at v.youku.com/v_show/id_XMjYyMDAzMDc2.html?f=5939905dbscan - setting limit on maximum cluster span 
By my understanding of DBSCAN, it's possible for you to specify an epsilon of, say, 100 meters and — because DBSCAN takes into account density-reachability and not direct density-reachability when finding clusters — end up with a cluster in which the maximum distance between any two points is > 100 meters. In a more extreme possibility, it seems possible that you could set epsilon of 100 meters and end up with a cluster of 1 kilometer:
see [2][6] in this array of images from scikit learn for an example of when that might occur. (I'm more than willing to be told I'm a total idiot and am misunderstanding DBSCAN if that's what's happening here.)
Is there an algorithm that is density-based like DBSCAN but takes into account some kind of thresholding for the maximum distance between any two points in a cluster?

DBSCAN indeed does not impose a total size constraint on the cluster.
The epsilon value is best interpreted as the size of the gap separating two clusters (that may at most contain minpts-1 objects).
I believe, you are in fact not even looking for clustering: clustering is the task of discovering structure in data. The structure can be simpler (such as k-means) or complex (such as the arbitrarily shaped clusters discovered by hierarchical clustering and k-means).
You might be looking for vector quantization - reducing a data set to a smaller set of representatives - or set cover - finding the optimal cover for a given set - instead.
However, I also have the impression that you aren't really sure on what you need and why.
A stength of DBSCAN is that it has a mathematical definition of structure in the form of density-connected components. This is a strong and (except for some rare border cases) well-defined mathematical concept, and the DBSCAN algorithm is an optimally efficient algorithm to discover this structure.
Direct density reachability however, doesn't define a useful (partitioning) structure. It just does not partition the data into disjoint partitions.
If you don't need this kind of strong structure (i.e. you don't do clustering as in "structure discovery", but you just want to compress your data as in vector quantization), you could give "canopy preclustering" a try. It can be seen as a preprocessing step designed for clustering. Essentially, it is like DBSCAN, except that it uses two epsilon values, and the structure is not guaranteed to be optimal in any way, but will highly depend on the ordering of your data. If you then preprocess it appropriately, it can still be useful. Unless you are in a distributed setting, canopy preclustering however is at least as expensive than a full DBSCAN run. Due to the loose requirements (in particular, "clusters" may overlap, and objects are expected to belong to multiple "clusters"), it is easier to parallelize.
Oh, and you might also just be looking for complete-linkage hierarchical clustering. If you cut the dendrogram at your desired height, the resulting clusters should all have the desired maximum distance inbetween of any two objects. The only problem is that hierarchical clustering usually is O(n^3), i.e. it doesn't scale to large data sets. DBSCAN runs in O(n log n) in good implementations (with index support).
Can stop-words be found automatically? 
In NLP, stop-words removal is a typical pre-processing step. And it is typically done in an empirical way based on what we think stop-words should be.
But in my opinion, we should generalize the concept of stop-words. And the stop-words could vary for corpora from different domains. I am wondering if we can define the stop-words mathematically, such as by its statistical characteristics. And then can we automatically extract stop-words from a corpora for a specific domain.
Is there any similar thought and progress on this? Could anyone shed some light?
 short answer: depending on your corpus and task, you can set up different stop word list. Getting the cut-off term frequency value is magic.
Usually the stop-words occurs much more frequently than the other semantic words...So while building my application I used the combination of both; a fixed list and the statistical method. I was using NLTK and it already had a list of some common stop words; so I first removed the words which appears in this list, but of-course this didn't removed all the stop-words...As you already mentioned that the stop words differs from corpora to corpora. Then I evaluated the frequency of each word appearing in the corpora and removed the words which have a frequency above a "certain limit". This certain limit which I mention, was the value I fixed after observing the frequency of all the words...hence again this limit also depends on corpora to corpora...but you can easily calculate this once you carefully observe the list of all the words in order of their frequency...This statistical method will ensure that you are removing the Stop-Words which do not appears in list of common stop-words...After that to refine the data I also used POS tagging...and removed the proper nouns which still exist after the first two steps..

I am not an expert, but hope my answer makes sense.
Statistically extracting stop words from a corpus sounds interesting! I would consider calculating inverse document frequency, as mentioned in the other answers, apart from using regular stop words from a common stop-word list, like the one in NLTK. Stop words not only vary from corpora to corpora, they may also vary from problem to problem. For example, in one of the problems I was working, I was using a corpus of news articles, where you find a lot of time-sensitive and location-sensitive words. These were crucial information, and statistically removing words like "today", "here", etc. would have affected my results dearly. Because, news articles talk about not just one particular event, but also similar events that had happened in the past or in another location. 
My point, in short, is that you would need to consider the problem being addressed as well, and not just the corpus. 
Thanks,
Ramya

Stop words are ubiquitous. They will appear in every (or almost every) document. A good way to mathematically define stop words for corpora from different domains is to compute the inverse document frequency (IDF) of a word.
IDF is a better way over frequency computation to define stop words because simple frequency calculations are adversely affected by a few specialized documents containing a special word many times. This method has been used to automatically learn stop words in foreign languages (ref. Machine Learning with SVM and Other Kernel Methods).

Yes, stop-words can be detected automatically.
Word frequencies as a whole
One way is to look at word frequencies as a whole.
Calculate the frequency of all words in the combined texts. Sort them in falling order and remove the top 20% or so.
You may also wish to remove the bottom 5%. These are not stop-words, but for a lot of machine learning, they are inconsequential. Maybe even misspellings.
Words per "document"
Another way is to analyze words per "document."
In a set of documents, stop-words can be detected by finding words that exist in a large number of documents. They would be useless for categorizing or clustering documents in this particular set.
E.g. a machine learning system categorizing scientific papers might, after analysis mark the word "abstract" as a stop-word, even though it may only exist once per document. But in all likelihood in almost all of them.
The same would be true for words that are only found in a very limited number of documents. They are likely misspelled or so unique they might never be seen again.
However, in this case, it's important that the distribution between document groups in the learning set is even or a set divided into one large and one small group might lose all its significant words (since they may exist in too many documents or too few).
Another way to avoid problems with unevenly distributed groups in the training set is to only remove words that exist in all or almost all documents. (I.e. our favorite stop-words like "a", "it", "the", "an", etc will exist in all English texts).
Zipf's Law
When I studied Machine Learning and the discussion of stop-words came up, Zipf's Law was mentioned. However, today I couldn't tell you how or why, but maybe it's a general principle or mathematical foundation you'd want to look into...
I googled "Zipf's Law automatic stop word detection" and a quick pick found me two PDFs that may be of interest...

"Automatically Building a Stopword List for an Information Retrieval System"; Rachel Tsz-Wai Lo, Ben He, Iadh Ounis
"Automatically Generation and Evaluation of Stop Words List for Chinese Patents"; Deng Na, Chen Xu


Actually the common approach to build stopwords is to just use the most common (in documents, i.e. by DF) words. Build a list of the top 100, 200, 1000 words, and review them. Just browse the list until you find a word that in your opinion should not be a stopword. Then consider to either skip it, or break the list at this point.
In many data sets, you will have domain specific stopwords. If you use StackOverflow for example, "java" and "c#" could well be stopwords (and this actually won't harm much; in particular if you still also use the tags). Other domain specific stop words could be "code", "implement", "program".
In scikit learn, how to deal with the data mixed with numerical and nominal value? 
I know that the computation in scikit-learn is based on NumPy so everything is a matrix or array.
How does this package handle mixed data (numerical and nominal values)?
For example, a product could have the attribute 'color' and 'price', where color is nominal and price is numerical. I notice there is a model called 'DictVectorizer' to numerate the nominal data. For example, two products  are:
products = [{'color':'black','price':10}, {'color':'green','price':5}]

And the result from 'DictVectorizer' could be:
[[1,0,10],
 [0,1,5]]

If there are lots of different values for the attribute 'color', the matrix would be very sparse. And long features will degrade the performance of some algorithms, such as decision trees. 
Is there any way to use the nominal value without the need to create dummy codes?
 It's worth noting that Weka Instances store nominal values as floating point numbers corresponding to the index of the nominal in the attribute's definition. You could simply follow this same strategy to generate a numeric dataset for use with scikit-learn. Thanks a lot for enlarging my knowledge.
The DecisionTree class in scikit-learn will need some refactoring to deal efficiently with high-cardinality categorical features (and maybe even with naturally sparse data such as text TF-IDF vectors).
Nobody is working on that yet AFAIK.
 thanks a lot. In scikit, is there any smart way to do refactoring compared with manual operation? My answer states that this current state of affair is a limitation of the current implementation of the Decision Tree in scikit-learn. There is no easy fix I know of to remove that limitation. I don't understand what you call "manual operation".What free/paid search API's allow for programmatic querying and caching/storage of the resulting data? 
If you've done any serious research into search API's, you know that most of them have a huge slew of TOS/TOU restrictions that make them nearly impossible to use in anything but the most inane applications.
Bing's 2.0 API, Yahoo Search BOSS, Google Places, Google AJAX Search (dead), et al, are far too restrictive for us.  I need to run a finite and relatively small number of queries (perhaps 500k) one time only, storing specific data from the results for use within our application.
For example, we need to match up business names with their target websites (we have written the algorithm to make a 'best guess' from a set of results if necessary; we just need a vanilla result set).  Also, we need to match an address to this company in question.
Unfortunately, I can find ZERO search API's that will allow us to fire off queries in a programmatic, non-user-initiated manner.
We're even quite eager to give someone cold, hard cash for access to this kind of data; Google, Bing, Yahoo, and others simply seem to not want our money (as evidenced by their TOSes)...
Any thoughts?
 Hi, everyone!  I see that this has received a close vote.  If there is a SO community that would be more appropriate for this question, please let me know.  I honestly looked through them all, and the original SO proper seemed to be the most relevant.  Thanks! :) Have you tried Blekko? What do you mean with "I can find ZERO search API's that will allow us to fire off queries in a programmatic, non-user-initiated manner" ? There were a discussion around the Custom Search Engine of Google having the possibility to search the whole web (adding a site and removing it later). Also you can buy "credits" for the Custom Search Engine, although some user found a limitation even in that case. Anyway, I understand your point around the limitations of the current Search APIs, and Google is the best search engine, even if others compete nobody has a larger index. Thanks so much for your response, sw.  Prompted by your suggestion, I checked out Blekko, and their TOU is also quite restrictive.  (For the time being, however, there is a glimmer of hope for the Blekko API: dev-ops.net/2011/02/02/… ) Google's CSE won't work for us; we prefer a long-term legitimate solution rather than a short-term, legally questionable patch.  We have money and are willing to part with it! :)  Why are none of the big names willing to accomodate entities with legitimate business needs like ours? Well, I even wrote an article about it: blog.databigbang.com/google-search-no-api since there is a business opportunity there. I think in your case you must add a combination of [many] data sources, but it will not be straightforward to mix/clean/etc the data. I'll be interested on discussing it by chat since it's a very interesting subject. I am now on #bigdata on freenode.

A freely accessible index of 5 billion web pages, their page rank, their link graphs and other metadata, hosted on Amazon EC2.

http://commoncrawl.org/
Their Terms of Service (or TOU) are pretty reasonable and unrestricted too:
http://commoncrawl.org/about/terms-of-use/
 Haven't looked into this one much (it might satisfy the requirements, not sure); I thought I'd add it as a comment: 80legs.com
If you know some visual basic I'd suggest playing around with Bing Ad Intelligence. It's a free Excel plugin and all you need to use it is a free Microsoft account. 
The query limit is 20,000 words per query. You can get information on Clicks, Impressions, CTR, CPC, Average Bid and Total Cost. The query limit is a little lower if you use the more advanced keyword research features.
How can i cluster document using k-means (Flann with python)? 
I want to cluster documents based on similarity.
I haved tried ssdeep (similarity hashing), very fast but i was told that k-means is faster and flann is fastest of all implementations, and more accurate so i am trying flann with python bindings but i can't find any example how to do it on text (it only support array of numbers).
I am very very new to this field (k-means, natural language processing). What i need is speed and accuracy.
My questions are: 

Can we do document similarity grouping / Clustering using KMeans (Flann do not allow any text input it seems )
Is Flann the right choice? If not please suggest me High performance library that support text/docs clustering, that have python wrapper/API.
Is k-means the right algorithm?

 This is pretty much the same question as stackoverflow.com/questions/8057442/document-clustering-basics. I suggest you look at scikit-learn, which has most of the functionality you need built-in, and is pretty scalable: scikit-learn.org/stable/auto_examples/document_clustering.html. Alternatively, NLTK also has k-means: nltk.org/_modules/nltk/cluster/kmeans.html. Thanks a lot. How is performance scikit vs nltk ,  can you comment on them? I've never tried NLTK's clustering, but it's a safe bet to assume scikit-learn will be orders of magnitude faster for largish datasets. NLTK may be easier to use, though. If you need to do approximate k-NN queries, then FLANN is state of the art (there is no approximate k-NN query module in scikit-learn nor NLTK as far as I know). But k-NN queries and K-Means clustering are not solving the same problem.
You need to represent your document as an array of numbers (aka, a vector).  There are many ways to do this, depending on how sophisticated you want to be, but the simplest way is just to represent is as a vector of word counts.
So here's what you do:

Count up the number of times each word appears in the document.
Choose a set of "feature" words that will be included in your vector.  This should exclude extremely common words (aka "stopwords") like "the", "a", etc.
Make a vector for each document based on the counts of the feature words.

Here's an example.
If your "documents" are single sentences, and they look like (one doc per line):
there is a dog who chased a cat
someone ate pizza for lunch
the dog and a cat walk down the street toward another dog

If my set of feature words are [dog, cat, street, pizza, lunch], then I can convert each document into a vector:
[1, 1, 0, 0, 0]  // dog 1 time, cat 1 time
[0, 0, 0, 1, 1]  // pizza 1 time, lunch 1 time
[2, 1, 1, 0, 0]  // dog 2 times, cat 1 time, street 1 time

You can use these vectors in your k-means algorithm and it will hopefully group the first and third sentence together because they are similar, and make the second sentence a separate cluster since it is very different.
 Very interesting , i read somewhere a few days ago that scikit.learn have such feature to vectorize any text file or strings. I am wondering the data structure that it gives out will be suitable for Flann? I would just add that you could use some stemming algorithm to ensure you consider small variations of the same word as the same keyword. This will decrease the number of variables and should make the overall process more accurate. See this link for more info link yes that would be nice , i can use NLTK to produce / tokenize words. If I don't have any feature words and my bag of words is just "any word that's not a stopword" will each vector be the length of all possible words, with each indice representing the occurences of the word assigned to that indice? @Carpetfizz. That's true.
There is one big problem here:
K-means is designed for Euclidean distance.
The key problem is the mean function. The mean will reduce variance for Euclidean distance, but it might not do so for a different distance function. So in the worst case, k-means will no longer converge, but run in an infinite loop (although most implementations support stopping at a maximum number of iterations).
Furthermore, the mean is not very sensible for sparse data, and text vectors tend to be very sparse. Roughly speaking the problem is that the mean of a large number of documents will no longer look like a real document, and this way become dissimilar to any real document, and more similar to other mean vectors. So the results to some extend degenerate.
For text vectors, you probably will want to use a different distance function such as cosine similarity.
And of course you first need to compute number vectors. For example by using relative term frequencies, normalizing them via TF-IDF.
There is a variation of the k-means idea known as k-medoids. It can work with arbitrary distance functions, and it avoids the whole "mean" thing by using the real document that is most central to the cluster (the "medoid"). But the known algorithms for this are much slower than k-means.
 Thank you so much for pointing that out. any implementations of K-medoids that you recommend?How to use weka for predict results 
Im new to weka and I'm confused with the tool. What I needed to do is im having a data set about fruit price and relating attributes and im trying to predict the specific fruit price using the data set. Since I'm new to weka I couldn't figure out how to do this task. Please help me or guide me to a tutorial about how to do predictions and what is the best method or the algorithm to do this task.
Thank You.
 Give us some sample of your input and the desired result
If you want to know more about how to save a trained classifier and load the same later on to predict, please refer the following.
With the assumption that you want to use the Weka GUI, you have to go through 2 steps as below:

First using some pre-labelled data train a classifier[use your fruit prices data]. Make sure it is in ARFF format. After training save the model to your disk.
More on this can be found here: http://weka.wikispaces.com/Saving+and+loading+models
In the second step, you have use the already trained model[done in step 1]. Precisely you have to load the model file[saved in step 1] and then use the 'supplied test set" option on the "Classifiers" tab. In "supplied test set" option select the un-labelled data. 
More on this can be found here: http://weka.wikispaces.com/Making+predictions

I would suggest first playing around with the already provided ARFF data with your Weka install [these ARFF files basically sitting under your Weka install directory, In my case it is under: C:\Program Files\Weka-3-7\data]. 
Some more useful URLS:

http://www.ibm.com/developerworks/opensource/library/os-weka1/index.html 
http://ortho.clmed.ncku.edu.tw/~emba/2006EMBA_MIS/3_16_2006/WekaIntro.pdf

Hope that helps.
 Both of the wikispaces links are dead.Python, Scipy: Building triplets using large adjacency matrix 
I am using an adjacency matrix to represent a network of friends which can be visually interpreted as 
Mary     0        1      1      1

Joe      1        0      1      1

Bob      1        1      0      1

Susan    1        1      1      0 

         Mary     Joe    Bob    Susan

Using this matrix, I want to compile a list of all possible friendship triangles with the condition that user 1 is friends with user 2, and user 2 is friends with user 3.  For my list, it is not required that user 1 is friends with user 3.
(joe, mary, bob)
(joe, mary, susan)
(bob, mary, susan)
(bob, joe, susan)

I have a bit of code that works well with small triangles, but I need it to scale for very large sparse matrices.
from numpy import *
from scipy import *

def buildTriangles(G):
    # G is a sparse adjacency matrix
    start = time.time()
    ctr = 0
    G = G + G.T          # I do this to make sure it is symmetric
    triples = []
    for i in arange(G.shape[0] - 1):  # for each row but the last one
        J,J = G[i,:].nonzero()        # J: primary friends of user i
                                      # I do J,J because I do not care about the row values
        J = J[ J < i ]                # only computer the lower triangle to avoid repetition
        for j in J:
            K, buff = G[:,j].nonzero() # K: secondary friends of user i
            K = K[ K > i ]             # only compute below i to avoid repetition
            for k in K:
                ctr = ctr + 1
                triples.append( (i,j,k) )
    print("total number of triples: %d" % ctr)
    print("run time is %.2f" % (time.time() - start())
    return triples

I was able to run the code on a csr_matrix in approximately 21 minutes.  The matrix was 1032570 x 1032570 and contained 88910 stored elements. There were a total of 2178893 triplets generated.
I need to be able to do something similar with a 1968654 x 1968654 sparse matrix with 9428596 stored elements.  
I'm very new to python (little less than a month of experience) and not the greatest at linear algebra, which is why my code does not take advantage of matrices operations.
Can anyone make any suggestions for improvement or let me know if my objective is even realistic?  
 I don't think assigning the same value twice in a statement (J,J=) has any guaranteed meaning in Python. I find it very confusing and so do you, judging by your comment, so you might want to get rid of it. @larsmans My apologies.  nonzero() returns the indices of a matrix as a 2-d array.  Alternatively I could have done row, col = G[i,:].nonzero() and then J = col.  I used the J,J= approach because I was concerned about memory usage and wanted to eat up the row array, since it wasn't needed. Don't apologize, I didn't mean to be harsh. It's just not a Pythonic idiom and I think Guido is at lib to change the meaning of that construct between Python versions, so you can't rely on it working. It's better to del a variable if it really matters, although in this case J = G[i, :].nonzero()[1] will work too. Thanks for the suggestions.  It definitely cleaned up the code a bit.  The work you were doing with the Wikipedia articles is exactly what I am trying to do.  I will look more into a linear algebra approach to the problem.
I think you can find triangles only in rows or columns. for example:
Susan    1        1      1      0 
        Mary     Joe    Bob    Susan

this means Mary, Joe, Bob are all friends of Susan, so, use combinations to choose two person from [Mary, Joe, Bob], and combine it with Susan will get one triangle. itertools.combinations() do this quickly.
Here is the code:
import itertools
import numpy as np

G = np.array(   # clear half of the matrix first
    [[0,0,0,0],
     [1,0,0,0],
     [1,1,0,0],
     [1,1,1,0]])
triples = []     
for i in xrange(G.shape[0]):
    row = G[i,:]
    J = np.nonzero(row)[0].tolist() # combinations() with list is faster than NumPy array.
    for t1,t2 in itertools.combinations(J, 2):
        triples.append((i,t1,t2))
print triples

 Thanks for your answer.  I haven't even considered that approach, but it makes a lot of sense.  You basically reduce the problem to finding permutations of two.  Would all the triples be unique? @will: To clarify, do you mean that (Mary, Susan, Joe) and (Joe, Susan, Mary) are counted as distinct or identical? @Iterator I mean to count them as identical.  I believe that this method does work in this regard.  After looking at it further, I now realize that each new row is guaranteed to not have been in the earlier permutations. +1 to user772649.  This is great.  I want to find this function in other languages I work in.  I've always had to write it myself.
Here's some suggestions for optimization:
K = K[ K > i ]             # only compute below i to avoid repetition
for k in K:
    ctr = ctr + 1
    triples.append( (i,j,k) )

Don't increment in a loop, it's terribly slow. Just ctr += K.shape[0] will do. Then, eliminate the most deeply nested loop altogether by replacing the append with
triples += ((i, j, k) for k in K[K > i])

Now, if you want real performance on this task, you will have to get into some linear algebra. "I want to compile a list of all possible friendship triangles" means that you want to square the adjacency matrix, which you can do with a simple **2.
Then realize that 1.968.654² means a very big matrix, and even though it's very sparse, its square will be much less so and will take a lot of memory. (I once tackled a similar problem where I considered links between Wikipedia articles at distance two, which took 20 minutes to solve, on a supercomputer cluster node, in C++. This is not a trivial problem. The Wikipedia adjacency matrix was a few orders of magnitude denser, though.)
 When you mention "real performance" - can you elaborate on how you multiply two matrices and get a list (rather than a count) of 2-step pairings? @Iterator: Multiplying a square matrix with itself gives you a new matrix of the same rank that has value >0 for all i, j that are connected at step distance 2. Matrix multiplication is a heavily optimized operation in SciPy (implemented in C, I think, or maybe even Fortran). You can then extract the list yourself with much less searching in the matrix. Yes, you get the step 2 counts, which is what I said: you can get a count of the (i,*,k) pairs.  The identities of the intermediate j nodes are lost.  I understand (and stated) everything you said, but you haven't demonstrated any speedup of the naming of the full triplet.  I think you're not thinking this through all the way.What is Java Data Mining, JDM? 
I am looking at JDM. Is this simply an API to interact with other tools that do the actual data mining? Or is this a set of packages that contain the actual data mining algorithms?
 I guess the question is: Do any vendors with an interest in data mining (e.g., Oracle, IBM, etc.) have implementations of the spec that you can use?  If not, I'd agree with skaffman: Dead Duck.
Ah, the wonders of the interweb:

Java Data Mining (JDM) is a standard
  Java API for developing data mining
  applications and tools. JDM defines an
  object model and Java API for data
  mining objects and processes. JDM
  enables applications to integrate data
  mining technology for developing
  predictive analytics applications and
  tools. The JDM 1.0 standard was
  developed under the Java Community
  Process as JSR 73. As of 2006, the JDM
  2.0 specification is being developed under JSR 247.

Lists some implementations also, although it looks like it may be a dead duck.
 I know this is 1yr old question, but do you where can I get the jar for the JDM? There is no jar. It is a specification, and a dead one that noone ever used.
Wikipedia says:

Java Data Mining (JDM) is a standard Java API for developing data mining applications and tools. JDM defines an object model and Java API for data mining objects and processes.

According to this article and the JSR for JDM 2.0 (#247):

By extending the existing JDM standard with new mining functions and algorithms, data mining clients can be coded against a single API that is independent of the underlying data mining system. The goal of JDM is to provide for data mining systems what JDBCTM did for relational databases.

So it appears that, yes, JDM is an API to interact with other tools that do the actual mining.  It also appears that this JSR is currently inactive.
 "currently inactive" is a nice way of putting it. It's dead. Nobody seems to have ever used it.
AFAIK, this is just an API to link Java applications with data mining tools.  See the actual API and this whitepaper for more details.
Here's the package list from the JDM API (can't seem to find it online):

javax.datamining     This package contains Java classes and interfaces describing basic objects used in the other JDM packages.
javax.datamining.algorithm.feedforwardneuralnet  This package contains definitions for describing the algorithm settings specific to building a fully connected, n-layer, feed-forward neural network model.
javax.datamining.algorithm.kmeans    This package contains definitions for describing the algorithm settings specific to building a k-means clustering model.
javax.datamining.algorithm.naivebayes    This package contains definitions for describing the algorithm settings specific to building a Naive Bayes model.
javax.datamining.algorithm.svm   This package is Algorithm.SVM.
javax.datamining.algorithm.svm.classification    This package is Algorithm.SVM.Classification.
javax.datamining.algorithm.svm.regression    This package is Algorithm.SVM.Regression.
javax.datamining.algorithm.tree  This package contains definitions for describing the algorithm settings specific to building a decision tree model.
javax.datamining.association     This package contains Java classes describing the settings and model for the association mining function.
javax.datamining.attributeimportance     This package contains Java classes describing the settings and model for the attribute importance mining function.
javax.datamining.base    This package contains Java interfaces that are the top-level objects in JDM and consist of named objects as well as top-level base interfaces such as ModelDetail and AlgorithmSettings.
javax.datamining.clustering  This package contains Java classes describing the settings, model and apply output for clustering.
javax.datamining.data    This package contains Java classes describing mining data including the physical and logical aspects of data as well as the definition of matrix.
javax.datamining.modeldetail.feedforwardneuralnet    This package contains Java classes describing the algorithm-related details of a feed forward neural network model.
javax.datamining.modeldetail.naivebayes  This package contains Java classes describing the algorithm-related details of a Naive Bayes model.
javax.datamining.modeldetail.svm     This package is ModelDetail.SVM.
javax.datamining.modeldetail.tree    This package contains Java classes describing the algorithm-related details of a tree model.
javax.datamining.resource    This package contains Java classes used to create the connection to the data mining engine (DME) and maintain metadata objects associated with the DME.
javax.datamining.rule    This package contains Java interfaces representing the predicate rules created as part of the models such as tree model.
javax.datamining.statistics  This package contains Java classes describing the statistics of the input mining data.
javax.datamining.supervised  This package contains Java classes describing the generic settings, model, test task and test result of supervised learning.
javax.datamining.supervised.classification   This package contains Java classes describing the settings, model, and test task and result for classification.
javax.datamining.supervised.regression   This package contains Java classes describing the settings, model, and test task and result for regression mining function.
javax.datamining.task    This package contains Java classes that define build mining task, a primary mining operation that builds mining models.
javax.datamining.task.apply  This package contains Java classes that define the apply mining task for applying a mining model.

 If it's not online, it's almost certainly defunct. This comes as something of a relief, since I'd never heard of it. That was my thinking as well. Google results are only in the 1000s.
This project is inactive. It is an API. It does not provide algorithm implementations.
 Let's call it dead, not just inactive. And I fear, it never lived.Creating a comparable and flexible fingerprint of an object 
My situation
Say I have thousands of objects, which in this example could be movies.
I parse these movies in a lot of different ways, collecting parameters, keywords and statistics about each of them. Let's call them keys. I also assign a weight to each key, ranging from 0 to 1, depending on frequency, relevance, strength, score and so on.
As an example, here are a few keys and weights for the movie Armageddon:
"Armageddon"
------------------
disaster       0.8
bruce willis   1.0
metascore      0.2
imdb score     0.4
asteroid       1.0
action         0.8
adventure      0.9
...            ...

There could be a couple of thousands of these keys and weights, and for clarity, here's another movie:
"The Fast and the Furious"
------------------
disaster       0.1
bruce willis   0.0
metascore      0.5
imdb score     0.6
asteroid       0.0
action         0.9
adventure      0.6
...            ...

I call this a fingerprint of a movie, and I want to use them to find similar movies within my database.
I also imagine it will be possible to insert something other than a movie, like an article or a Facebook profile, and assign a fingerprint to it if I wanted to. But that shouldn't affect my question.
My problem
So I have come this far, but now comes the part I find tricky. I want to take the fingerprint above and turn it into something easily comparable and fast. I tried creating an array, where index 0 = disaster, 1 = bruce willis, 2 = metascore and their value is the weight.
It comes out something like this for my two movies above:
[ 0.8 , 1.0 , 0.2 , ... ]
[ 0.1 , 0.0 , 0.5 , ... ]

Which I have tried comparing in different ways, by just multiplying:
public double CompareFingerprints(double[] f1, double[] f2)
{
    double result = 0;

    if (f1.Length == f2.Length)
    {
        for (int i = 0; i < f1.Length; i++)
        {
            result += f1[i] * f2[i];
        }
    }

    return result;
}

or comparing:
public double CompareFingerprints(double[] f1, double[] f2)
{
    double result = 0;

    if (f1.Length == f2.Length)
    {
        for (int i = 0; i < f1.Length; i++)
        {
            result += (1 - Math.Abs(f1[i] - f2[i])) / f1.Length;
        }
    }

    return result;
}

and so on.
These have returned a very satisfying results, but they all have one problem in common: They work great for comparing two movies, but in reality, it's quite time consuming and feels like very bad practice when I want to compare a single movies fingerprint with thousands of fingerprints stored in my MSSQL database. Specially if it's supposed to work with things like autocomplete where I want to return the results in fractions of a second.
My question
Do I have the right approach here or am I reinventing the wheel in a really inefficient way? I hope my question isn't to broad for Stack Overflow, but I have narrowed it down with a few thoughts below.
A couple of thoughts

Should my fingerprint really be an array of weights?
Should I look into hashing my fingerprint? It might help with fingerprint storage, but complicate comparison. I have found some hints that this might be a valid approach, by using Locality-sensitive hashing, but the math is a bit out of my reach.
Should I fetch all thousands of movies from SQL and work with the result, or is there a way to implement my comparison into an SQL query and only return the top 100 hits?
Is sparse data representation something to look into? (Thanks Speed8ump)
Could I apply methods used when comparing actual fingerprints or for OCR?
I have heard that there is software that detects exam cheating by finding similarities in thousands of published papers and previous tests. What method do they use?

Cheers!
 since no one mentioned that, I guess it is wrong. but why not hashing ? speaking of tags (in one of the answers below), adding a tag for algorithm or similar might get the right eyeballs looking at this question. @NerfHerder The data-mining tag might also be appropriate. I've just started reading about it, but your fingerprinting sounds like you are trying to create a sparse data representation. @Mhd.Tahawi Magnus has sufficient rep to make a separate chat room where you can talk about it.
Alternative: Feature Vector
Whet you are describing is a classical feature vector. Each column in the feature vector describes a category. Your feature vector is a sepcial kind: It has fuzzy data, describing the degree of belonging to some category. 
When processing such vectors, you should apply fuzzy logic for the calculations. With fuzzy logic you have to play areound a little bit, until you find the best numericla operators to match your fuzzy operations. E.g. fuzzy AND and OR could be computed with "min" and "max" or with "*" and "+" or even with more complex exponential operations. You have to find the right balance between good results and fast computations.
Unfortunately fuzzy logic does not fit very well with SQL databases. If you go the fuzzy way, you should consider to hold all your data in memory and to use some kind of numerical processing acceleration (processor SIMD instructions, CUDA/OpenCL, FPGA, etc.). 
Alternative: Star / Snowflake Schema
A different approach is to build a classical data warehouse scheme. This fits well with modern SQL databases. They have nice accelerations to retrieve data from a medium sized data warehouse (up to a few billion records):

Materialized views (for data reduction)
(Compressed) bitmap indexes (for fast combining of multiple features)
Compressed storage (for fast transfer of huge amounts of date)
Pertitioning (to physically separete data according to their features)

To use these optimizations, you must first prepare your date.
Hierarchical dimensions
You should order your features hierarchical, according to the snowflake schema. When the data is ordered this way (and you have the according indexes), the database can use a new set of optimizations, e.g. bitmap filtering. 
Data organized in this way should be mainly read only. The database will need data structures that are very fast for special kinds of queries but are also very expensive to update. 
An example is a bitmap index. A bitmap index is a binary matrix. The rows of the matrix are the rows of one table in your database. The columns are the possible values of one row in this table. The entry in the matrix is 1, when the column in the according row in the table as the value according to the column of the matrix. Otherwise it is 0.
A bitmap index will be stored in a compressed binary format. For the database it is very easy to combine multiple bitmap indexes by using fast binary processing (by ANDing or ORing the binary values using in processor SIMD instructions or even OpenCL/CUDA, etc.).
There are special kind of bitmap indexes that can span multiple tables, so called bitmap join indexes. They are specially built for data organized in a snowflake schema.
Dimension reduction
You should also use dimension reduction to reduce the amount of features that must be stored. For this you can use techniques like principal component analysis. With this you can combine multiple highly coupled features to one artificial features and remove features completely that don't change their value at all.
Discrete dimension members
For fuzzy logic, using floating numbers is nice. But when storing data in a data warehouse, it is a good idea to reduce to possible values. Bitmap indexes and partitioning will only work with a limited amount of values. You can use classification algorithms to reach this, e.g. self organizing feature maps or particle swarm optimizations.
Alternative 3: Hybrid approach
You can easily combine the two approaches described above. You store the date in your data warehouse, using condensed descriptions (fewer dimensions, fewer members). Each data set contains the original features. When you retrieved the data sets from the data warehouse, you can then use the techniques from alternative 1 to operate with the full descriptions, e.g. to determine the top candidates for competition according to the current context.
 This looks VERY promising. I'll have a closer look at it, but it certainly looks like the correct answer, if there is such a thing.
Idea is cool, this way I can find all good (imdb > 5.5) movies with Bruce, where he play a main role (bruce willis > 0.9), which are actions (action > 0.5) and are not horrors (horror < 0.1). I hate horrors.
Your thoughts:

array of weights is bad, because if you get more and more keys, and if movie doesn't have this actor, then it still has to has a value (0), which is a waste of space (imagine million of keys attached to each movie).
hashing doesn't makes sense, as you are not going to access anything by exact value, you will always compare keys with user entered values and many of them will be optional (which means you don't care if they are 0 or 10).
depends, see below.

I think what you need here is a sort of Tag system (like SO one), where you can easily add new tags (to example, for new actors or when there will be something better than blue-ray or HD, etc). So a table with tag [id]-[name].
Then your movies have to have a field which stores a dictionary [id]-[score] of zero to million tags. This should be a blob (or is there any way to hold dictionary or array in SQL database?), or array (if your tag id starting from 0 and incremented by 1 you don't need key, but index).
When you are searching for movies, matching fingerprints conditions, you will have to read fingerprint from database for each movie. This should be slower than if SQL query would do it, but still ok (you will have maybe 100-1000 tags per movie, which makes it only a few KB to read), unless you have to transfer this data over network, then consider to use server application. Perhaps stored procedures can help.
 I agree with the main points.  I don't see how hashing would work with multiple keys like that.  From an space efficiency standpoint, using tags for actors (maybe locations) might be useful and keep the rating for keys (action, tear jerker, etc.) but that might make the fingerprinting more complicated. @NerfHerder, it will not, that's why I discourage from using it. While a tag system is great in many ways, in fact I have one in place already, I'm looking for something that can include other things than actors, genres, titles and so on. Weights is crucial, and as shown in my example, meta score and imdb rating may be two of many non-tag keys making up the fingerprint.
Fingerprint Format
Regarding your 1st question, whether you should use an array of weights, that comes down to the level of detail you want. An array of weights will offer the highest fingerprint "resolution", for lack of a better term; it allows for a much more fine-grained measurement of how similar any two given movies are. Sinatr's suggestion of using tags in place of weights has a lot of optimization potential, but it essentially limits you to weights of 0 or 1, and thus has trouble representing existing weights in the 0.3-0.7 range. You'll have to decide for yourself whether the performance gains of going to a representation with less detail outweigh the reduced comparison accuracy those representations have.
Hashes
Regarding your 2nd question, I'm afraid I can't offer much guidance. I'm not familiar with the use of hashing in this sort of context, but I don't see how you could compare them easily; the whole point of hashes in most uses is that they can't easily be reversed to learn about the original input.
SQL Optimization
For your 3rd question, the SQL query you use to get comparison candidates is probably a rich source of performance optimization potential, especially if you know some characteristics of your fingerprints. In particular if high weights or low weights are relatively rare, then you can use those to weed out a lot of poor candidates. For example, if you were using movies you would expect a lot of the weights to be 0 (most movies do not contain Bruce Willis). You could look at any weights in your candidate movie that are higher than .8 or so (you'll need to do some fine-tuning to determine the exact values that work well for your data set) and then have your SQL query exclude results that have a 0 in at least some fraction of those keys (again, the fraction will need fine-tuning). This allows you to quickly discard results that are unlikely to be good matches in the SQL query stage rather than doing a full (expensive) comparison on them.
Other Options
Another approach that might work, depending on how often the fingerprints of your objects change, is to pre-compute the fingerprint comparison values. Then getting the best candidates is a single query from an indexed table: SELECT id1, id2, comparison FROM precomputed WHERE (id1 = foo OR id2 = foo) AND comparison > cutoff ORDER BY comparison DESC. Pre-computing the comparisons for a new object would be part of the process of adding it, so if being able to quickly add objects is a priority then this approach may not work well. Alternately, you could simply cache values once you've computed them, rather than pre-computing them. This doesn't do anything for the initial search, but later searches reap the benefits and adding objects stays cheap.
 I really like your SQL insights, which seems to be close to a final approach. I'm evaluating it right now.
I think hashing is what you are looking for, a hash table gives you O(1) for insertion, deletion and search.
I had a similar situation where I had to hash an array of eight distinct integers. I used the following  code from the C++ boost library.
size_t getHashValue ()const{

        size_t seed = 0;
        for (auto  v : board)
            seed ^= v + 0x9e3779b9 + (seed << 6) + (seed >> 2);

        return seed;


    }

my array was called board and this is the syntax for a foreach loop in C++, size_t is just an unsigned integer and the rest is the same as in C#.
note since I had distinct values, I can easily use the value itself as a hash function, that way I can guarantee a distinct hash value for each element in my array.
since that is not your case, you would need to change your code to include the hash of each entry in your array to build the hash of the entire array as follows:
foreach (float entry in array)
    // hashOf is something you would need to do 
    seed ^= hashOf(entry) + 0x9e3779b9 + (seed << 6) + (seed >> 2); 

in case your entries  have only one digit after the decimal point, you can multiply by 10 and move your problem to the integer domain.
Hope this helps.
EDIT:
see this question for hashing decimal values: C# Decimal.GetHashCode() and Double.GetHashCode() equal.
the performance of this approach relays on the hashing function, the more equal the probability distribution of your function, the better performance you get.
but IMHO hash table is the best you can get see this
 Although hashing will work great for 'a perfect match', it will not help out at all to find a 'near match'. One of the properties of hashing algorithm is that even the smallest difference will cause a totally different hash. I like the concept of hashing for fingerprint storage, but it may complicate comparison. I don't think, as @deroby points out, that I can find partial matches when comparing two hashed fingerprints?No. of hidden layers, units in hidden layers and epochs till Neural Network starts behaving acceptable on Training data 
I am trying to solve this Kaggle Problem using Neural Networks. I am using Pybrain Python Library.
It's a classical supervised Learning Problem. In following code: 'data' variable is numpy array(892*8). 7 fields are my features and 1 field is my output value which can be '0' or '1'.
from pybrain.datasets import ClassificationDataSet
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.tools.shortcuts import buildNetwork

dataset = ClassificationDataSet(7,1)
for i in data:
    dataset.appendLinked(i[1:],i[0])
net = buildNetwork(7,9,7,1, bias = True,hiddenclass = SigmoidLayer, outclass = TanhLayer)
trainer = BackpropTrainer(net, learningrate = 0.04, momentum = 0.96, weightdecay = 0.02, verbose = True)
trainer.trainOnDataset(dataset, 8000)
trainer.testOnData(verbose = True)

After training my Neural Network, when I am testing it on Training Data, its always giving a single output for all inputs. Like:
Testing on data:
out:     [  0.075]
correct: [  1.000]
error:  0.42767858
out:     [  0.075]
correct: [  0.000]
error:  0.00283875
out:     [  0.075]
correct: [  1.000]
error:  0.42744569
out:     [  0.077]
correct: [  1.000]
error:  0.42616996
out:     [  0.076]
correct: [  0.000]
error:  0.00291185
out:     [  0.076]
correct: [  1.000]
error:  0.42664586
out:     [  0.075]
correct: [  1.000]
error:  0.42800026
out:     [  0.076]
correct: [  1.000]
error:  0.42719380
out:     [  0.076]
correct: [  0.000]
error:  0.00286796
out:     [  0.076]
correct: [  0.000]
error:  0.00286642
out:     [  0.076]
correct: [  1.000]
error:  0.42696969
out:     [  0.076]
correct: [  0.000]
error:  0.00292401
out:     [  0.074]
correct: [  0.000]
error:  0.00274975
out:     [  0.076]
correct: [  0.000]
error:  0.00286129

I have tried altering learningRate, weightDecay, momentum, number of hidden units, number of hidden layers, class of hidden layers, class of output layers so as resolve it, but in every case it gives same output for every input if input comes from Training Data.
I think I should run it more than 8000 times because when I was building Neural Network for 'XOR', It took atleast 700 iterations before it started giving errors on nano scale. Training data size on 'XOR' was only 4 whereas in this case it is 892. So I ran 8000 iterations on 10 % of the original data(Now size of Training Data is 89), even then it was giving same output for every input in Training Data. And since I want to classify input into '0' or '1', if I'm using class of Output Layer to be Softmax, then it is always giving '1' as output.
No matter which configuration(no. of hidden units, class of output layer, learning rate, class of hidden layer, momentum), was I using in 'XOR', it more or less started converging in every case.
Is is possible that there is some configuration that will finally yield lower error rates. Atleast some configuration so that it won't give same output for all inputs in Training Data.
I ran it for 80,000 iteration(Training Data Size is 89). Output Sample:
Testing on data:
out:     [  0.340]
correct: [  0.000]
error:  0.05772102
out:     [  0.399]
correct: [  0.000]
error:  0.07954010
out:     [  0.478]
correct: [  1.000]
error:  0.13600274
out:     [  0.347]
correct: [  0.000]
error:  0.06013008
out:     [  0.500]
correct: [  0.000]
error:  0.12497886
out:     [  0.468]
correct: [  1.000]
error:  0.14177601
out:     [  0.377]
correct: [  0.000]
error:  0.07112816
out:     [  0.349]
correct: [  0.000]
error:  0.06100758
out:     [  0.380]
correct: [  1.000]
error:  0.19237095
out:     [  0.362]
correct: [  0.000]
error:  0.06557341
out:     [  0.335]
correct: [  0.000]
error:  0.05607577
out:     [  0.381]
correct: [  0.000]
error:  0.07247926
out:     [  0.355]
correct: [  1.000]
error:  0.20832669
out:     [  0.382]
correct: [  1.000]
error:  0.19116165
out:     [  0.440]
correct: [  0.000]
error:  0.09663233
out:     [  0.336]
correct: [  0.000]
error:  0.05632861

Average error: 0.112558819082
('Max error:', 0.21803000849096299, 'Median error:', 0.096632332865968451)
It's giving all outputs within range(0.33, 0.5).
 Why doesn't the reported error match |out-correct|? Also, have you tried training for much more than 8000 iterations? What's the highest number of hidden nodes you have tried? I have tried 2 hidden layers with 11 nodes in each. It was not working upto 8000 iterations. Right now I am using 2 hidden layers with 9 and 7 nodes and running 80,000 iteration, its been running for around 3 hours, will report results once its completed, although looking at Total Error, I don't really think that it'll be any better. I am sorry, I have no idea why reported error is not matching |out-correct|. I have updated the question with results of running it for 80,000 iteration. Seems like a clear improvement. I get the impression though that the weights start really low, possibly at zero, and that would mean that the labels in your dataset should be -1 and 1, instead of 0 and 1. Another thing you could do is to print the average error after every x training iterations, to see how it evolves. Try to start with learning rate 0.2-0.4 with decrease in time proportional to current epoch (LR * (N - i)/N). What are the bounds? I suppose they are different for different features? Then normalize them to be within the same range. Use either Sigmoid or Tanh layers in all net. If you'll use Sigmoid - leave 0 and 1. If you'll use Tanh - use [-1, +1] as outputs.
There is yet another neural network metric, which you did not mention - number of adaptable weights. I'm starting the answer from this because it's related to the numbers of hidden layers and units in them.
For good generalization, number of weights must be much less Np/Ny, where Np is a number of patterns and Ny is a number of net outputs. What is the "much" exactly is discussible, I suggest several times difference, say 10. For approximately 1000 patterns and 1 output in your task this will imply 100 weights.
It does not make sense to use 2 hidden layers. 1 is sufficient for most of tasks where non-linearity involved. In your case, the additional hidden layer makes only the difference by impacting overall perfomance. So if 1 hidden layer is used, number of neurons in it can be approximated as number of weights divided by number of inputs, that is 100/7 = 14.
I suggest to use the same activation function in all neurons, either Hypertanh or Sigmoid everywhere. Your output values are actually already normalized for Sigmoid. Anyway, you can  improve NN performance by input data normalization to fit into [0,1] in all dimentions. Of course, normalize each feature on its own.
If you can do with the Pybrain lib, start learning with greater learning rate and then decrease it smoothly proportional to current step (LR * (N - i)/N), where i is current step, N - is a limit, LR - initial learning rate.
As @Junuxx suggested, output current error every M steps (if this possible) just to make sure your program works as expected. Stop learning if the difference in errors in successive steps becomes less than a threshold. Just for beginning and rough estimation of the proper NN parameters choosing set the threshold to 0.1-0.01 (there is no need in "nano scale").
The fact of running a network on 89 patterns in 80000 steps and getting the results your have is strange. Please, double check you pass correct data to the NN, and please examine what does the error values you provided mean. Possibly, either the errors, or outputs displayed are taken from wrong place. I think 10000 steps must be far enough to get acceptable results for 89 patters.
As for the specific task, I think SOM net could be another option (possily better suited than BP).
As a sidenote, I'm not familiar with Pybrain, but have coded some NNs in C++ and other languages, so your timing looks highly outsized.
 I started with a learning rate of 0.2 and after every iteration it got  multiplied by 0.999 for 20,000 iterations, still outputs were same. I have checked data again, its correct. @JackSmith, Well, then it can be something Pybrain specific, which is out of my scope. Still you should try to understand what are the errors. Try pure BP without softmax. Did you try input data normalization? I was trying without Softmax earlier, With Softmax, every output is '1'. And normalization didn't work. Please, provide an excerpt of input data (actual vectors that you pass into the net) in your question. Try another NN tool if possible, for example, neuroph.sourceforge.net. I hope someone could tell what is wrong with Pybrain setup for the given task, and how to interpet your error values. Data Slice:                                                      array([['1', '2', '0', '55', '0', '0', '16', '1'],                            ['0', '3', '1', '2', '4', '1', '29.125', '2'],                         ['1', '2', '1', '28.0', '0', '0', '13', '1'],                          ['0', '3', '0', '31', '1', '0', '18', '1'],                            ['1', '3', '0', '28.0', '0', '0', '7.225', '0'],                       ['0', '2', '1', '35', '0', '0', '26', '1'],                       Output value is at index 0 and rest is my feature vector.Using adaboost within R's caret package 
I've been using the ada R package for a while, and more recently, caret. According to the documentation, caret's train() function should have an option that uses ada.  But, caret is puking at me when I use the same syntax that sits within my ada() call.
Here's a demonstration, using the wine sample data set.
library(doSNOW)
registerDoSNOW(makeCluster(2, type = "SOCK"))
library(caret)
library(ada)

wine = read.csv("http://www.nd.edu/~mclark19/learn/data/goodwine.csv")


set.seed(1234) #so that the indices will be the same when re-run
trainIndices = createDataPartition(wine$good, p = 0.8, list = F)
wanted = !colnames(wine) %in% c("free.sulfur.dioxide", "density", "quality",
                            "color", "white")

wine_train = wine[trainIndices, wanted]
wine_test = wine[-trainIndices, wanted]
cv_opts = trainControl(method="cv", number=10)


 ###now, the example that works using ada() 

 results_ada <- ada(good ~ ., data=wine_train, control=rpart.control
 (maxdepth=30, cp=0.010000, minsplit=20, xval=10), iter=500)

##this works, and gives me a confusion matrix.

results_ada
     ada(good ~ ., data = wine_train, control = rpart.control(maxdepth = 30, 
     cp = 0.01, minsplit = 20, xval = 10), iter = 500)
     Loss: exponential Method: discrete   Iteration: 500 
      Final Confusion Matrix for Data:
      Final Prediction
      etc. etc. etc. etc.

##Now, the calls that don't work. 

results_ada = train(good~., data=wine_train, method="ada",
control=rpart.control(maxdepth=30, cp=0.010000, minsplit=20, 
xval=10), iter=500)
   Error in train.default(x, y, weights = w, ...) : 
   final tuning parameters could not be determined
   In addition: Warning messages:
   1: In nominalTrainWorkflow(dat = trainData, info = trainInfo, method = method,  :
    There were missing values in resampled performance measures.
   2: In train.default(x, y, weights = w, ...) :
    missing values found in aggregated results

 ###this doesn't work, either

results_ada = train(good~., data=wine_train, method="ada", trControl=cv_opts,
maxdepth=10, nu=0.1, iter=50)

  Error in train.default(x, y, weights = w, ...) : 
  final tuning parameters could not be determined
  In addition: Warning messages:
  1: In nominalTrainWorkflow(dat = trainData, info = trainInfo, method = method,  :
    There were missing values in resampled performance measures.
  2: In train.default(x, y, weights = w, ...) :
   missing values found in aggregated results

I'm guessing it's that train() wants additional input, but the warning thrown doesn't give me any hints on what's missing. Additionally, I could be missing a dependency, but there's no hint on what should be there....

Look up ?train and search for ada you'll see that:
Method Value: ada from package ada with tuning parameters: iter, maxdepth, nu (classification only)
So you must be missing the nu parameter, and the maxdepth parameter.
 take a look at the last train() call I made -- it includes all the parameters you mentioned. results_ada = train(good~., data=wine_train, method="ada", trControl=cv_opts, maxdepth=10, nu=0.1, iter=50) also, I tried taking out trControl=cv_opts, but there was no difference. Still got the error.
So this seems to work:
wineTrainInd <- wine_train[!colnames(wine_train) %in% "good"]
wineTrainDep <- as.factor(wine_train$good)

results_ada = train(x = wineTrainInd, y = wineTrainDep, method="ada")

results_ada
Boosted Classification Trees 

5199 samples
   9 predictors
   2 classes: 'Bad', 'Good' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 5199, 5199, 5199, 5199, 5199, 5199, ... 

Resampling results across tuning parameters:

  iter  maxdepth  Accuracy  Kappa  Accuracy SD  Kappa SD
  50    1         0.732     0.397  0.00893      0.0294  
  50    2         0.74      0.422  0.00853      0.0187  
  50    3         0.747     0.437  0.00759      0.0171  
  100   1         0.736     0.411  0.0065       0.0172  
  100   2         0.742     0.428  0.0075       0.0173  
  100   3         0.748     0.442  0.00756      0.0158  
  150   1         0.737     0.417  0.00771      0.0184  
  150   2         0.745     0.435  0.00851      0.0198  
  150   3         0.752     0.449  0.00736      0.016   

Tuning parameter 'nu' was held constant at a value of 0.1
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were iter = 150, maxdepth = 3 and nu
 = 0.1.

And the reason is found in another question:
caret::train: specify model-generation-parameters
I think you passed tuning parameters as arguments, when train is attempting to find optimal tuning parameters itself. You could define a grid of parameters for a grid search if you did want to define your own.

What is the type of data in wine$good? If it is a factor, try explicitly mentioning that it is so:
wine$good <- as.factor(wine$factor)
stopifnot(is.factor(wine$good))

Reason : often, R packages need some help in distinguishing classification vs. regression scenarios, and there may be some generic code inside caret which may be mistakenly identifying the exercise as a regression problem (,ignoring the fact that ada does only classification).
 I tried your suggestion (explicitly starting wine as a factor), but I still get the error... does a reproducible example of the above work on your system? Finally got around to trying it and sorry, I get the same error as you and could not figure it out. method="rf" works fine, but I suppose that is of no consolation, i.e., you really want method="ada". Aha, train(up ~ ., data=sym[,c(6, 14)], "ada"), without any suggestions about parameters, works! It seems that, Tuning parameter 'nu' was held constant at a value of 0.1 Accuracy was used to select the optimal model using  the largest value. The final values used for the model were iter = 50, maxdepth = 1 and nu = 0.1. Can you post your reproducible example? Also, what if I do want to pass parameter values?
Please include the parameters within tuneGrid
Grid <- expand.grid(maxdepth=25,nu=2,iter=100)
results_ada = train(good~., data=wine_train, method="ada",
trControl=cv_opts,tuneGrid=Grid)

This will work.
Algorithm to handle data aggregation from multiple error-prone sources 
I'm aggregating concert listings from several different sources, none of which are both complete and accurate. Some of the data comes from users (such as on last.fm), and may be incorrect. Other data sources are highly accurate, but may not contain every event. I can use attributes such as the event date, and the city/state to try to match listings from disparate sources. I'd like to be reasonably certain that the events are valid. It seems like it would be a good strategy to consume as many different sources as possible to validate listings on error-prone sources.
I'm not sure what the technical term for this is, as I'd like to research it further. Is it data mining? Are there any existing algorithms? I understand a solution will never be completely accurate.

I believe the term you are looking for is Record Linkage - 

the process of bringing together two or more records relating to the same entity(e.g., person, family, event, community, business, hospital, or geographical area)

This presentation (PDF) looks like a nice introduction to the field. One algorithm you might use is Fellegi-Holt - a statistical method for editing records.
 link to pdf you gave is no longer valid
Here is an approach that locates it within statistics - specifically, it uses a Hidden Markov Model (http://en.wikipedia.org/wiki/Hidden_Markov_model):
1) Use your matching process to produce a cleaned list of possible events. Consider each event to be marked "true" or "bogus", even though the markings are hidden from you. You might imagine that some source of events produces them, generating them as either "true" or "bogus" according to a probability which is an unknown parameter.
2) Associate unknown parameters with each source of listings. These give the probability that this source will report a true event produced by the source of events, and the probability that it will report a bogus event produced by the source.
3) Notice that if you could see the markings of "true" or "bogus" you could easily work out the probabilities for each source. Unfortunately, of course, you can't see these hidden markings.
4) Let's call these hidden markings "Latent Variables" because then you can use the http://en.wikipedia.org/wiki/Em_algorithm to hillclimb to promising solutions for this problem, from random starts.
5) You can obviously make the problem more complicated by dividing events up into classes, and giving sources of listing parameters which make them more likely to report some classes of events than others. This might be useful if you have sources that are extremely reliable for some sorts of events.
 Very thorough, but perhaps a bit over my head. Appreciate it though.
One potential search term is "fuzzy logic".
I'd use a float or double to store a probability (0.0 = disproved ... 1.0 = proven) of some event details being correct.  As you encounter sources, adjust the probabilities accordingly.  There's a lot for you to consider though:

attempting to recognise when multiple sources have copied from each other and reduce their impact
giving more weight to more recent data or data that explicitly acknowledges the old data (e.g. given a 100% reliable site saying "concert X to be held on 4th August", and a unknown blog alleging "concert X moved from 4th August to 9th", you might keep the probability of there being such a concert at 100% but have a list with both dates and whatever probabilities you think appropriate...)
beware assuming things are discrete; contradictory information may reflect multiple similar events, dual billing, same-surnamed performers etc. - the more confident you are that the same things are referenced, the more the data can combined to reinforce or negate each other
you should be able to "backtest" your evolving logic by using data related to a set of concerts where you now have full knowledge of their actual staging or lack thereof; process data posted before various cut-off dates prior to the events to see how the predictions you derive reflect the actual outcomes, tweak and repeat (perhaps automatically)

It may be most practical to start scraping from the sites you have, then consider the logical implications of the types of information you're seeing.  Which aspects of the problem need to be handled using fuzzy logic can then be decided.  An evolutionary approach may mean reworking things, but may end up faster than getting bogged down in a nebulous design phase.
 Thankfully, I only have to detect duplicates and use multiple sources for verification. Normalization of venues will be a big problem, however.
Data mining is about finding information from structured sources like a database, or a post where the fields are separated for you. There's some text mining in here when you have to parse the information out of free text. In either case, you could keep track of how many data sources agree on a show as a confidence measure. Either display the confidence measure or use it to decide if your data is good enough. There's lots to play with. Having a list of legitimate cities, venues and acts can help you decide if a string represents a legitimate  entity. Your lists might even be in a database that lets you compare city and venue for consistency.
Removing “almost duplicate” strings in subquadratic time 
I'm trying to do machine learning on a real-life dataset (hotel reviews). Unfortunately, it's plagued by spam, which comes in the form of almost identical reviews, complicating matters for me greatly.
I would like to remove "almost duplicates" from the dataset based on the edit distance or something similar, and since the dataset size is >100K, the algorithm has to be subquadratic in the size of the dataset. Right now I can only think of flagging individual sentences or phrases that are repeated too often and then removing all reviews that have them, but it's easy to see how such strategy could backfire. Is there a common algorithm that does better?
 This smells like a bulk of nearest neighbor queries, albeit with an unusual distance metric (I don't think edit distance satisfies the triangle inequality). I suggest looking into the usual data structures for speeding up nearest neighbor searches. What about manually seeding a bayesian filter with some of the selected spam entries? what is "subquadratic"? This is the "near duplicates" detection problem. One common technique is called shingling. If you search on these terms, you should find some useful algorithms. Locality-Sensitive Hashing (LSH) seems an appealing heuristic, see for example Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions. But I can only second ElKamina that "solving this problem in whole might involve writing a decent research paper." The basic idea is that with LSH, similar entries (w.r.t. a threshold) are likely to end up in the same bin, but entries sufficiently different end up in different bins.
Obviously solving this problem in whole might involve writing a decent research paper. Here is my suggestion.
In bioinformatics we face this problem all the time. The most used algorithm is BLAST (http://en.wikipedia.org/wiki/BLAST). Please go through the algorithm and you might get an idea of what is involved.

A quick and dirty way of doing this is to find the key words that occur in the reviews and store them in universal dictionary and then scan each document for those words. Make a hash table of key words for each documents. Then compare all pairs of documents then evaluate the count of similar key words in each pair and then if it is greater than a threshold then mark them as similar, you can use fast union find datastructure for finding unions of two documents if similar. At the end you will have sets of similar documents. 
Note: I cant think of any way to make it subquadratic but i seems difficult because you need to check all pairs of documents in worst case if you need to find if there are similar ones.     
what is the bootstrapped data in data mining? 
recently I came across this term,but really have no idea what it refers to.I've searched online,but with little gain.
Thanks.

If you don't have enough data to train your algorithm you can increase the size of your training set by (uniformly) randomly selecting items and duplicating them (with replacement).

Take a sample of the time of day that you wake up on Saturdays. Some Friday nights you have a few too many drinks, so you wake up early (but go back to bed). Other days you wake up at a normal time. Other days you sleep in. 
Here are the results:
[3.1, 4.8, 6.3, 6.4, 6.6, 7.3, 7.5, 7.7, 7.9, 10.1]
What is the mean time that you wake up?
Well it's 6.8 (o'clock, or 6:48). A touch early for me. 
How good a prediction is this of when you'll wake up next Saturday? Can you quantify how wrong you are likely to be? 
It's a pretty small sample, and we're not sure of the distribution of the underlying process, so it might not be a good idea to use standard parametric statistical techniques†. 
Why don't we take a random sample of our sample, and calculate the mean and repeat this? This will give us an estimate of how bad our estimate is.
I did this several times, and the mean was between 5.98 and 7.8
This is called the bootstrap, and it was first mentioned by Bradley Efron in 1979. 
A variant is called the jackknife, where you sample all but one of your dataset, take the mean, and repeat. The jackknife mean is 6.8 (same as the arithmetic mean) and ranges from 6.4 to 7.2.
Another variant is called k-fold cross-validation, where you (at random) split your data set into k equally-sized sections, calculate the mean of all but one section, and repeat k times. The 5-fold cross-validation mean is 6.8 and ranges from 4 to 9.
† This distribution does happen to be Normal. The 95% confidence interval of the mean is 5.43 to 8.11, reasonably close but bigger than the bootstrap mean. 
 Any critical papers/thoughts on possible bias introduced by bootstrapping? I would take the time to read the original: stat.cmu.edu/~fienberg/Statistics36-756/Efron1979.pdf
In machine learning bootstrapping is iterative training on a known set. http://en.wikipedia.org/wiki/Bootstrapping_(machine_learning)
 it's not super helpful to just provide a link to wikipedia. it's pretty easy to find on your own :)what is the difference between Association rule mining & frequent itemset mining 
i am new to data mining and confuse about Association rules and frequent item mining. for me i think both are same but i need views from experts on this forum
My question is 
what is the difference between Association rule mining & frequent itemset mining?
Thanks

An association rule is something like "A,B → C", meaning that C tends to occur when A and B occur. An itemset is just a collection such as "A,B,C", and it is frequent if its items tend to co-occur. The usual way to look for association rules is to find all frequent itemsets and then postprocess them into rules.

The input of frequent itemset mining is :

a transaction database
a minimum support threshold  minsup

The output is :

the set of all itemsets appearing in at least minsup transactions.   An itemset is just a set of items that is unordered.

The input of assocition rule mining is :

a transaction database
a minimum support threshold  minsup
a minimum confidence threshold  minconf

The output is :

the set of all valid association rule. An association rule X-->Y is a relationship between two itemsets X and Y such that X and Y are disjoint and are not empty.  A valid rule is a rule having a support higher or equals to minsup and a confidence higher or equal to minconf.  The support is defined as  sup(x-->Y) = sup (X U Y) / (number of transactions). The confidence is defined as conf(x-->Y) = sup (X U Y) /  sup (X).

Now the relationship between itemset and association rule mining is that it is very efficient to use the frequent itemset to generate rules (see the paper by Agrawal 1993) for more details about this idea.  So association rule mining will be broken down into two steps:
- mining frequent itemsets
- generating all valid association rules by using the frequent itemsets.

Frequent itemset mining is the first step of Association rule mining. 
Once you have generated all the frequent itemsets, you proceed by iterating over them, one by one, enumerating through all the possible association rules, calculate their confidence, finally, if the confidence is > minConfidence, you output that rule. 

Frequent itemset mining is a step of Association rules mining. After applying Frequent itemset mining algorithm like Apriori, FPGrowth on data, you will get frequent itemsets. From these
discovered frequent itemsets, you will generate association rules(Usually done by subset generation).

By using Association rule mining we will get the frequently itemsets that present in the given dataset. it also provide different types of algorithms for mining the frequent itemsets but it is done in different way that means either horizontal or vertical format. Apriori algorithm follow the horizontal format for mining the frequent itemsets and eclat algorithm follow the vertical format for mining the frequent datasets.

Association Rule mining:
Association rule mining is used to find the patterns in data.it finds the features which occur together and correlated.

Example:

For example, people who buy diapers are likely to buy baby powder. Or we can rephrase the statement by saying: If (people buy diaper), then (they buy baby powder). Note the if, then rule. This does not necessarily mean that if people buy baby powder, they buy diaper. In General, we can say that if condition A tends to B it does not necessarily mean that B tends to A.
Frequent item set mining:
Frequent item set mining is used to find the common item sets in data. it can generate association rules from the given transactional datasets.

Example:

If there are 2 items X and Y purchased frequently then its good to put them together in stores or provide some discount offer on one item on purchase of other item. This can really increase the sales. For example it is likely to find that if a customer buys Milk and bread he/she also buys Butter.
So the association rule is [‘milk]^[‘bread’]=>[‘butter’]. So seller can suggest the customer to buy butter if he/she buys Milk and Bread.
clustering very large dataset in R 
I have a dataset consisting of 70,000 numeric values representing distances ranging from 0 till 50, and I want to cluster these numbers; however, if I'm trying the classical clustering approach, then I would have to establish a 70,000X70,000 distance matrix representing the distances between each two numbers in my dataset, which won't fit in memory, so I was wondering if there is any smart way to solve this problem without the need to do stratified sampling?
I also tried bigmemory and big analytics libraries in R but still can't fit the data into memory
 Is this solution (using cluster::clara) relevant/useful? no not really cause the problem is that the distance matrix will be too large to fit into any memory
You can use kmeans, which normally suitable for this amount of data, to calculate an important number of centers (1000, 2000, ...) and perform a hierarchical clustering approach on the coordinates of these centers.Like this the distance matrix will be smaller.
## Example
# Data
x <- rbind(matrix(rnorm(70000, sd = 0.3), ncol = 2),
           matrix(rnorm(70000, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")

# CAH without kmeans : dont work necessarily
library(FactoMineR)
cah.test <- HCPC(x, graph=FALSE, nb.clust=-1)

# CAH with kmeans : work quickly
cl <- kmeans(x, 1000, iter.max=20)
cah <- HCPC(cl$centers, graph=FALSE, nb.clust=-1)
plot.HCPC(cah, choice="tree")

 using your method and after running cah <- HCPC(cl$centers, graph=FALSE, nb.clust=-1) I get this error : Error in catdes(data.clust, ncol(data.clust), proba = proba, row.w = res.sauv$call$row.w.init) :    object 'data.clust' not found
70000 is not large. It's not small, but it's also not particularly large... The problem is the limited scalability of matrix-oriented approaches.
But there are plenty of clustering algorithms which do not use matrixes and do no need O(n^2) (or even worse, O(n^3)) runtime.
You may want to try ELKI, which has great index support (try the R*-tree with SortTimeRecursive bulk loading). The index support makes it a lot lot lot faster.
If you insist on using R, give at least kmeans a try and the fastcluster package. K-means has runtime complexity O(n*k*i) (where k is the parameter k, and i is the number of iterations); fastcluster has an O(n) memory and O(n^2) runtime implementation of single-linkage clustering comparable to the SLINK algorithm in ELKI. (The R "agnes" hierarchical clustering will use O(n^3) runtime and O(n^2) memory).
Implementation matters. Often, implementations in R aren't the best IMHO, except for core R which usually at least has a competitive numerical precision. But R was built by statisticians, not by data miners. It's focus is on statistical expressiveness, not on scalability. So the authors aren't to blame. It's just the wrong tool for large data.
Oh, and if your data is 1-dimensional, don't use clustering at all. Use kernel density estimation. 1 dimensional data is special: it's ordered. Any good algorithm for breaking 1-dimensional data into inverals should exploit that you can sort the data.

Another non-matrix oriented approach, at least for visualizing cluster in big data, is the largeVis algorithm by Tang et al. (2016). The largeVis R package has unfortunately been orphaned on CRAN due to lacking package maintenance, but a (maintained?) version can still be compiled from its gitHub repository via (having installed Rtools), e.g.,
library(devtools)     
install_github(repo = "elbamos/largeVis")

A python version of the package exists as well. The underlying algorithm uses segmentation trees and a neigbourhood refinement to find the K most similar instances for each observation and then projects the resulting neigbourhood network into dim lower dimensions. Its been implemented in C++ and uses OpenMP (if supported while compiling) for multi-processing; it has thus been sufficiently fast for clustering any larger data sets I have tested so far.
How to test if a kernel is a valid kernel 
If I define my own method of determining the similarity between two input entities of my Support Vector Machine classifier, and thus define it as my kernel, how do I verify if it is indeed a valid kernel that I can use?
For example, if my inputs are strings, and the kernel I choose is lets say some kind of a string distance metric, how can I decide if I can use it or not for my SVM. I know there are some conditions for a valid SVM kernel. Can anyone tell me what they are and how does one go about verifying those conditions?

Kernel functions must satisfy Mercer's condition  You can also find answers to what you are asking on the  stats forum.
 Thanks. The link to the other post is very helpful.
The most straight forward test is based on the following:  A kernel function is valid if and only if the kernel matrix for any particular set of data points has all non-negative eigenvalues.  You can easily test this by taking a reasonably large set of data points and simply checking if it is true.  For example, if you selected 2000 data samples at random, created their corresponding 2000x2000 kernel matrix, and observed that it had non-negative eigenvalues, then it is highly likely that you have a legit kernel.  Alternatively, if there are any negative eigenvalues then the candidate kernel function is definitely not a legitimate kernel.

Also a reference you can check out is http://cs.nyu.edu/~dsontag/courses/ml12/slides/lecture6.pdf where author provides "Kernel Algebra" that follows from the statements made above -- Mercer's Theorem that the corresponding Kernel Matrix is symmetric positive semi-definite and the positive eigenvalues follows from that.  The author also shows, as an example, that the Gaussian function makes a valid Kernel.  I will provide it here in case you don't want to look up the reference:
 
Time Series Breakout/Change/Disturbance Detection in R: strucchange, changepoint, BreakoutDetection, bfast, and more 
I would like for this to become a sign-post for various time series breakout/change/disturbance detection methods in R.  My question is to describe the motivation and differences in approaches with each of the following packages.  That is, when does it make more sense to use one approach over the other, similarities/differences, etc.
Packages in question:

strucchange (example here)
changepoint (example here)
BreakoutDetection (link includes simple example)
qcc's Control Charts (tutorial here)
bfast
Perhaps (?) to a lesser extent: AnomalyDetection and mvOutlier

I am hopeful for targeted answers. Perhaps a paragraph for each method. It is easy to slap each of these across a time series but that can come at the cost of abusing/violating assumptions. There are resources that provide guidelines for ML supervised/unsupervised techniques. I (and surely others) would appreciate some guidepost/pointers around this area of time-series analysis. 

Two very different motivations have led to time-series analysis:

Industrial quality control and detection of outliers, detecting deviations from a stable noise.
Scientific understanding of trends, where the understanding of trends and of their determinants is of central importance.

Of course both are to a large extent two sides of a same coin and the detection of outliers can be important for time series cleaning before trends analysis. I will nevertheless try hereafter to use this distinction as a red line to explain the diversity of packages offered by R to study time-series. 
In quality control, the stability of the mean and standard deviation are of major importance as exemplified by the history of one of the first statistical efforts to maintain industrial quality, the control chart. In this respect, qcc is a reference implementation of the most classical quality control diagrams: Shewhart quality control, cusum and EWMA charts. 
The old but still active mvoutlier and the more recent AnomalyDetection focus on outliers detection. mvoutlier mainly uses the Mahalanobis distance and can work with two dimensional datasets (rasters) and even multi-dimensional datasets using using the algorithm of Filzmoser, Maronna, and Werner (2007). AnomalyDetection uses the time series decomposition to identify both local anomalies (outlyers) and global anomalies (variations not explained by seasonal patterns). 
and BreakoutDetection 
As AnomalyDetection, BreakoutDetection have been open-sourced by twitter in 2014. BreakoutDetection, open-sourced in 2014 by Twitter, intends to detect breakouts it time series, that is groups of anomalies, using non-parametric statistics. The detection of breakouts comes very close to the detection of trends and understanding of patterns. In a similar optic, the brca package focuses on the analysis of irregularly sampled time-series, particularly to identify behavioral changes in animal movement.
Definitely shifting to determination of changes in trends changepoint implements multiple (simple) frequentist and non-parametric methods to detect single or multiple breaks in time series trends. strucchange allows to fit, plot and test trend changes using regression models. Finally, bfast builds on strucchange to analyze raster (e.g. satellite images) time series and handles missing data.
large scale data mining with clojure 
I'm looking for a good reference on
large scale data mining with Clojure
I know of many good clojure programming books (Programming Clojure, Joy of Clojure, ...), and many good data mining text books (mining of massive data sets, managing gigabytes, ...). However I'm not aware of any reference that specifically addresses
large scale data mining with Clojure
The "with clojure" part is rather important to me for the following reasons:
* most theoretical analysis uses big-Oh running time, which ignores constants
* constants matter, if it ends up being a matter of 1 second vs 1 hour (for things that need to be real time)
* or 1 hour vs 1 week (for batch jobs)

In particular, I think there's a lot of interplay between the JVM, Clojure Data Structures, whether data is stored in memory or lazily read from disk -- that can have the "same" algorithm have drastically different running times by "slightly" different implementations.
Thus, my question (all of the above was to avoid being closed by "Check Google"):
what is a good resource on massive data mining with Clojure?
Thanks!
 A lot of such constant costs comes from the VM actually. In particular, the cost of wrapped primitive objects in data mining is significant, which is why Java is always rather slow there. I doubt that Clojure will help there much. In fact, many of the things Clojure is proud about sound exactly like this problem. Immutability for example. When mining large data, you want to avoid copies, and immutability and nice interfaces usually require copies! I'd give Vala a try instead. Clojure of data structures are immutable but do not require the copying existing data. Check out 'Clojure persistent data structures'.
I don't think anyone's yet written a good comprehensive reference. But there is certainly lots of work going on in this space (my own company included!)
Some interesting links to follow up:

Storm - distributed realtime computation using Clojure. Could be used for large scale data mining.
http://www.infoq.com/presentations/Why-Prismatic-Goes-Faster-With-Clojure - interesting video regarding Clojure performance and optimisation for machine learning applications
Incanter - probably the leading Clojure library for statistics and data visualisation
Weka - very comprehensive data mining / machine learning library for Java (and hence very easy to use directly from Clojure)

 What is your company? Your profile appears to link to a website with an online game.
There is a wonderful book that is coming out in May 2013: Clojure Data Analysis Cookbook. I will probably buy it.
http://www.amazon.co.uk/Clojure-Data-Analysis-Cookbook-ebook/dp/B00BECVV9C/ref=sr_1_1?s=books&ie=UTF8&qid=1360697819&sr=1-1

In Detail
Data is everywhere and it's increasingly important to be able to gain
  insights that we can act on. Using Clojure for data analysis and
  collection, this book will show you how to gain fresh insights and
  perspectives from your data with an essential collection of practical,
  structured recipes.
"The Clojure Data Analysis Cookbook" presents recipes for every stage
  of the data analysis process. Whether scraping data off a web page,
  performing data mining, or creating graphs for the web, this book has
  something for the task at hand.
You'll learn how to acquire data, clean it up, and transform it into
  useful graphs which can then be analyzed and published to the
  Internet. Coverage includes advanced topics like processing data
  concurrently, applying powerful statistical techniques like Bayesian
  modelling, and even data mining algorithms such as K-means clustering,
  neural networks, and association rules.
Approach
Full of practical tips, the "Clojure Data Analysis Cookbook" will help
  you fully utilize your data through a series of step-by-step, real
  world recipes covering every aspect of data analysis.
Who this book is for
Prior experience with Clojure and data analysis techniques and
  workflows will be beneficial, but not essential.

 released. more info at github repo.Randomness in Artificial Intelligence & Machine Learning 
This question came to my mind while working on 2 projects in AI and ML. What If I'm building a model (e.g. Classification Neural Network,K-NN, .. etc) and this model uses some function that includes randomness. If I don't fix the seed, then I'm going to get different accuracy results every time I run the algorithm on the same training data. However, If I fix it then some other setting might give better results.
Is averaging a set of accuracies enough to say that the accuracy of this model is xx % ?
I'm not sure If this is the right place to ask such a question/open such a discussion.

There are models which are naturally dependent on randomness (e.g., random forests) and models which only use randomness as part of exploring the space (e.g., initialisation of values for neural networks), but actually have a well-defined, deterministic, objective function.
For the first case, you will want to use multiple seeds and report average accuracy, std. deviation, and the minimum you obtained. It is often good if you have a way to reproduce this, so just use multiple fixed seeds.
For the second case, you can always tell, just on the training data, which run is best (although it might actually not be the one which gives you the best test accuracy!). Thus, if you have the time, it is good to do say, 10 runs, and then evaluate on the one with the best training error (or validation error, just never evaluate on testing for this decision). You can go a level up and do multiple multiple runs and get a standard deviation too. However, if you find that this is significant, it probably means you weren't trying enough initialisations or that you are not using the right model for your data.

Simple answer, yes, you randomize it and use statistics to show the accuracy. However, it's not sufficient to just average a handful of runs. You need, at a minimum, some notion of the variability as well. It's important to know whether "70%" accurate means "70% accurate for each of 100 runs" or "100% accurate once and 40% accurate once".
If you're just trying to play around a bit and convince yourself that some algorithm works, then you can just run it 30 or so times and look at the mean and standard deviation and call it a day. If you're going to convince anyone else that it works, you need to look into how to do more formal hypothesis testing.

Stochastic techniques are typically used to search very large solution spaces where exhaustive search is not feasible. So it's almost inevitable that you will be trying to iterate over a large number of sample points with as even a distribution as possible. As mentioned elsewhere, basic statistical techniques will help you determine when your sample is big enough to be representative of the space as a whole.
To test accuracy, it is a good idea to set aside a portion of your input patterns and avoid training against those patterns (assuming you are learning from a data set). Then you can use the set to test whether your algorithm is learning the underlying pattern correctly, or whether it's simply memorizing the examples.
Another thing to think about is the randomness of your random number generator. Standard random number generators (such as rand from <stdlib.h>) may not make the grade in many cases so look around for a more robust algorithm.

I generalize the answer from what i get of your question,
I suppose Accuracy is always average accuracy of multiple runs and the standard deviation. So if you are considering accuracy you get using different seeds to the random generator, are you not actually considering a greater range of input (which should be a good thing). But you have to consider the Standard deviation to consider the accuracy. Or did i get your question it totally wrong ?

I believe cross-validation may give you what you ask about: an averaged, and therefore more reliable, estimate of classification performance. It contains no randomness, except in permuting the data set initially. The variation comes from choosing different train/test splits.
DBMS_DATA_MINING.CREATE_MODEL causes “ORA-40103: invalid case-id column: TID” on 11.2.0.1.0 64b, but on 10g OK 
I have a problem with DBMS_DATA_MINING.CREATE_MODEL on version 11.2. On 10g this code below works OK, and I'm quite sure that on 11.1 it works too.
CREATE OR REPLACE VIEW "SH"."ITEMS" AS SELECT PROD_ID AS item FROM SALES GROUP BY PROD_ID;
CREATE OR REPLACE VIEW "SH"."TRANSACTIONS" AS SELECT "SH"."SALES"."PROD_ID" AS item , "SH"."SALES"."CUST_ID" tid FROM "SH"."SALES" where cust_id between 100001 AND 104500 GROUP BY cust_id, prod_id;
CREATE TABLE "SH"."AR_SETTINGS" ( "SETTING_NAME" VARCHAR2(30 BYTE), "SETTING_VALUE" VARCHAR2(128 BYTE) );
INSERT INTO SH.AR_SETTINGS (SETTING_NAME, SETTING_VALUE) VALUES ('ASSO_MAX_RULE_LENGTH', '6' );
INSERT INTO SH.AR_SETTINGS (SETTING_NAME, SETTING_VALUE) VALUES( 'ASSO_MIN_CONFIDENCE', TO_CHAR(0.7));
INSERT INTO SH.AR_SETTINGS (SETTING_NAME, SETTING_VALUE) VALUES( 'ASSO_MIN_SUPPORT', TO_CHAR(0.1));

BEGIN DBMS_DATA_MINING.CREATE_MODEL( model_name => 'AR_sh', mining_function => DBMS_DATA_MINING.ASSOCIATION, data_schema_name => 'sh', data_table_name => 'transactions', case_id_column_name => 'tid', settings_schema_name => 'sh', settings_table_name => 'ar_settings'); END;

causes:
ORA-40103: invalid case-id column: TID
ORA-06512: at "SYS.DBMS_DATA_MINING", line 1779
ORA-06512: at line 1
40103. 00000 - "invalid case-id column: %s"
*Cause: The column designated as case-id is not of one of CHAR, VARCHAR2,
NUMBER data type. Case-id columns of type CHAR and VARCHAR2 must
be of length less than or equal to 128 bytes.
*Action: Change the schema of your input data to supply a case-id column
of appropriate data type and/or length.

to be sure:
describe "SH"."TRANSACTIONS"
Name Null Type 
--
ITEM NOT NULL NUMBER 
TID NOT NULL NUMBER

and
select * from v$version;

returns:
Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production 
PL/SQL Release 11.2.0.1.0 - Production 
CORE    11.2.0.1.0  Production 
TNS for 64-bit Windows: Version 11.2.0.1.0 - Production 
NLSRTL Version 11.2.0.1.0 - Production

The sample code from dmardemo.sql causes the same error.
I don't know what is wrong. Please help.

Your code and the samples both work for me.  I'm also using 11.2.0.1.0, except I'm using 32-bit instead of 64-bit.
I'm not sure what this means.  Maybe there was a problem with your installation?  You might want to look at Verifying Your Data Mining Installation.
 I doubt Vista/7 that makes a difference.  Once thing Oracle seems to do very well is keep their software consistent between operating systems.  I'm surprised that a difference in cust_id would make a difference either; your example still worked for me even when I changed the range to return no results.  Can you reproduce your problem on another database, one from a fresh 11g install?  If your database was upgraded from 10g it looks like there are some additional steps that you must take for data mining: docs.oracle.com/cd/E18283_01/datamine.112/e16807/… it happens on fresh 11.2 installation (without upgrade)
or maybe the range where cust_id between 100001 AND 104500 changed between versions? 

Purely a guess, but is it possible that a synonym for TRANSACTIONS is not pointing to SH.TRANSACTIONS but some other table?  I know you specify the schema name SH, but may still cause issues if this is the case (describe TRANSACTIONS to see).
 i checked using describe and it was OK
Thanks for the help. After couple of system restarts it started to work. Without any reason (no configuration changes).
normalization methods for stream data 
I am using Clustream algorithm and I have figured out that I need to normalize my data. I decided to use min-max algorithm to do this, but I think in this way the values of new coming data objects will be calculated differently as the values of min and max may change. Do you think that I'm correct? If so, which algorithm shall I use? 

Instead to compute the global min-max based on the whole data, you can use a local nomarlization based on a sliding window (e.g. using just the last 15 secconds of data). This approach is very commom to compute Local Mean Filter on signal and image processing.
I hope it can help you.
 Hi! I know it's a little bit late, but currently I find myself with the same problem. The thing is that, if you use a sliding window, current micro clusters that are represented with a normalization considering old min and max values, are in a different 'scale' than the new elements for which you got different min and max values. How do you deal with this? I mean, you normalize the new data points but the current micro clusters managed by the algorithm are normalized considering the old values ... you cannot send the new elements to the algorithm because that would be inconsistent!
When normalizing stream data you need to use the statistical properties of the train set. During streaming you just need to cut too big/low values to a min/max value. There is no other way, it's a stream, you know. 
But as a tradeoff, you can continuously collect the statistical properties of all your data and retrain your model from time to time to adapt to evolving data. I don't know Clustream but after short googling: it seems to be an algorithm to help to make such tradeoffs.
CSV Autodetection in Java 
What would be a reliable way of autodetecting that a file is actually CSV, if CSV was redefined to mean "Character-Separated Values", i.e. data using any single character (but typically any non-alphanumeric symbol) as the delimiter and not only commas?
Essentially, with this (re)definition, CSV = DSV ("Delimiter-Separated Values"), discussed, for example, in this Wikipedia article, whereas the "Comma-Separated Values" format is defined in RFC 4180.
More specifically, is there a method for statistically deducting that the data is of somehow "fixed" length, meaning "possible CSV"? Just counting the number of delimiters does not always work, because there are CSV files with variable numbers of fields per record (i.e., records that, opposite to what RFC 4180 mandates, do not have the same number of fields across the same file). 
CSV recognition seems to be a particularly challenging problem, especially if detection cannot based on the file extension (e.g., when reading a stream that does not have such information anyway).
Proper ("full") autodetection needs at least 4 decisions to be made reliably:

Detecting that a file is actually CSV
Detecting the presence of headers
Detecting the actual separator character
Detecting special characters (e.g., quotes)

Full autodetection seems to have no single solution, due to the similarities of other datasets (e.g., free text that uses commas), especially for corner cases like variable length records, single or double quoted fields, or multiline records.
So, the best approach seems to be telescopic detection, in which formats that can also be classified as CSV (e.g., log file formats like the Apache CLF) are examined before the application of the CSV detection rules.
Even commercial applications like Excel seem to rely on the file extension (.csv) in order to decide for (1), which is obviously no autodetection, although the problem is greatly simplified if the application is told that the data is CSV.
Here are some good relevant articles discussing heuristics for (2) and (3):

Autodetection of headers (Java)
Autodetection of separator (C#)
Autodetection of headers and separator (Python)

The detection of (4), the type of quotes, can be based on processing a few lines from the file and looking for corresponding values (e.g., an even number of ' or " per row would mean single or double quotes). Such processing can be done via initializing an existing CSV parser (e.g., OpenCSV) that will take proper care of CSV row separation (e.g., multiline events).
But what about (1), i.e., deciding that the data is CSV in the first place?
Could data mining help in this decision?
 Is ThIs Comment CSV?  If So, How can I determIne the Rule? A single-line phrase could always be considered CSV with the presence of a delimiter (e.g., ',' or even '?' in your comment). There is obviously no 100% solution for all datasets, but actual CSV data will have at least a few records to process, so my question is, given such records, how can you reliably tell? If I provide another lIne?  Can I determIne the Rule?  What would I need besIde multIple lInes? I should think such a task is more effort than it's worth. Especially when you consider that some non-english csv uses period as a delimiter as comma is used as a decimal separator. That was a pain to figure out even by myself, let alone a computer trying to figure it out automatically. There's no generic solution.  Best you can hope for is a heuristic, tuned to the applications you use.  You can have a 1000-line file that looks good for 999 lines, and then the very last line breaks the pattern.
If you can't constrain whats used as a delimiter then you can use brute-force.
You could iterate through all possible combinations of quote character, column delimiter, and record delimiter (256 * 255 * 254 = 16581120 for ASCII).
id,text,date
1,"Bob says, ""hi
..."", with a sigh",1/1/2012

Remove all quoted columns, this can be done with a RegEx replace.
//quick javascript example of the regex, you'd replace the quote char with whichever character your currently testing
var test='id,text,date\n1,"bob, ""hi\n..."", sigh",1/1/2011';
console.log(test.replace(/"(""|.|\n|\r)*?"/gm,""));

id,text,date
1,,1/1/2012

Split on record delimiter
["id,text,date", "1,,1/1/2012"]

Split records on column delimiter
[ ["id", "text", "date"], ["1", "", "1/1/2012"] ]

If the number of columns per record match you have some CSV confidence.
3 == 3

If the number of columns don't match try another combination of row, column and quote character
EDIT
Actually parsing the data after you have confidence on the delimiters and checking for column type uniformity might be a useful extra step

Are all the columns in the first (header?) row strings
Does column X always parse out to null/empty or a valid (int, float, date)

The more CSV data (rows, columns) there is to work with, the more confidence you can extract from this method.
I think this question is kind of silly / overly general, if you have a stream of unknown data you'd definitely want to check for all of the "low hanging fruit" first. Binary formats usually have fairly distinct header signatures, then there's XML and JSON for easily detectable text formats.
 What about the ties?  If you claim the delimiter is "i" and there are no quotes, it seems to work.  Also, if you claim the delimiter is "1" and there are no quotes, that would be equally valid.  How can you decide among them? This is a clever approach that, unfortunately, won't work, exactly because of the ties. Essentially, the first match will be returned as the solution, which will probably not be correct, unless you can prioritize the delimiters based on probability of occurrence (and that's a big if). @PNS: That essentially means that the problem is undecidable. Largely yes, because virtually every format that has a structure is CSV. Even applying the "strict" rule that, for example, given a separator character, every record must have exactly the same number of fields, this still matches well-known formats like Apache CLF (with ':' as the separator). So, CSV is something like a superset of many formats. If an application checks for a specific set of formats, then doing the last check for CSV, using some of the rules mentioned here, will work better. @PNS: "some of the rules mentioned here" An important insight.  There's no general solution -- it's undecidable.  You must impose numerous constraints to "sniff" the CSV format.  CSV sniffing is a common algorithm, but it's limited to a subset of all potential CSV's.
There are always going to be non-CSV files that look like CSV, and vice versa. For instance, there's the pathological (but perfectly valid) CSV file that frankc posted in the Java link you cited:
Name
Jim
Tom
Bill

The best one can do, I think, is some sort of heuristic estimate of the likelihood that a file is CSV. Some heuristics I can think of are:

There is a candidate separator character that appears on every line (or, if you like, every line has one token).
Given a candidate separator character, most (but not necessarily all) of the lines have the same number of fields.
The presence of a first line that looks like it might be a header increases the likelihood of the file containing CSV data.

One can probably think up other heuristics. The approach would then be to develop a scoring algorithm based on these. The next step would be to score a collection of known CSV and non-CSV files. If there is a clear-enough separation, then the scoring could be deemed useful and the scores should tell you how to set a detection threshold.
 The "candidate separator" -- I think -- is undecidable.  Every character -- even if it doesn't occur -- is a candidate separator.  That means that each file has 128 ASCII (or 65,000 Unicode) separators, all equally valid.  What makes one separator a better choice than another? @S.Lott - Suppose the file contains neither ! nor @. There's nothing to be gained by including both of them in the list of candidate separators; you just need a single non-occurring character to test that case. (Alternatively--and more sensibly, I think--one can just give up on detecting single-column CSV files, since every file will look like a single-column CSV file for any separator character that doesn't occur in the file.) "every file will look like a single-column CSV file for any separator character that doesn't occur in the file".  That seems to make the whole thing undecidable.  I can't see how to proceed given that fact. @S.Lott - Like I suggested: proceed by giving up on detecting that case. "every file will look like a single-column CSV file".  That means give up on every file, right?  It's undecidable, isn't it?Can someone please explain data mining, SSIS, BI, ETL and other related technologies? 
I was talking with a co-worker yesterday regarding a situation where he used SSIS (or something like that) to do some really cool thing with an SSIS Package where he passed in a name like "Dr. Reginald Williams, PhD." and based on some weighting scheme the system was smart enough to figure out how to tokenize it and store it in the database as "Salutation- First Name - Last Name - Suffix". He threw out some buzzwords like BI, and SSIS, ETL, and Data mining. I really wanted more information, but didn't even know where to begin to ask. 
I'm a .Net developer and thoroughly versed in C#, Vb.Net, WPF, etc..., but I have no idea what these technologies are, how to add them to my skill set, and whether or not it's something that I really should be focusing on. Any and all direction would be helpful.

SSIS == SQL Server Integration Services and it is an Extract Transform and Load (ETL) tool, it is a far superior implementation of what was Data Transformation Services or DTS in SQL7, SQL2K era. It is a great tool for expressing workflow processes wherein data is moved from point A to point B (and c and d etc) and undergoes changes through that process such as consolidation to a denormalized design or data cleansing.
BI or Business Intelligence is a moniker for a entire category in the tech world and it is a great place to be right now. BI skills are very valued and hard to come by, one of the reasons this is the case is that it is hard to recreate a true BI case in a lab so teaching is almost always done in a real world situation. 
From a high level, BI projects usually involve an end point of reporting. Often times as developers we are used to transactional report writing such as the details of a PO but BI can get into very broad reports that cover product sales trends over decades and deal with hundreds of millions of records. The way we design databases for applications is not ideal for this kind of reporting so other tools and technologies were invented and are used in the BI space. These are things like Cubes which you often hear called OLAP cubes. OLAP cubes usually originate from a data warehouse which is nothing more than another database - but typical warehouses contain data that came from more than one, and often dozens of other application databases. Your inventory app, purchasing app, HR app and a whole bunch of others all contain bits and pieces of data that create a complete picture of the  business, a BI architect will use something like SSIS to pull the data from all these systems, massage it and store it in the data warehouse which is designed with a different kind of design better for reporting. Once it is in the warehouse he will use Analysis services to create cubes on that data and something like Reporting Services to show you reports over that data.
Edit: sorry, forgot Data Mining, it is another non-specific term that describes and concept or a process and not so much a tool. In a simple example, it is a methodical approach to identifying patterns in data. In the past a good business analysy would look through data for trends but with modern databases you are talking about datasets way too large to manually comb through - Data mining allows you to instruct the computer to comb through that data and identify patterns that are of interest.
Hope that helps

What your coworker did might be better described as "intelligent parsing" of a string. That could be done at many levels of sophistication -- for example, using statistical models to give you the likelihood that "Dr." is a salutation and not a first name. Or it could just use a simple lookup list of common salutations, in which case it's just regular procedural code, nothing more.
SSIS is short for SQL Server Integration Services. It's basically DTS on steroids; some people love it, and some people hate it. It'd be tricky to use that by itself to do the kind of thing you're talking about; it's mainly just for taking data from various sources and combining it, transforming it, and loading it somewhere else. It can do some nifty things, many of which tend to be data-mining like, but ultimately it's a production tool for cramming data one direction or another. It isn't particularly well respected in the data mining community.
Data Mining is an entire academic discipline, focused on using some (typically large) quantity of data to either predict future answers or better understand patterns in existing data. It's definitely a great area to get into, but not something you can just pick up and do without some intensive study of math and algorithms. A good book on the subject is this one.
"Business Intelligence" is really more of a buzzword than a specific technology, and can mean different things to different people. At base, the idea suggests doing less dumb stuff with business data, and generally it refers to analysis of trends over time, often using OLAP. It may also include data mining or AI algorithms, but since there's no rigorous definition, just about anybody who wants to sell you something will tell you it offers "Business Intelligence", and hope you don't dig any further.

SSIS is SQL Server Integration Services and is useful for doing the ETL (Extract, Transform, and Load) that are the front end of many data warehousing/business intelligence solutions that integrate data into easy to use dimensional models. SSIS is also useful for smaller projects as a convenient way to load legacy data or data from other repositories or files.
Data mining usually implies using the data from the integrated sources to infer information that would not be obvious from transactional data (via the integration of multiple sources giving more "dimensions" to the data.
BI is a huge topic so it may not be something to focus on unless you want to get into that field, but SSIS can be useful on smaller projects and is worth learning about in any event.
How to find the minimum support in Apriori algorithm 
When the percentage values of support and confidence is given how can I find the minimum support in Apriori algorithm. For an example when support and confidence is given as 60% and 60% respectively what is the minimum support?
 what if there is no confidence and support percentage given. then how to find minimum support..
The support and confidence are measures to measure how interesting a rule is.
The minimum support and minimum confidence are set by the users, and are parameters of the Apriori algorithm for association rule generation. These parameters are used to exclude rules in the result that have a support or a confidence lower than the minimum support and minimum confidence respectively.
So to answer your question, when you say that: "For an example when support and confidence is given as 60% and 60% respectively what is the minimum support?"  you probably mean that you have set the minimum support and confidence to 60 %.
I think that you are just confused by the terms.
 Support and Confidence are not enough to determine how interesting a rule is. We need Correlation analysis to make it sure (Chi-Square test or Lift test).
Check out the full explanation of Apriori algorithm with live usable example here:
http://www.codeding.com/articles/apriori-algorithm
You can add new items and enter the minimum support threshold and minimum confidence threshold and see the resultant large itemsets generated instantly in the demo Silverlight widget.

My answer is coming a little too late, but I guess what Chanikag is asking is - "How to minimum support count when the support threshold is given as 60%". The Minimum Support Count would be count of transactions, so it would be 60% of the total number of transactions. If the number of transactions is 5, your minimum support count would be 5*60/100 = 3.
 what if the number of transactions are 6? it must be 4 I think if 3.5 ? should we take 3 or 4 ?
Minimum support count is the % of the all transaction.suppose you have 60% support count and 5 is the total transaction then in number the min_support will be 5*60/100=3.  
 what if minimum support count is 3.5 what should we take ? 3 or 4 ? It's 3.5 we should take. no need to round up/down.
I'm not sure your question makes sense.  From your example if you have at least one rule returned with Support and Confidence of 60% you can be sure that the Minimum-Support is at least 60% but it could be more.
Minimum-Support is a parameter supplied to the Apriori algorithm in order to prune candidate rules by specifying a minimum lower bound for the Support measure of resulting association rules.  There is a corresponding Minimum-Confidence pruning parameter as well.
Each rule produced by the algorithm has it's own Support and Confidence measures.  Roughly, Support is the ratio of instances for which the rule is true out of all instances.  Confidence is the ratio of instances for which the rule is true out of the number of instances for which the antecedent (LHS of the implication) is true.
Check out Wikipedia for more rigorous definitions.
Expectation Maximization coin toss examples 
I've been self-studying the Expectation Maximization lately, and grabbed myself some simple examples in the process:
http://cs.dartmouth.edu/~cs104/CS104_11.04.22.pdf
There are 3 coins 0, 1 and 2 with P0, P1 and P2 probability landing on Head when tossed. Toss coin 0, if the result is Head, toss coin 1 three times else toss coin 2 three times. The observed data produced by coin 1 and 2 is like this: HHH, TTT, HHH, TTT, HHH. The hidden data is coin 0's result. Estimate P0, P1 and P2.
http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf
There are two coins A and B with PA and PB being the probability landing on Head when tossed. Each round, select one coin at random and toss it 10 times then record the results. The observed data is the toss results provided by these two coins. However, we don't know which coin was selected for a particular round. Estimate PA and PB.
While I can get the calculations, I can't relate the ways they are solved to the original EM theory. Specifically, during the M-Step of both examples, I don't see how they're maximizing anything. It just seems they are recalculating the parameters and somehow, the new parameters are better than the old ones. Moreover, the two E-Steps don't even look similar to each other, not to mention the original theory's E-Step.
So how exactly do these example work?
 cs.stackexchange.com (computer science) might be a better place to ask.
The second PDF won't download for me, but I also visited the wikipedia page http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm which has more information. http://melodi.ee.washington.edu/people/bilmes/mypapers/em.pdf (which claims to be a gentle introduction) might be worth a look too.
The whole point of the EM algorithm is to find parameters which maximize the likelihood of the observed data. This is the only bullet point on page 8 of the first PDF, the equation for capital Theta subscript ML.
The EM algorithm comes in handy where there is hidden data which would make the problem easy if you knew it. In the three coins example this is the result of tossing coin 0. If you knew the outcome of that you could (of course) produce an estimate for the probability of coin 0 turning up heads. You would also know whether coin 1 or coin 2 was tossed three times in the next stage, which would allow you to make estimates for the probabilities of coin 1 and coin 2 turning up heads. These estimates would be justified by saying that they maximized the likelihood of the observed data, which would include not only the results that you are given, but also the hidden data that you are not - the results from coin 0. For a coin that gets A heads and B tails you find that the maximum likelihood for the probability of A heads is A/(A+B) - it might be worth you working this out in detail, because it is the building block for the M step.
In the EM algorithm you say that although you don't know the hidden data, you come in with probability estimates which allow you to write down a probability distribution for it. For each possible value of the hidden data you could find the parameter values which would optimize the log likelihood of the data including the hidden data, and this almost always turns out to mean calculating some sort of weighted average (if it doesn't the EM step may be too difficult to be practical).
What the EM algorithm asks you to do is to find the parameters maximizing the weighted sum of log likelihoods given by all the possible hidden data values, where the weights are given by the probability of the associated hidden data given the observations using the parameters at the start of the EM step. This is what almost everybody, including the Wikipedia algorithm, calls the Q-function. The proof behind the EM algorithm, given in the Wikipedia article, says that if you change the parameters so as to increase the Q-function (which is only a means to an end), you will also have changed them so as to increase the likelihood of the observed data (which you do care about). What you tend to find in practice is that you can maximize the Q-function using a variation of what you would do if you know the hidden data, but using the probabilities of the hidden data, given the estimates at the start of the EM-step, to weight the observations in some way.
In your example it means totting up the number of heads and tails produced by each coin. In the PDF they work out P(Y=H|X=) = 0.6967. This means that you use weight 0.6967 for the case Y=H, which means that you increment the counts for Y=H by 0.6967 and increment the counts for X=H in coin 1 by 3*0.6967, and you increment the counts for Y=T by 0.3033 and increment the counts for X=H in coin 2 by 3*0.3033. If you have a detailed justification for why A/(A+B) is a maximum likelihood of coin probabilities in the standard case, you should be ready to turn it into a justification for why this weighted updating scheme maximizes the Q-function.
Finally, the log likelihood of the observed data (the thing you are maximizing) gives you a very useful check. It should increase with every EM step, at least until you get so close to convergence that rounding error comes in, in which case you may have a very small decrease, signalling convergence. If it decreases dramatically, you have a bug in your program or your maths.

As luck would have it, I have been struggling with this material recently as well.  Here is how I have come to think of it:
Consider a related, but distinct algorithm called the classify-maximize algorithm, which we might use as a solution technique for a mixture model problem.  A mixture model problem is one where we have a sequence of data that may be produced by any of N different processes, of which we know the general form (e.g., Gaussian) but we do not know the parameters of the processes (e.g., the means and/or variances) and may not even know the relative likelihood of the processes.  (Typically we do at least know the number of the processes.  Without that, we are into so-called "non-parametric" territory.)  In a sense, the process which generates each data is the "missing" or "hidden" data of the problem. 
Now, what this related classify-maximize algorithm does is start with some arbitrary guesses at the process parameters.  Each data point is evaluated according to each one of those parameter processes, and a set of probabilities is generated-- the probability that the data point was generated by the first process, the second process, etc, up to the final Nth process.  Then each data point is classified according to the most likely process. 
At this point, we have our data separated into N different classes.  So, for each class of data, we can, with some relatively simple calculus, optimize the parameters of that cluster with a maximum likelihood technique.  (If we tried to do this on the whole data set prior to classifying, it is usually analytically intractable.)  
Then we update our parameter guesses, re-classify, update our parameters, re-classify, etc, until convergence.  
What the expectation-maximization algorithm does is similar, but more general:  Instead of a hard classification of data points into class 1, class 2, ... through class N, we are now using a soft classification, where each data point belongs to each process with some probability.  (Obviously, the probabilities for each point need to sum to one, so there is some normalization going on.)  I think we might also think of this as each process/guess having a certain amount of "explanatory power" for each of the data points.  
So now, instead of optimizing the guesses with respect to points that absolutely belong to each class (ignoring the points that absolutely do not), we re-optimize the guesses in the context of those soft classifications, or those explanatory powers.  And it so happens that, if you write the expressions in the correct way, what you're maximizing is a function that is an expectation in its form. 
With that said, there are some caveats:
1)  This sounds easy.  It is not, at least to me.  The literature is littered with a hodge-podge of special tricks and techniques-- using likelihood expressions instead of probability expressions, transforming to log-likelihoods, using indicator variables, putting them in basis vector form and putting them in the exponents, etc.  
These are probably more helpful once you have the general idea, but they can also obfuscate the core ideas.
2)  Whatever constraints you have on the problem can be tricky to incorporate into the framework.  In particular, if you know the probabilities of each of the processes, you're probably in good shape.  If not, you're also estimating those, and the sum of the probabilities of the processes must be one; they must live on a probability simplex.  It is not always obvious how to keep those constraints intact. 
3)  This is a sufficiently general technique that I don't know how I would go about writing code that is general.  The applications go far beyond simple clustering and extend to many situations where you are actually missing data, or where the assumption of missing data may help you.  There is a fiendish ingenuity at work here, for many applications. 
4)  This technique is proven to converge, but the convergence is not necessarily to the global maximum; be wary.
I found the following link helpful in coming up with the interpretation above: Statistical learning slides
And the following write-up goes into great detail of some painful mathematical details:  Michael Collins' write-up

I wrote the below code in Python which explains the example given in your second example paper by Do and Batzoglou.
I recommend that you read this link first for a clear explanation of how and why the 'weightA' and 'weightB' in the code below are obtained.
Disclaimer : The code does work but I am certain that it is not coded optimally. I am not a Python coder normally and have started using it two weeks ago.
import numpy as np
import math

#### E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* #### 

def get_mn_log_likelihood(obs,probs):
    """ Return the (log)likelihood of obs, given the probs"""
    # Multinomial Distribution Log PMF
    # ln (pdf)      =             multinomial coeff            *   product of probabilities
    # ln[f(x|n, p)] = [ln(n!) - (ln(x1!)+ln(x2!)+...+ln(xk!))] + [x1*ln(p1)+x2*ln(p2)+...+xk*ln(pk)]     

    multinomial_coeff_denom= 0
    prod_probs = 0
    for x in range(0,len(obs)): # loop through state counts in each observation
        multinomial_coeff_denom = multinomial_coeff_denom + math.log(math.factorial(obs[x]))
        prod_probs = prod_probs + obs[x]*math.log(probs[x])

multinomial_coeff = math.log(math.factorial(sum(obs))) -  multinomial_coeff_denom
likelihood = multinomial_coeff + prod_probs
return likelihood

# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T
# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T
# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T
# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T
# 5th:  Coin A, {THHHTHHHTH}, 7H,3T
# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45

# represent the experiments
head_counts = np.array([5,9,8,4,7])
tail_counts = 10-head_counts
experiments = zip(head_counts,tail_counts)

# initialise the pA(heads) and pB(heads)
pA_heads = np.zeros(100); pA_heads[0] = 0.60
pB_heads = np.zeros(100); pB_heads[0] = 0.50

# E-M begins!
delta = 0.001  
j = 0 # iteration counter
improvement = float('inf')
while (improvement>delta):
    expectation_A = np.zeros((5,2), dtype=float) 
    expectation_B = np.zeros((5,2), dtype=float)
    for i in range(0,len(experiments)):
        e = experiments[i] # i'th experiment
        ll_A = get_mn_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) # loglikelihood of e given coin A
        ll_B = get_mn_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) # loglikelihood of e given coin B

        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of A proportional to likelihood of A 
        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of B proportional to likelihood of B                            

        expectation_A[i] = np.dot(weightA, e) 
        expectation_B[i] = np.dot(weightB, e)

    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); 
    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); 

    improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))
    j = j+1


The key to understanding this is knowing what the auxiliary variables are that make estimation trivial. I will explain the first example quickly, the second follows a similar pattern.
Augment each sequence of heads/tails with two binary variables, which indicate whether coin 1 was used or coin 2. Now our data looks like the following:
 c_11 c_12
 c_21 c_22
 c_31 c_32
...
For each i, either c_i1=1 or c_i2=1, with the other being 0. If we knew the values these variables took in our sample, estimation of parameters would be trivial: p1 would be the proportion of heads in samples where c_i1=1, likewise for c_i2, and \lambda would be the mean of the c_i1s.
However, we don't know the values of these binary variables. So, what we basically do is guess them (in reality, take their expectation), and then update the parameters in our model assuming our guesses were correct. So the E step is to take the expectation of the c_i1s and c_i2s. The M step is to take maximum likelihood estimates of p_1, p_2 and \lambda given these cs.
Does that make a bit more sense? I can write out the updates for the E and M step if you prefer. EM then just guarantees that by following this procedure, likelihood will never decrease as iterations increase.
Effects of Stemming on the term frequency? 
How are the term frequencies (TF), and inverse document frequency (IDF), affected by stop-word removal and stemming?
Thanks!

tf is term frequency
idf is inverse document frequency which is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.
stemming effect is grouping all words which are derived from the same stem (ex: played, play,..), this grouping will increase the occurrence of this stem because frequencies are calculated using stem not words,
For example, if you have 2 documents:
the first one contains 'play' 2 times and 'played' 5 times,
and the second document contains 'play' 3 times and 'played' 1 time
if you do a search for 'play' without stemming the second document will be first because it has more occurrence of the word 'play', while if you do stemming, both words will be 'play' after stemming and the first document will be first cause it contains the stem play 7 times and the second document contains the stem play 4 times.
Concerning stopwords removal, it is found frequently in all document and isn't consider as a keyword for any of them, it will have high freq without any scene.
OpenNLP Name Finder 
I am using the NameFinder API example doc of OpenNLP. After initializing the Name Finder the documentation uses the following code for the input text:
for (String document[][] : documents) {

  for (String[] sentence : document) {
    Span nameSpans[] = nameFinder.find(sentence);
    // do something with the names
  }

  nameFinder.clearAdaptiveData()
}

However when I bring this into eclipse the 'documents' (not 'document') variable is giving me an error saying the variable documents cannot be resolved. What is the documentation referring to with the 'documents' array variable? Do I need to initialize an array called 'documents' which hold txt files for this error to go away?
Thank you for your help.

The OpenNLP documentation states that the input text should be segmented into documents, sentences and tokens. The piece of code you provided illustrates how to deal with several documents.
If you have only one document you don't need the first for, just the inner one with the array of sentences, which is composed by as an array of tokens.
To create an array of sentences from a document you can use the OpenNLP SentenceDetector, and for each sentence you can use OpenNLP Tokenizer to get the array of tokens.
Your code will look like this:
// somehow get the contents from the txt file 
//      and populate a string called documentStr

String sentences[] = sentenceDetector.sentDetect(documentStr);
for (String sentence : sentences) {
    String tokens[] = tokenizer.tokenize(sentence);
    Span nameSpans[] = nameFinder.find(tokens);
    // do something with the names
    System.out.println("Found entity: " + Arrays.toString(Span.spansToStrings(nameSpans, tokens)));
}

You can learn how to use the SentenceDetector and the Tokenizer from OpenNLP documentation documentation.
 Thank you for your reply! I plugged that in but still get error: "Type mismatch: cannot convert from element type String to String[]" and the sentences variable is erroring on me on line 5: for(String[] sentence: sentences){ Yes, there was an error. Just removed the [] from for (String sentence[] : sentences). Thank you. wcolen, thanks for all of your help. The only issue when I delete the array syntax [] is the next line now barks at me because the find method takes an array as argument, so sentence doesnt work: Span nameSpans[] = nameFinder.find(sentence); oops... sorry again. I see it now. The tokenization command is missing. I will fix it for you. I also improved the example output. The nameSpans are pointing to the start and end index of the tokens array, so we should use the method Span.spansToStrings to print the output.Java text clustering library 
Which of the data mining java libraries can do text clusterization?
 What do you mean by "text clusterization"? I tried to google it and it has no idea. Do you mean Document Clustering?
Check this tututorial http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html.
Also you can try carrot(http://search.carrot2.org/stable/search)

Apache Solr
 Where is the clustering in Solr?'Similarity' in Data Mining 
In the field of Data Mining, is there a specific sub-discipline called 'Similarity'? If yes, what does it deal with. Any examples, links, references will be helpful.
Also, being new to the field, I would like the community opinion on how closely related Data Mining and Artificial Intelligence are. Are they synonyms, is one the subset of the other?
Thanks in advance for sharing your knowledge.
 Related: stackoverflow.com/questions/3007790/finding-the-closest-match

In the field of Data Mining, is there a specific sub-discipline called 'Similarity'? 

Yes. There is a specific subfield in data mining and machine learning called metric learning, which aims to learn a better distance metric among data instances. 
Do you know any of the following concepts?
Euclidean distance
Mahalanobis distance
Pearson correlation
Cosine similarity and here
Kernel functions 
After you know these, you will know what is 'similarity'. 

I would like the community opinion on how closely related Data Mining and Artificial Intelligence are.

It is very hard to distinguish what is data mining, what is AI. Don't discuss this question when you are new in the field. When you have learned 10 algorithms in data mining and read some AI books, you will know the difference and the relation. 

Appropriate definitions of 'similarity' (which features you extract, what you do with them afterwards) are almost the definition of clustering, and clustering is a fairly wide sub-field of data mining.
If you make the standard cynical definition of AI as the set of problems we can't solve well (indeed, that we can't specify well enough to start solving), data mining shades into it once the space in which you're looking for correlations starts to be larger than your algorithms can handle.

Just to stress the importance of the "similarity" concept. 
Data mining (AI, machine learning, modelling etc) is about bringing some function to either it's maximum or minimum value. Take the best optimization/learning/mining algorithm and a wrong function and you get a complete garbage. Note that we use "value" and not "valueS". That's because there is no (to my best knowledge) algorithm (computational or other) that is capable of optimizing more than one value. However, in our Universe, complex optimizations are more frequent than one-dimensional ones (we want to be rich AND young AND healthy). That is why there a plethora of similarity and other scoring functions exists. And that is why none of them is "the right one"

Similarity is a concept that is used in several data mining tasks such as clustering, classification. Dependings on what kind of data you have, you may used different similarity measures such as cosine similarity for text documents, euclidian distance, etc

There are lots of similarity measurement used in data mining. for text mining, to find similarity in texts, cosine similarity, jaccard similarity widely used
For reference, you can see raghavan and amnnings information retrieval book
What FFT descriptors should be used as feature to implement classification or clustering algorithm? 
                    
I have some geographical trajectories sampled to analyze, and I calculated the histogram of data in spatial and temporal dimension, which yielded a time domain based feature for each spatial element. I want to perform a discrete FFT to transform the time domain based feature into frequency domain based feature (which I think maybe more robust), and then do some classification or clustering algorithms.

But I'm not sure using what descriptor as frequency domain based feature, since there are amplitude spectrum, power spectrum and phase spectrum of a signal and I've read some references but still got confused about the significance. And what distance (similarity) function should be used as measurement when performing learning algorithms on frequency domain based feature vector(Euclidean distance? Cosine distance? Gaussian function? Chi-kernel or something else?)

Hope someone give me a clue or some material that I can refer to, thanks~


Edit

Thanks to @DrKoch, I chose a spatial element with the largest L-1 norm and plotted its log power spectrum in python and it did show some prominent peaks, below is my code and the figure

import numpy as np
import matplotlib.pyplot as plt
sp = np.fft.fft(signal)
freq = np.fft.fftfreq(signal.shape[-1], d = 1.) # time sloth of histogram is 1 hour
plt.plot(freq, np.log10(np.abs(sp) ** 2))
plt.show()


And I have several trivial questions to ask to make sure I totally understand your suggestion:

In your second suggestion, you said "ignore all these values." 

Do you mean the horizontal line represent the threshold and all values below it should be assigned to value zero?
"you may search for the two, three largest peaks and use their location and probably widths as 'Features' for further classification." 

I'm a little bit confused about the meaning of "location" and "width", does "location" refer to the log value of power spectrum (y-axis) and "width" refer to the frequency (x-axis)? If so, how to combine them together as a feature vector and compare two feature vector of "a similar frequency and a similar widths" ?


Edit
I replaced np.fft.fft with np.fft.rfft to calculate the positive part and plot both power spectrum and log power spectrum.

code:

f, axarr = plt.subplot(2, sharex = True)
axarr[0].plot(freq, np.abs(sp) ** 2)
axarr[1].plot(freq, np.log10(np.abs(sp) ** 2))
plt.show()


figure:


Please correct me if I'm wrong:

I think I should keep the last four peaks in first figure with power = np.abs(sp) ** 2 and power[power < threshold] = 0 because the log power spectrum reduces the difference among each component. And then use the log spectrum of new power as feature vector to feed classifiers.

I also see some reference suggest applying a window function (e.g. Hamming window) before doing fft to avoid spectral leakage. My raw data is sampled every 5 ~ 15 seconds and I've applied a histogram on sampling time, is that method equivalent to apply a window function or I still need apply it on the histogram data?
 Why do you need to do FFT over the original data? Cannt you just use the original features? @AbhimanuKumar Theoretically yes, but since the trajectories were sampled by civil GPS device and its high accuracy could not be guaranteed, and I thought the feature of frequency domain is more reliable than that of time domain~ A spectrum of a real (not complex valued) series is always symmetrical, so you should ignore all results for x < 0 The result at x==0 e.g. fft(0) represents the DC component. You should either remove this component (subtract Average(x[]) from all y  values) or ignore the fft(0) value. @DrKoch yes, I've done this in the updated part and I've learned a lot, thanks~ :)
Generally you should extract just a small number of "Features" out of the complete FFT spectrum.
First: Use the log power spec. 
Complex numbers and Phase are useless in these circumstances, because they depend on where you start/stop your data acquisiton (among many other things)
Second: you will see a "Noise Level" e.g. most values are below a certain threshold, ignore all these values.
Third: If you are lucky, e.g. your data has some harmonic content (cycles, repetitions) you will see a few prominent Peaks.
If there are clear peaks, it is even easier to detect the noise: Everything between the peaks should be considered noise.
Now you may search for the two, three largest peaks and use their location and probably widths as "Features" for further classification.
Location is the x-value of the peak i.e. the 'frequency'. It says something how "fast" your cycles are in the input data.
If your cycles don't have constant frequency during the measuring intervall (or you use a window before caclculating the FFT), the peak will be broader than one bin. So this widths of the peak says something about the 'stability' of your cycles.
Based on this: Two patterns are similar if the biggest peaks of both hava a similar frequency and a similar widths, and so on.

EDIT
Very intersiting to see a logarithmic power spectrum of one of your examples.
Now its clear that your input contains a single harmonic (periodic, oscillating) component with a frequency (repetition rate, cycle-duration) of about f0=0.04.
(This is relative frquency, proprtional to the your sampling frequency, the inverse of the time beetween individual measurment points)
Its is not a pute sine-wave, but some "interesting" waveform. Such waveforms produce peaks at 1*f0, 2*f0, 3*f0 and so on.
(So using an FFT for further analysis turns out to be very good idea)
At this point you should produce spectra of several measurements and see what makes a similar measurement and how differ different measurements. What are the "important" features to distinguish your mesurements? Thinks to look out for:

Absolute amplitude: Height of the prominent (leftmost, highest) peaks.
Pitch (Main cycle rate, speed of changes): this is position of first peak, distance between consecutive peaks.
Exact Waveform: Relative amplitude of the first few peaks.

If your most important feature is absoulute amplitude, you're better off with calculating the RMS (root mean square) level of our input signal.
If pitch is important, you're better off with calculationg the ACF (auto-correlation function) of your input signal.
Don't focus on the leftmost peaks, these come from the high frequency components in your input and tend to vary as much as the noise floor.
Windows
For a high quality analyis it is importnat to apply a window to the input data before applying the FFT. This reduces the infulens of the "jump" between the end of your input vector ant the beginning of your input vector, because the FFT considers the input as a single cycle.
There are several popular windows which mark different choices of an unavoidable trade-off: Precision of a single peak vs. level of sidelobes:
You chose a "rectangular window" (equivalent to no window at all, just start/stop your measurement). This gives excellent precission of your peaks which now have a width of just one sample. Your sidelobes (the small peaks left and right of your main peaks) are at -21dB, very tolerable given your input data. In your case this is an excellent choice.
A Hanning window is a single cosine wave. It makes your peaks slightly broader but reduces side-lobe levels.
The Hammimg-Window (cosine-wave, slightly raised above 0.0) produces even broader peaks, but supresses side-lobes by -42 dB. This is a good choice if you expect further weak (but important) components between your main peaks or generally if you have complicated signals like speech, music and so on.

Edit: Scaling
Correct scaling of a spectrum is a complicated thing, because the values of the FFT lines depend on may things like sampling rate, lenght of FFT, window, and even implementation details of the FFT algorithm (there exist several different accepted conventions).
After all, the FFT should show the underlying conservation of energy. The RMS of the input signal should be the same as the RMS (Energy) of the spectrum.
On the other hand: if used for classification it is enough to maintain relative amplitudes. As long as the paramaters mentioned above do not change, the result can be used for classification without further scaling.
 I've edited the question and there are several trivial questions which I hope for your answer, thank~ Thank you very much for the thorough explanation :) I'm now much clearer about the significance of using a window function, but I'm not sure how to decide the weighting factor to recover the correct FFT signal amplitude level after the windowing, could you suggest any rule of thumb?How to group nearby latitude and longitude locations stored in SQL 
Im trying to analyse data from cycle accidents in the UK to find statistical black spots.  Here is the example of the data from another website. http://www.cycleinjury.co.uk/map
I am currently using SQLite to ~100k store lat / lon locations.  I want to group nearby locations together.  This task is called cluster analysis.
I would like simplify the dataset by ignoring isolated incidents and instead only showing the origin of clusters where more than one accident have taken place in a small area.    
There are 3 problems I need to overcome.

Performance - How do I ensure finding nearby points is quick.  Should I use SQLite's implementation of an R-Tree for example?
Chains - How do I avoid picking up chains of nearby points?
Density - How to take cycle population density into account?  There is a far greater population density of cyclist in london then say Bristol, therefore there appears to be a greater number of backstops in London.  

I would like to avoid 'chain' scenarios like this:

Instead I would like to find clusters:

London screenshot (I hand drew some clusters)...

Bristol screenshot - Much lower density - the same program ran over this area might not find any blackspots if relative density was not taken into account.

Any pointers would be great!
 Have you tried to use regular grid to limit number of points to search? Would be easier to implement than R-Tree... @AlexeiLevenkov Thanks.  Does this mean grouping the points by tiles?  How do you deal with points overlapping tile boundaries?  Do you have too look at adjacent tiles? Yes - somewhat simplest optimization for such type of search. But finding existing implementation (i.e. as you've mentioned SQLite one) it likely to provide more reliable and faster solution. Is the data available somewhere? It's interesting. Yes - Took me ages to find it initially... data.gov.uk/dataset/road-accidents-safety-data The UK government publishes a huge amount of data. It;s really quite cool!
Well, your problem description reads exactly like the DBSCAN clustering algorithm (Wikipedia). It avoids chain effects in the sense that it requires them to be at least minPts objects.
As for the differences in densities across, that is what OPTICS (Wikipedia) is supposed do solve. You may need to use a different way of extracting clusters though.
Well, ok, maybe not 100% - you maybe want to have single hotspots, not areas that are "density connected". When thinking of an OPTICS plot, I figure you are only interested in small but deep valleys, not in large valleys. You could probably use the OPTICS plot an scan for local minima of "at least 10 accidents".
Update: Thanks for the pointer to the data set. It's really interesting. So I did not filter it down to cyclists, but right now I'm using all 1.2 million records with coordinates. I've fed them into ELKI for analysis, because it's really fast, and it actually can use the geodetic distance (i.e. on latitude and longitude) instead of Euclidean distance, to avoid bias. I've enabled the R*-tree index with STR bulk loading, because that is supposed to help to get the runtime down a lot. I'm running OPTICS with Xi=.1, epsilon=1 (km) and minPts=100 (looking for large clusters only). Runtime was around 11 Minutes, not too bad. The OPTICS plot of course would be 1.2 million pixels wide, so it's not really good for full visualization anymore. Given the huge threshold, it identified 18 clusters with 100-200 instances each. I'll try to visualize these clusters next. But definitely try a lower minPts for your experiments.
So here are the major clusters found:

51.690713 -0.045545 a crossing on A10 north of London just past M25
51.477804 -0.404462 "Waggoners Roundabout"
51.690713 -0.045545 "Halton Cross Roundabout" or the crossing south of it
51.436707 -0.499702 Fork of A30 and A308 Staines By-Pass
53.556186 -2.489059 M61  exit to A58, North-West of Manchester
55.170139 -1.532917 A189, North Seaton Roundabout
55.067229 -1.577334 A189 and A19, just south of this, a four lane roundabout.
51.570594 -0.096159 Manour House, Picadilly Line
53.477601 -1.152863 M18 and A1(M)
53.091369 -0.789684 A1, A17 and A46, a complex construct with roundabouts on both sides of A1.
52.949281 -0.97896 A52 and A46
50.659544 -1.15251 Isle of Wight, Sandown.
...

Note, these are just random points taken from the clusters. It may be sensible to compute e.g. cluster center and radius instead, but I didn't do that. I just wanted to get a glimpse of that data set, and it looks interesting.
Here are some screenshots, with minPts=50, epsilon=0.1, xi=0.02:

Notice that with OPTICS, clusters can be hierarchical. Here is a detail:

 Really cool answer! Im going to look though this all tonight!  Thank you :)
First, your example is quite misleading.  You have two different sets of data, and you don't control the data.  If it appears in a chain, then you will get a chain out.
This problem is not exactly suitable for a database.  You'll have to write code or find a package that implements this algorithm on your platform.
There are many different clustering algorithms.  One, k-means, is an iterative algorithm where you look for a fixed number of clusters.  k-means requires a few complete scans of the data, and voila, you have your clusters.  Indexes are not particularly helpful.
Another, which is usually appropriate on slightly smaller data sets, is hierarchical clustering -- you put the two closest things together, and then build the clusters.  An index might be helpful here.
I recommend though that you peruse a site such as kdnuggets in order to see what software -- free and otherwise -- is available.
Similarity distance measures 
Vectors like this 
v1 = {0 0 0 1 1 0 0 1 0 1 1}
v2 = {0 1 1 1 1 1 0 1 0 1 0}
v3 = {0 0 0 0 0 0 0 0 0 0 1}

Need to calculate similarity between them. Hamming distance between v1 and v2 is 4 and between v1 and v3 is also 4. But because I am interested in the groups of '1' which are together for me v2 is far more similar to v1 then v3 is.
Is there any distance metrics that can capture this in the data?
The data represent occupancy of house in time, that's why it is important to me. '1' means occupied, '0' means non occupied.
 so what is the question ? sorry, already edited, if there is any distance metrics that can capture this I am interested in the groups of '1' which are together. Could you explain what you mean by that? 1 and 2 are more simliar because of the same amount of groups? well basically yes, 1 and 2 are more similar there is same amount of groups. Because v2 is basically vector v1 only with the first group of '1' being "wider". V3 is almost empty vector
It sounds like you need cosine similarity measure: 
similarity = cos(v1, v2) = v1 * v2 / (|v1| |v2|)

where v1 * v2 is dot product between v1 and v2: 
v1 * v2 = v1[1]*v2[1] + v1[2]*v2[2] + ... + v1[n]*v2[n]

Essentially, dot product shows how many elements in both vectors have 1 at the same position: if v1[k] == 1 and v2[k] == 1, then final sum (and thus similarity) is increased, otherwise it isn't changed. 
You can use dot product itself, but sometimes you would want final similarity to be normalized, e.g. be between 0 and 1. In this case you can divide dot product of v1 and v2 by their lengths - |v1| and |v2|. Essentially, vector length is square root of dot product of the vector with itself: 
|v| = sqrt(v[1]*v[1] + v[2]*v[2] + ... + v[n]*v[n])

Having all of these, it's easy to implement cosine distance as follows (example in Python): 
from math import sqrt

def dot(v1, v2):
    return sum(x*y for x, y in zip(v1, v2))

def length(v):
    return sqrt(dot(v, v))

def sim(v1, v2): 
    return dot(v1, v2) / (length(v1) * length(v2))

Note, that I described similarity (how much two vectors are close to each other), not distance (how far they are). If you need exactly distance, you can calculate it as dist = 1 / sim. 
 I believe it should be dist = 1 - sim and not 1 / sim @ThalisK.: both will work. The idea is that distance is in some sense inverse of similarity, so any inverse (and strictly monotone) function should work, and you can select concrete function based on your concrete interpretation of "distance". Thanks. That makes sense. I would appreciate it if you would then take a look at this question: stackoverflow.com/questions/25181104/… if i have more than three vectors, like if i want to check v4 is different from v1, v2, and v3, can i apply your answer? Cosine similarity is a pairwise distance measure, so you can use it to any number of vectors as long as you consider their pairs (e.g. v4 vs v1, v4 vs v2, etc.). If you want a measure that works with 3 or more vectors at the same time, you should be more specific about desirable properties of this measure. E.g. you may want an average distance of v4 from v1, v2 and v3 and that is as easy as (dist(v4, v1) + dist(v4, v2) + dist(v4, v3)) / 3. So it all depends on what exactly you want to achieve.
There are literally hundreds of distance functions, including distance measures for sets, such as Dice and Jaccard.
You may want to get the book "Dictionary of Distance Functions", it's pretty good.
 Looked up the title and didn't find it. Could your provide author, isbn, exact title please? First result on Google Books. ISBN: 9780080465548 The book is a great reference. It has been updated and renamed as Encyclopedia of distances: link.springer.com/book/10.1007/978-3-662-52844-0

Case 1: IF the position of the ones in the series is relevant, THEN:

I recommend Dynamic Time Warping Distance (DTW). In application of time-series data it has proven incredibly useful. 
To check whether it can be applied to your problem, I used the code presented here: https://jeremykun.com/2012/07/25/dynamic-time-warping/
d13 = dynamicTimeWarp(v1,v3)
d12 = dynamicTimeWarp(v1,v2)
d23 = dynamicTimeWarp(v2,v3)

d23,d12,d13
(3, 1, 3)

As you see, d12 is lowest, therefore v1 and v2 are most similiar. Further information of DTW can be found anywhere in this forum and for research papers, I recommend anything by Eamonn Keogh.

Case 2: Position of ones is not relevant:

I simply agree Deepu for taking the average as a feature. 

I think you can simply take the average of the values in each set. For example v1 here will have an average 0.4545, average of v2 is 0.6363 and average of v3 is  0.0909. If the only possible values in the set are 0 and 1, then the sets with equal or nearly equal values will serve your purpose.  
 That's actually good idea, the problem I have though is that I have to mix the two metrics somehow together. Because vectors 0 0 1 1  and 1 1 0 0 would with average return both 0,5 and with my metrics 4 that all of them are displaced. Is it possible to somehow combine these two metrics that each yields half of the final  value? Or is that too unpredictable? What about Standard Deviation? Will it help? In a way I guess if the distribution below it was gaussian. But if I again take the the 0 0 1 1  and 1 1 0 0  example the std will have same results. I know how you mean it but then I would have to first cluster it make means of the clusters and then compare means and std of each cluster. But if such a complicated solution makes significant different.
There is a web site introducing the various type of vector similarity methods
http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/
I think it will help you to decide what similarity you should use
.
.
Briefly explaining the above link, there are five popular similarity measurement between vectors

Euclidean Distance - Simply the absolute distance between the vectors
Cosine - Cosine degree(theta) difference between the vectors
Manhattan -the sum of the absolute differences of their Cartesian coordinates, for example,


In a plane with p1 at (x1, y1) and p2 at (x2, y2). Manhattan distance
  = |x1 – x2| + |y1 – y2|


Minkowski - generalized metric form of Euclidean distance and Manhattan distance
Jaccard - Similarity between the objects. So each feature in one set will be compared to another set and finds out its difference

.
With the keyword above you can google for further explanation.
Hope it would help you
clustering and matlab 
I'm trying to cluster some data I have from the KDD 1999 cup dataset 
the output from the file looks like this:
0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.

with 48 thousand different records in that format. I have cleaned the data up and removed the text keeping only the numbers. The output looks like this now:
 
I created a comma delimited file in excel and saved as a csv file then created a data source from the csv file in matlab, ive tryed running it through the fcm toolbox in matlab (findcluster outputs 38 data types which is expected with 38 columns). 
The clusters however don't look like clusters or its not accepting and working the way I need it to. 
Could anyone help finding the clusters? Im new to matlab so don't have any experience and I'm also new to clustering. 
The method:

Chose number of clusters (K)
Initialize centroids (K patterns randomly chosen from data set)
Assign each pattern to the cluster with closest centroid
Calculate means of each cluster to be its new centroid
Repeat step 3 until a stopping criteria is met (no pattern move to another cluster)

This is what I'm trying to achieve:

This is what I'm getting:

load kddcup1.dat
plot(kddcup1(:,1),kddcup1(:,2),'o')  
[center,U,objFcn] = fcm(kddcup1,2);
Iteration count = 1, obj. fcn = 253224062681230720.000000
Iteration count = 2, obj. fcn = 241493132059137410.000000
Iteration count = 3, obj. fcn = 241484544542298110.000000
Iteration count = 4, obj. fcn = 241439204971005280.000000
Iteration count = 5, obj. fcn = 241090628742523840.000000
Iteration count = 6, obj. fcn = 239363408546874750.000000
Iteration count = 7, obj. fcn = 238580863900727680.000000
Iteration count = 8, obj. fcn = 238346826370420990.000000
Iteration count = 9, obj. fcn = 237617756429912510.000000
Iteration count = 10, obj. fcn = 226364785036628320.000000
Iteration count = 11, obj. fcn = 94590774984961184.000000
Iteration count = 12, obj. fcn = 2220521449216102.500000
Iteration count = 13, obj. fcn = 2220521273191876.200000
Iteration count = 14, obj. fcn = 2220521273191876.700000
Iteration count = 15, obj. fcn = 2220521273191876.700000

figure
plot(objFcn)
title('Objective Function Values')
xlabel('Iteration Count')
ylabel('Objective Function Value')

    maxU = max(U);
    index1 = find(U(1, :) == maxU);
    index2 = find(U(2, :) == maxU);
    figure
    line(kddcup1(index1, 1), kddcup1(index1, 2), 'linestyle',...
    'none','marker', 'o','color','g');
    line(kddcup1(index2,1),kddcup1(index2,2),'linestyle',...
    'none','marker', 'x','color','r');
    hold on
    plot(center(1,1),center(1,2),'ko','markersize',15,'LineWidth',2)
    plot(center(2,1),center(2,2),'kx','markersize',15,'LineWidth',2)

 really dont know what im doing wrong? Can you include the part of your code where you generate the clusters? Why is this tagged with "C#"?
Since you are new to machine-learning/data-mining, you shouldn't tackle such advanced problems. After all, the data you are working with was used in a competition (KDD Cup'99), so don't expect it to be easy!
Besides the data was intended for a classification task (supervised learning), where the goal is predict the correct class (bad/good connection). You seem to be interested in clustering (unsupervised learning), which is generally more difficult.
This sort of dataset requires a lot of preprocessing and clever feature extraction. People usually employ domain knowledge (network intrusion detection) to obtain better features from the raw data.. Directly applying simple algorithms like K-means will generally yield poor results.
For starters, you need to normalize the attributes to be of the same scale: when computing the euclidean distance as part of step 3 in your method, the features with values such as 239 and 486 will dominate over the other features with small values as 0.05, thus disrupting the result.
Another point to remember is that too many attributes can be a bad thing (curse of dimensionality). Thus you should look into feature selection or dimensionality reduction techniques.
Finally, I suggest you familiarize yourself with a simpler dataset...
Issues in getting trigrams using Gensim 
I want to get bigrams and trigrams from the example sentences I have mentioned.
My code works fine for bigrams. However, it does not capture trigrams in the data (e.g., human computer interaction, which is mentioned in 5 places of my sentences)
Approach 1 Mentioned below is my code using Phrases in Gensim.
from gensim.models import Phrases
documents = ["the mayor of new york was there", "human computer interaction and machine learning has now become a trending research area","human computer interaction is interesting","human computer interaction is a pretty interesting subject", "human computer interaction is a great and new subject", "machine learning can be useful sometimes","new york mayor was present", "I love machine learning because it is a new subject area", "human computer interaction helps people to get user friendly applications"]
sentence_stream = [doc.split(" ") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b' ')
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)

Approach 2 I even tried to use Phraser and Phrases both, but it didn't work.
from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = ["the mayor of new york was there", "human computer interaction and machine learning has now become a trending research area","human computer interaction is interesting","human computer interaction is a pretty interesting subject", "human computer interaction is a great and new subject", "machine learning can be useful sometimes","new york mayor was present", "I love machine learning because it is a new subject area", "human computer interaction helps people to get user friendly applications"]
sentence_stream = [doc.split(" ") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)

Please help me to fix this issue of getting trigrams.
I am following the example documentation of Gensim.

I was able to get bigrams and trigrams with a few modifications to your code:

from gensim.models import Phrases
documents = ["the mayor of new york was there", "human computer interaction and machine learning has now become a trending research area","human computer interaction is interesting","human computer interaction is a pretty interesting subject", "human computer interaction is a great and new subject", "machine learning can be useful sometimes","new york mayor was present", "I love machine learning because it is a new subject area", "human computer interaction helps people to get user friendly applications"]
sentence_stream = [doc.split(" ") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]

    print(bigrams_)
    print(trigrams_)

I removed the threshold = 1 parameter from the bigram Phrases because otherwise it seems to form weird digrams that allow the construction of weird trigrams (notice that bigram is used to build the trigram Phrases); this parameter would probably come useful when you have more data. For trigrams, the min_count parameter also needs to be specified because it defaults to 5 if not provided.
In order to retrieve the bigrams and trigrams of each document, you can use this list comprehension trick to filter elements that aren't formed by two or three words, respectively. 

Edit - a few details about the threshold parameter:
This parameter is used by the estimator to determine if two words a and b form a phrase, and that is only if:

(count(a followed by b) - min_count) * N/(count(a) * count(b)) > threshold

where N is the total vocabulary size. By default the parameter value is 10 (see docs). So, the higher the threshold, the harder the constraints for words to form phrases.
For example, in your first approach you were trying to use threshold = 1, so you would get ['human computer','interaction is'] as digrams of 3 out of your 5 sentences that begin with "human computer interaction"; that weird second digram is a result of the more relaxed threshold. 
Then, when you try to get trigrams with default threshold = 10 you only get ['human computer interaction is'] for those 3 sentences, and nothing for the remaining two (filtered by threshold); and because that was a 4-gram instead of a trigram it would also be filtered by if t.count(' ') == 2. In case that, for example, you lower the trigram threshold to 1, you can get ['human computer interaction'] as trigram for the two remaining sentences. It doesn't seem easy to get a good combination of parameters, here's more about it. 
I'm not an expert, so take this conclusion with a grain of salt: I think it's better to firstly get good digram results (not like 'interaction is') before moving on, as weird digrams can add confusion to further trigrams, 4-gram... 
 Many thanks for your very valuable answer. Cheers! :) By the way, can please tell me what happens with threshold value, as it is not very clear for me? You're welcome! Yes, I edited the answer, hopefully now it's a bit clearer. Thanks a lot!  Found your answer very useful :) It was not obvious from gensim that delimiter=b' ' must be in binary format. Thanks for that. How to use it for train and test data? It doesn't have any fit and transform method like scikit learn vectorizers.When are n-grams (n>3) important as opposed to just bigrams or trigrams? 
I am just wondering what is the use of n-grams (n>3) (and their occurrence frequency) considering the computational overhead in computing them. Are there any applications where bigrams or trigrams are simply not enough? 
If so, what is the state-of-the-art in n-gram extraction? Any suggestions? I am aware of the following:

A new method of n-gram statistics for large number of n and automatic
extraction of words and phrases from large text data of Japanese
Using suffix arrays to compute term frequency and document frequency
for all substrings in a corpus 
Word association norms, mutual information, and lexicography
Retrieving collocations from text: Xtract

 This probably doesn't reach the level of information Legend is looking for, but this video from Pycon 2012 does a pretty good job explaining the basics of computing n-grams in python (and using them to build a search engine): pyvideo.org/video/715/building-a-python-based-search-engine . For anyone else who stumbles on this question. The "computational overhead" of computing ngrams is negligible: You can do it in a single pass through your corpus. Even storing higher-order ngrams is not a huge deal. The real cost is that for larger n, you need a bigger and bigger corpus to overcome sparsity problems. @alexis: It would be great if you could provide more information. Specifically, something related to sparsity problems, any research that shows "computational overhead of computing n-grams is negligible"? Thank You. @alexis: Just checking with you again (in regard to my comment again). Thanks. @Legend, did you see my answer below?
I'm not familiar with a good deal of the tags listed here, however n-grams (the abstract concept) are often useful related to statistical models. As a result, here's some applications which aren't restricted merely to bigrams and trigrams:

Compression algorithms (the PPM variety especially) where the length of the grams depends on how much data is available for providing specific contexts.
Approximate string matching (e.g. BLAST for genetic sequence matching)
Predictive models (e.g. name generators)
Speech recognition (phonemes grams are used to help evaluate the likelihood of possibilities for the current phoneme undergoing recognition)

Those are the ones off the top of my head, but there's much more listed on Wikipedia.
As far as "state-of-the-art" n-gram extraction, no idea. N-gram "extraction" is an adhoc attempt to speed up certain processes while still maintaining the benefits of n-gram style modeling. In short, "state-of-the-art" depends on what you're trying to do. If you're looking at fuzzy matching or fuzzy grouping, it depends on what kind of data you're matching/grouping. (E.g. street addresses are going to be very different to fuzzy match than first names.)

An (unconventional) way to think about higher order n-grams can be done by making the connection to an unnormalized autocorrelation function, i.e. the correlation of a signal with itself. A 2-gram corpus would measure the correlation of a word with a "time"-lag of a single word, while 3-gram can give us the information for a "time"-lag of two steps. Higher order n-grams give a measure of the probability distribution of a particular corpus (be it Moby Dick or human DNA). In this way, if an n-gram is distinct from the null expected value, then there is useful statistical information for that value of n.

I don't think your question is posed quite right: Ngrams are a tool, not a problem to be solved, so there is no "state of the art" in ngrams. As @Hooked pointed out, an ngram is a kind of auto-correlation function (or "autoregressive function"). So what you really want to know is if there are any problems for which the state of the art solutions involve long ngrams. 
For numerical applications such as fitting financial or weather models, or speech recognition, you'd definitely use vectors of dimension > 3. For example, autoregressive Hidden Markov Models fit a piecewise function of the last n measurements, where n can be moderately large if past states are relevant to predicting the future. 
But all your examples concern word ngrams, and I can't think of any work that found n > 3 to be useful in that domain. I don't think it's a question of computational cost or finding enough training data: Superficial auto-correlation in language seems to peter out after 3 words or so. Random example: this article tries to reinterpret Zipf's law in terms of ngram-based information content. They consider n up to 4, but get the highest overall correlations for the trigram counts. 
I don't mean to say that n > 3 is not useful; but your observation that it doesn't seem to come up much is well founded.
But note that the complexity of counting ngrams in a text is not an issue: If you have a tokenized corpus of length L, you could collect all ngrams of the corpus like this:
    for i in range(0, L-n):
        tuple = corpus[i:i+n]
        ngrams[tuple] += 1

As you can see this requires only O(L) steps, i.e., it is linear on the size of the corpus and does not grow with n. So collecting ngrams of any dimension is a non-issue. But the number of possible ngrams quickly mushrooms. To illustrate, if you distinguish 32 letter tokens (letters and some punctuation classes), there are 1024 letter bigrams but 1048576 tetragrams. To find enough of them to populate your frequency tables, you need exponentially more text. 
For word ngrams the sparsity problem is even worse, since not only do you have a lot more than 32 different word tokens, but the vocabulary size increases (slowly) with corpus size: the famous "long tail" property. So your data will be sparse (even for small n) no matter how large a corpus you collect. You'll then need to fit complicated statistical models, whose computation cost depends on the number of distinct ngrams. 
As a result, sparsity is always an issue in word ngram applications (hence "smoothing" is usually necessary). If you google "ngram sparsity" you'll find a ton of references.

In addition to Kaganar's answer:
Any kind of stylometric analysis (e.g., author profiling based on writing styles, or, trying to detect the epoch of a text) will require longer n-grams for shallow syntactic parsing. Usually such approaches are complemented by deep syntactic parsing based on PCFG, TAG, etc.

You can also use n>3 language models if your datset is very large.
 should be a commentThe relationship between latent Dirichlet allocation and documents clustering 
I would like to clarify the relationship between latent Dirichlet allocation (LDA) and the generic task of document clustering. 
The LDA analysis tends to output the topic proportions for each document. If my understanding is correct, this is not the direct result of document clustering. However, we can treat this probability proportions as a feature reprsentation for each document. Afterwards, we can invoke other established clustering method based on the feature configurations generated by LDA analysis.
Is my understanding correct? Thanks.

Yes, you can treat the output of LDA as features for your documents; this is exactly what Blei, Ng and Jordan did in the paper that introduced LDA. They did it for classification, but for clustering the procedure is the same.
(In machine learning terminology, this use of LDA is called dimensionality reduction because it reduces the feature space's number of dimensions from |V|, the vocabulary size, to some number k of topics selected by the user.)
 But in their paper, they claimed to use posterior dirichlet parameter $\gamma(w)$, which is different with probability proportion here. I agree that the underlying thoughts are the same in terms of feature reduction. But my concern is that why they choose to use $\gamma(w)$, which seems to me that it does not have very clear physical meaning like probability proportion. I am very curious about their underlying reasons. But I did not got clear explanations in the paper. @user: I'm not too familiar with the LDA internals. I suggest you try clustering on the proportions, and if it doesn't work, ask over at metaoptimize.com what the reasons for this choice are. Post a link here if you do, I'm very interested. the posterior $\gamma(w)$ can be understood as smoothed mixing proportions of topics.Decision Tree Learning and Impurity 
There are three ways to measure impurity:



What are the differences and appropriate use cases for each method?
 @David: See here: en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity and here: people.revoledu.com/kardi/tutorial/DecisionTree/…
If the p_i's are very small, then doing multiplication on very small numbers (Gini index) can lead to rounding error.  Because of that, it is better to add the logs (Entropy).  Classification error, following your definition, provides a gross estimate since it uses the single largest p_i to compute its value.
 I can't see how you would have those issues in splitting a node... Random forest classifiers use Gini impurity and have been reported to have higher accuracies than most other tree-based classifiers. @Benjamin: I see nothing in the question which is specific to splitting a node.
The difference between entropy and other impurity measures, and in fact often the difference between information theoretic approaches in machine learning and other approaches, is that entropy has been mathematically proven to capture the concept of 'information'.  There are many classification theorems (theorems that prove a particular function or mathematical object is the only object that satisfies a set of criteria) for entropy measures that formalize  philosophical arguments justifying their meaning as measures of 'information'.
Contrast this with other approaches (especially statistical methods) that are chosen not for their philosophical justification, but primarily for their empirical justification - that is they seem to perform well in experiments.  The reason why they perform well is because they contain additional assumptions that may happen to hold at the time of the experiment.  
In practical terms this means entropy measures (A) can't over-fit when used properly as they are free from any assumptions about the data, (B) are more likely to perform better than random because they generalize to any dataset but (C) the performance for specific datasets might not be as good as measures that adopt assumptions.
When deciding which measures to use in machine learning it often comes down to long-term vs short-term gains, and maintainability.  Entropy measures often work long-term by (A) and (B), and if something goes wrong it's easier to track down and explain why (e.g. a bug with obtaining the training data). Other approaches, by (C), might give short-term gains, but if they stop working it can be very hard to distinguish, say a bug in infrastructure with a genuine change in the data where the assumptions no longer hold.
A classic example where models suddenly stopped working is the global financial crisis.  Bankers where being given bonuses for short term gains, so they wrote statistical models that would perform well short term and largely ignored information theoretic models.

I found this description of impurity measures to be quite useful. Unless you are implementing from scratch, most existing implementations use a single predetermined impurity measure. Note also that the Gini index is not a direct measure of impurity, not in its original formulation, and that there are many more than what you list above.
I'm not sure that I understand the concern about small numbers and the Gini impurity measure... I can't imagine how this would happen when splitting a node.

I have seen various efforts at informal guidance on this, ranging from "if you use one of the usual metrics, there there won't be much difference", to much more specific recommendations.  In reality, the only way to know with certainty which measure works best is to try all of the candidates.
Anyway, here is some perspective from Salford Systems (the CART vendor):
Do Splitting Rules Really Matter?
Comparing/Clustering Trajectories (GPS data of (x,y) points) and Mining the data 
I've got 2 questions on analyzing a GPS dataset.
1) Extracting trajectories I have a huge database of recorded GPS coordinates of the form (latitude, longitude, date-time). According to date-time values of consecutive records, I'm trying to extract all trajectories/paths followed by the person. For instance; say from time M, the (x,y) pairs are continuously changing up until time N. After N, the change in (x,y) pairs decrease, at which point I conclude that the path taken from time M to N can be called a trajectory. Is that a decent approach to follow when extracting trajectories? Are there any well-known approaches/methods/algorithms you can suggest? Are there any data structures or formats you would like to suggest me to maintain those points in an efficient manner? Perhaps, for each trajectory, figuring out the velocity and acceleration would be useful?
2) Mining the trajectories Once I have all the trajectories followed/paths taken, how can I compare/cluster them? I would like to know if the start or end points are similar, then how do the intermediate paths compare? 
How do I compare the 2 paths/routes and conclude if they are similar or not. Furthermore; how do I cluster similar paths together?
I would highly appreciate it if you can point me to a research or something similar on this matter.
The development will be in Python, but all kinds of library suggestions are welcome.
Thanks in advance.
 Check out your tags.  None of them are very popular.  I'd consider changing gps to gis and definitely add the python tag. You might consider crossposting your question on gis.stackexchange.com Do you have GPS device ID in that database? @yura - No, but why does it matter?
Have a look at work done at Geography Department of University of Zurich, especially by Patrick Laube and Somayeh Dodge.
Have a look at the paper 

Individual Movements and Geographical Data Mining. Clustering
  Algorithms for Highlighting Hotspots in Personal Navigation Routes

(link, presentation). It showcases use of DBSCAN Kernel Density Estimation methods on GPS data.
Also papers from Nokia's Mobile Data Challenge 2012 Workshop can be helpful here, especially:  

MobReduce: Reducing State Complexity of Mobility Traces (link)

by Fabian Hartmann, Christoph P. Mayer, Ingmar Baumgart and

A Trajectory Cleaning Framework for Trajectory Clustering(link)

by Agzam Idrissov, Mario A. Nascimento, University of Alberta
 Those people are doing PhD quality work in exactly this area with a thesis that  "focuses on development of a methodology in order to reveal similarities between trajectories of different types of moving objects in space and time."
1) Extracting trajectories 
I think you are in right direction. There are probably will be some noise in gps data, and random walking, you should do some smooth like splines to overcome it. 

2) Mining the trajectories
Is there are any business sense in similar trajectories? (This will help build distance metric and then you can use some of mahoot clustering algorithms)
 1. I think point where some person stoped are more interesting so you can generate statistics for popularity of places.
 2. If you need route similarity to find different paths to same start-end you need to cluster first start end location and then similare curves by (maximum distance beetween, integral distance - some of well known functional metrics)
In scikit-learn, can DBSCAN use sparse matrix? 
I got Memory Error when I was running dbscan algorithm of scikit.
My data is about 20000*10000, it's a binary matrix.
(Maybe it's not suitable to use DBSCAN with such a matrix. I'm a beginner of machine learning. I just want to find a cluster method which don't need an initial cluster number)
Anyway I found sparse matrix and feature extraction of scikit.
http://scikit-learn.org/dev/modules/feature_extraction.html
http://docs.scipy.org/doc/scipy/reference/sparse.html
But I still have no idea how to use it. In DBSCAN's specification, there is no indication about using sparse matrix. Is it not allowed?
If anyone knows how to use sparse matrix in DBSCAN, please tell me.
Or you can tell me a more suitable cluster method.
 Possible duplicate of scikit-learn DBSCAN memory usage
The scikit implementation of DBSCAN is, unfortunately, very naive. It needs to be rewritten to take indexing (ball trees etc.) into account.
As of now, it will apparently insist of computing a complete distance matrix, which wastes a lot of memory.
May I suggest that you just reimplement DBSCAN yourself. It's fairly easy, there exists good pseudocode e.g. on Wikipedia and in the original publication. It should be just a few lines, and you can then easily take benefit of your data representation. E.g. if you already have a similarity graph in a sparse representation, it's usually fairly trivial to do a "range query" (i.e. use only the edges that satisfy your distance threshold)
Here is a issue in scikit-learn github where they talk about improving the implementation. A user reports his version using the ball-tree is 50x faster (which doesn't surprise me, I've seen similar speedups with indexes before - it will likely become more pronounced when further increasing the data set size).
Update: the DBSCAN version in scikit-learn has received substantial improvements since this answer was written.
 You might be interested in scikit-learn 0.14, where DBSCAN was indeed modified to use ball trees. also DBSCAN now supports sparse input in v0.16
You can pass a distance matrix to DBSCAN, so assuming X is your sample matrix, the following should work:
from sklearn.metrics.pairwise import euclidean_distances

D = euclidean_distances(X, X)
db = DBSCAN(metric="precomputed").fit(D)

However, the matrix D will be even larger than X: n_samples² entries. With sparse matrices, k-means is probably the best option.
(DBSCAN may seem attractive because it doesn't need a pre-determined number of clusters, but it trades that for two parameters that you have to tune. It's mostly applicable in settings where the samples are points in space and you know how close you want those points to be to be in the same cluster, or when you have a black box distance metric that scikit-learn doesn't support.)
 DBSCAN doesn't require the distance matrix, that is a limitation of the current sklearn implementation, not of the algorithm. Plus, in many cases, both the epsion and the minpts parameter of DBSCAN can be chosen much easier than k. When using geographic data for example, a user may well be able to say that a radius of "1 km" is a good epsilon, and that there should be at least 10 events within this radius. @Anony-Mousse: I'm aware that the problems are in the implementation, not the algorithm. As for picking eps and minpts, yes, for some problems that may be easy, but for others, extensive tuning may be required. Not all problems live in Euclidean space or even on the surface of the earth.
Yes, since version 0.16.1.
Here's a commit for a test:
https://github.com/scikit-learn/scikit-learn/commit/494b8e574337e510bcb6fd0c941e390371ef1879

Sklearn's DBSCAN algorithm doesn't take sparse arrays. However, KMeans and Spectral clustering do, you can try these. More on sklearns clustering methods: http://scikit-learn.org/stable/modules/clustering.html
Splitting data into training/testing datasets in MATLAB? 
Upon some research I found two functions in MATLAB to do the task:

cvpartition function in the Statistics Toolbox
crossvalind function in the Bioinformatics Toolbox

Now I've used the cvpartition to create n-fold cross validation subsets before, along with the Dataset/Nominal classes from the Statistics toolbox. So I'm just wondering what are the differences between the two and the pros/cons of each?

Expanding on @Mr Fooz's answer

They look to be pretty similar based on the official docs of cvpartition and crossvalind, but crossvalind looks slightly more flexible (it allows for leave M out for arbitrary M, whereas cvpartition only allows for leave 1 out).

... isn't it true that you can always simulate a leave-M-out using kfold cross validation with an appropriate k value (split data into k fold, test on one, train on all others, and do this for all folds and take average) since leave-one-out is a special case of kfold where k=number of observations?

Amro, this is not directly an answer to your cvpartition vs crossvalind question, but there is a contribution at the Mathworks File Exchange called MulticlassGentleAdaboosting  by user Sebastian Paris that includes a nice set of functions for enumerating array indices for computing training, testing and validation sets for the following sampling and cross-validation strategies:

Hold out
Bootstrap
K Cross-validation
Leave One Out
Stratified Cross Validation
Balanced Stratified Cross Validation
Stratified Hold out
Stratified Boot Strap

For details, see the demo files included in the package, and more specifically the functions sampling.m and sampling_set.m. 

They look to be pretty similar based on the official docs of cvpartition and crossvalind, but crossvalind looks slightly more flexible (it allows for leave M out for arbitrary M, whereas cvpartition only allows for leave 1 out).

I know your question is not directly referring to the Neural network toolbox, but perhaps someone else might find this useful. To get your ANN input data seperated in to test/validation/train data, use the 'net.divideFcn' variable.
net.divideFcn = 'divideind';

net.divideParam.trainInd=1:94;  % The first 94 inputs are for training.
net.divideParam.valInd=1:94;    % The first 94 inputs are for validation.
net.divideParam.testInd=95:100; % The last 5 inputs are for testing the network.

In TeamCity, is there a way of seeing a report of tests ordered by failed-most-often across the whole history? 
We have some unreliable tests - unreliable because of environmental reasons.
We'd like to see a history of which tests have failed the most often, so we can drill into why and fix the environment issue that causes that particular failure or class of failure.
Is this possible in TeamCity 6.0.3?
We know about individual test history (although that page is really hard to remember how to find!), but that pre-supposes we already know what we're actually trying to find out.

If you go to the "Current problems" tab for a project, there is a link like "tests failed within 120 hours" at the top. There is some statistics which may be relevant to what you're looking for.
UPDATE: In newer versions of TeamCity, this page is not available. But, there is a new Flaky tests tab, which shows information about tests which fail un-predictably, and this page includes test failure counters.
 Is there a way to get to that view when the build is not broken...? Use URL like /project.html?projectId=project123&tab=tests where project123 - ID of your project Oh the irony of needing to fix tests that are passing!Clustering Algorithm with discrete and continuous attributes? 
Does anyone know a good algorithm for perform clustering on both discrete and continuous attributes? I am working on a problem of identifying a group of similar customers and each customer has both discrete and continuous attributes (Think type of customers, amount of revenue generated by this customer, geographic location and etc..)
Traditionally algorithm like K-means or EM work for continuous attributes, what if we have  a mix of continuous and discrete attributes?

If I remember correctly, then COBWEB algorithm could work with discrete attributes.
And you can also do different 'tricks' to the discrete attributes in order to create meaningful distance metrics.
You could google for clustering of categorical/discrete attributes, one of the first hits: ROCK: A Robust Clustering Algorithm for Categorical Attributes.

R is a great tool for clustering - the standard approach would be to calculate a dissimilarity matrix on your mixed data using daisy, then clustering with that matrix using agnes.
The cba module on CRAN includes a function to cluster on binary predictors based on ROCK.

You could also look at affinity propagation as a possible solution.
But to overcome the continuous / discrete dilemma you need to define a function that values the discrete states.

I would actually present pairs of the discrete attributes to users and ask them to define their proximity. You would present them with a scale reaching from [synonym..very foreign] or similar. Having many people do this you will end up with a widely accepted proximity function for the non-linear attribute values.

How about transforming each of your categorical attributes into a series of N-1 binary indicator attributes (where N is the number of categories)? 
You shouldn't be afraid of high dimensionality, as a sparse representation (such as mahout's SequentialAccessSparseVector can be employed).
Once you do that, you can use a classical K-means or whatever standard numeric-only clustering algorithm.
Mixed variables (categorical and numerical) distance function 
I want to fuzzy cluster a set of jobs. 
Jobs Attributes are:

Categorical: position,diploma, skills 
Numerical : salary , years of experience 

My question is: how to calculate the distance between different jobs?
                  e.g job1(programmer,bs computer science,(java ,.net,responsibility),1500, 3)
and job2(tester,bs computer science,(black and white box testing),1200,1)
PS: I'm beginner in data mining clustering, I highly appreciate your help. 

You may take this as your starting point: 
http://www.econ.upf.edu/~michael/stanford/maeb4.pdf.  Distance between categorical data is nicely explained at the end.

Here is a good walk-through of several different clustering methods and how to use them in R:  http://biocluster.ucr.edu/~tgirke/HTML_Presentations/Manuals/Clustering/clustering.pdf
In general, clustering for discrete data is related to either the use of counts (e.g. overlaps in vectors) or related to some statistic derived from counts.  As much as I'd like to address the statistical side, I suppose you're interested in the algorithm, so I'll leave it at that.
Probabilistic Generation of Semantic Networks 
I've studied some simple semantic network implementations and basic techniques for parsing natural language. However, I haven't seen many projects that try and bridge the gap between the two.
For example, consider the dialog:
"the man has a hat"
"he has a coat"
"what does he have?" => "a hat and coat"

A simple semantic network, based on the grammar tree parsing of the above sentences, might look like:
the_man = Entity('the man')
has = Entity('has')
a_hat = Entity('a hat')
a_coat = Entity('a coat')
Relation(the_man, has, a_hat)
Relation(the_man, has, a_coat)
print the_man.relations(has) => ['a hat', 'a coat']

However, this implementation assumes the prior knowledge that the text segments "the man" and "he" refer to the same network entity.
How would you design a system that "learns" these relationships between segments of a semantic network? I'm used to thinking about ML/NL problems based on creating a simple training set of attribute/value pairs, and feeding it to a classification or regression algorithm, but I'm having trouble formulating this problem that way.
Ultimately, it seems I would need to overlay probabilities on top of the semantic network, but that would drastically complicate an implementation. Is there any prior art along these lines? I've looked at a few libaries, like NLTK and OpenNLP, and while they have decent tools to handle symbolic logic and parse natural language, neither seems to have any kind of proabablilstic framework for converting one to the other.

There is quite a lot of history behind this kind of task. Your best start is probably by looking at Question Answering. 
The general advice I always give is that if you have some highly restricted domain where you know about all the things that might be mentioned and all the ways they interact then you can probably be quite successful. If this is more of an 'open-world' problem then it will be extremely difficult to come up with something that works acceptably.
The task of extracting relationship from natural language is called 'relationship extraction' (funnily enough) and sometimes fact extraction. This is a pretty large field of research, this guy did a PhD thesis on it, as have many others. There are a large number of challenges here, as you've noticed, like entity detection, anaphora resolution, etc. This means that there will probably be a lot of 'noise' in the entities and relationships you extract.
As for representing facts that have been extracted in a knowledge base, most people tend not to use a probabilistic framework. At the simplest level, entities and relationships are stored as triples in a flat table. Another approach is to use an ontology to add structure and allow reasoning over the facts. This makes the knowledge base vastly more useful, but adds a lot of scalability issues. As for adding probabilities, I know of the Prowl project that is aimed at creating a probabilistic ontology, but it doesn't look very mature to me.
There is some research into probabilistic relational modelling, mostly into Markov Logic Networks at the University of Washington and Probabilstic Relational Models at Stanford and other places. I'm a little out of touch with the field, but this is is a difficult problem and it's all early-stage research as far as I know. There are a lot of issues, mostly around efficient and scalable inference.
All in all, it's a good idea and a very sensible thing to want to do. However, it's also very difficult to achieve. If you want to look at a slick example of the state of the art, (i.e. what is possible with a bunch of people and money) maybe check out PowerSet.
 PR-OWL looks interesting, in that they've realized a usable semantic network needs to handle uncertainty. However, I've never liked OWL because it was never clear to me how they reify entities. Plus, like you mentioned, the project appears to be vapor at this point with no actual published code.
Interesting question, I've been doing some work on a strongly-typed NLP engine in C#: http://blog.abodit.com/2010/02/a-strongly-typed-natural-language-engine-c-nlp/ and have recently begun to connect it to an ontology store.
To me it looks like the issue here is really: How do you parse the natural language input to figure out that 'He' is the same thing as "the man"?  By the time it's in the Semantic Network it's too late: you've lost the fact that statement 2 followed statement 1 and the ambiguity in statement 2 can be resolved using statement 1.  Adding a third relation after the fact to say that "He" and "the man" are the same is another option but you still need to understand the sequence of those assertions.
Most NLP parsers seem to focus on parsing single sentences or large blocks of text but less frequently on handling conversations.  In my own NLP engine there's a conversation history which allows one sentence to be understood in the context of all the sentences that came before it (and also the parsed, strongly-typed objects that they referred to).  So the way I would handle this is to realize that "He" is ambiguous in the current sentence and then look back to try to figure out who the last male person was that was mentioned.
In the case of my home for example, it might tell you that you missed a call from a number that's not in its database.  You can type "It was John Smith" and it can figure out that "It" means the call that was just mentioned to you.  But if you typed "Tag it as Party Music" right after the call it would still resolve to the song that's currently playing because the house is looking back for something that is ITaggable.
 The problem is generally called anaphora resolution. If you just restrict it to resolving 'he' and 'she' it's called pronoun resolution. There are systems that can do it, but not usually very well. Last time I used one, it caused a lot of errors because it couldn't tell that Barack Obama was a man's name. Anaphora/pronoun resolution is one issue, but the problem is more general than that. If I refer to "the man" in two different discourses, it may or may not refer to the same entity. In an open-world domain, this problem potentially applies to any word in a sentence. In that case it's usually called (cross-document) entity resolution. There was a contest on that a while ago: kdnuggets.com/news/2007/n08/10i.html Not sure what came of it. Yeah, I remember the Spock challenge. It was never well defined, and the company running it seemed to lose interest in the event and was never very responsive to participant's questions.
I'm not exactly sure if this is what you want, but take a look at natural language generation wikipedia, the "reverse" of parsing, constructing derivations that conform to the given semantical constraints.
 Yeah, thanks, but technically that's the exact opposite of what I'm looking for ;)How to store many years worth of 100 x 25 Hz time-series - Sql Server or timeseries database 
I am trying to identify possible methods for storing 100 channels of 25 Hz floating point data. This will result in 78,840,000,000 data-points per year. 
Ideally all this data would be efficiently available for Web-sites and tools such as Sql Server reporting services. We are aware that relational databases are poor at handling time-series of this scale but have yet to identify a convincing time-series specific database.
Key issues are compression for efficient storage yet also offering easy and efficient queries, reporting and data-mining.

How would you handle this data?
Are there features or table designs in Sql Server that could handle such a quantity of time-series data?
If not, are there any 3rd party extensions for Sql server to efficiently handle mammoth time-series?
If not, are there time-series databases that specialise in handling such data yet provide natural access through Sql, .Net, and Sql Reporting services?

thanks!
 How large are the data points? What's the datatype of the samples? Is the sample/feed rate fixed or varying?  Do you need to store the sample time or can it be inferred?  And what kind of data is being sampled? Assume 32-bit floating point. There could be some optimisation but it would be perfect if we didn't have to handle that head-ache. It's virtually impossible to deal with the compression issues of FP data unless we know what kind of data it is: is it real-world physical measurements or synthetic/artifical/generated data?  Is the data frequency-based, periodic or aperiodic?  Are the data points monotonic accumulators (never goes down), or are they fluctuating levels or are they rate measurements?  Are they mostly flat with occasional spikes (events) or highly variable and chaotic?  And how accurate is the data and how much noise is in it?  Finally, is lossy compression acceptable?
I'd partition the table by, say, date, to split the data into tiny bits of 216,000,000 rows each.
Provided that you don't need a whole-year statistics, this is easily servable by indexes.
Say, the query like "give me an average for the given hour" will be a matter of seconds.
 Thanks for the answer.   Using sql server partioning or just multiple tables? If its multiple tables, are there any design patterns for easily handling queries across tables? @Duncan: SQL Server paritioning would be the best, but I don't know how many partitions will it allow. With multiple tables, you can run a scheduler to recreate views as SELECT * FROM table_20090609 UNION ALL SELECT * FROM table_20090608 etc. Don't forget to include a partitioning column into the tables.
I suppose you need a random access to the data series. The idea that I've already used for rainfall data table, is to subdivide the entire dataset in smaller part, to create an entry for each few minutes or even one minute.
Then you can pop this, still big, array from the db and access directly to the needed part, you can find a direct correlation between time offset and byte offset.
 Thanks for the answer. Using a blob for the big array? Are there any design approaches for making a blob easily queryable e.g. views?
The feature set you're describing is for an Analysis Cube. Check out Analysis services from Microsoft if you're in that part of the tech world:
http://msdn.microsoft.com/en-us/library/ms175609(SQL.90).aspx
As far as the model you're describing, you need to implement a Kimball model (the standard data warehousing model) with a Time dimension. I ran into this problem storing media log files a while back.
Good luck.
 Thanks for the answer. Its difficult to know where to get started with data-warehousing. I have googled and read around your link but would benefit from something like a sample project that tackles a similar problem. Are you aware of anything like this? You're right that Data Warehousing is hard to get started with. The best process I can recommend is (and keep in mind that I'm talking Microsoft Visual Studio and Sql Server 2005/2008 here) to download the sample warehouse of the AdventureWorks db:  microsoft.com/downloads/…   and then check out some of the hands on labs for Sql Server:  microsoft.com/sqlserver/2008/en/us/virtual-labs.aspx  Lastly, I recommend Kimball's book to get started:  ralphkimball.com/html/booksDWT2.html  Good luck!
You can check out Infobright Community or Enterprise Edition, I think.
It is column-oriented storage designed for analytics purposes and big (existing installations up to 30 TB now as they say) data and good compression rate.
Data loader are also pretty fast and connectors exists for ETL-tools (Talend, kettle and so on).
Community edition available free under GNU GPL terms, but allows to add data only via native loader. Enterprise edition supports add/update by single row via DML.
Another benefit that you can use it with all tools that supports MySQL connections.
Column-orientation allow you, f.e., add columns for date component on every needed aggregation level (I use date,  week numbers, months and qtr.) for better performance, but it's good without it as well.
I use it for relatively small (yet) amount of business transactions data for analytic purposes with R as data analysis tool via mysql interface and python (numpy) scripts as some kind of ETL.
Cons:
lack of official utf-8 support, aggregation by function values (select month(date from ...)) not implemented yet (plan: july 2009, AFAIK), but I use ETL for this.
Link: http://www.infobright.org/Download/ICE/
 please feel free to share you experience after exploring ICE :) I’m working on architecture of our small analysis/reporting app with R, Infobright and Django as reports viewer and interested in new thoughts about storing/representing large data :)
You have 
A.  365 x 24 x 100 = 876,000 hourly signals (all channels) per year
B. each signal comprising 3600 * 25 = 90,000 datapoints
How about if you store data as one row per signal, with columns for summary/query stats for currently supported use cases, and a blob of the compressed signal for future ones?
 Thanks for the answer. I may not fully understand the suggestion. Is the suggestion for each row to like (signalId, timeperiod, float ave, float min, float max, blob raw)?  Are there any examples of making a blob data easily queryable e.g. views? Something along those lines, but I'm not sure about making blob data queryable at all..  My thinking was to limit queries to additional  stats columns as needed.
Have you considered a time series database, like http://opentsdb.net ? 

Have you considered HBASE or Open TSDB. You can also have a look to Cassandra

If it's floating point data only, TSDBs will offer you far better performance. Timeseries compression algorithms are different therefore you get better storage and query rates. 
How to perform collaborative filtering in R 
I'm have matrix data containing some null values. To fill the null values, I'd like to perform collaborative filtering. As I am studying for R, rather I'd like to use R.
So, Does anyone know how to perform collaborative filtering in R?
 I would start by looking at the recommenderlab package, which I found via library("sos"); findFn("{collaborative filtering}") Any particular variant or just user based will do? What have you tried?
Like Ben Bolker said, recommenderlab is the package in R that you can use. A detailed document for the same is available at http://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf
Java Support for PMML 
I am new in PMML: Predictive Model Markup Language (www.dmg.org) and I was wondering if there is some kind of Java support (Open Source / professional) for creating/parsing PMML files. 
Initially I only have in mind the possibility of creating/parsing PMML files programatically from Java environments. 
I have been "googling" and I have found several possibilities:
Open source:

jpmml. (PMML 3.2). 

From Java.

JDM. javax.datamining. Seems it a dead ? Someone has more info?

Professional.

Zementis (http://www.zementis.com/pmml_tools.htm).

DIY

Use an XML Java library and build yourself a parser/writer of PMML files

I appreciate all your opinions. 
Thanks in advance
Oscar
 Agreeing with nfechner here. On a higher level I would advice the use of jpmml or your own home-made tools if it's an exploration of JPMML in Java. If you (or your employer) plan to make use of this in some IT solution then a commercial library could be a better idea. Thanks for your messages!. nfechner, I just wanted an informal 'poll' (+ opinions) to figure out the possibilities of PMML parsing in a Java environment. That is, write-read pmml content programmatically using existing libraries, with the objective to not "reinventing the wheel" for this issue. Basically, I'll follow Wivani advice by the moment (jpmml + some DIY library).
You should realize that the answer may depend on the MODEL-ELEMENT that you want to work with. It is also very likely that your best options for creating PMML and parsing PMML will come from different software packages. I am going to assume that by 'creation of PMML' you mean of the document and not of the model. I've never heard of anyone integrating automatic model fitting with execution but perhaps it exists already. Certainly a PMML model could be passed using SOAP.
I can't speak to the other projects but the product offered by Zementis, called Adapa, is used only for the execution of PMML. This product assumes that there is a model fitting application that will do the creating by exporting a fitted model into PMML. There are already a lot of well developed model fitting applications so I think this is a reasonable assumption. 
The version I have used (3.6) was generally fast but it couldn't handle ensembles of typical random forest size (500+ trees) without an especially large heap. I think they may have fixed this in newer versions. Though it isn't advertised, Zementis doesn't appear to offer a few of the models, namely Text Models, Sequences, Baseline Models, or Time Series (for which the PMML standard currently only has Exponential Smoothing anyway). My version also doesn't have K-Nearest Neighbors but I hear that more recent versions do. 
Unless you are considering integrated fitting and execution (in which case you should consider online learning) my advise would be to consider these questions in order: 

What is the model type that I am interested in using?
What application/s do I prefer to build models in?
Then lastly how will I execute this and what requirements do I have in this regard (web-services, cloud, performance etc)?

If you look at the list of members to the DMG group you will find many commercial vendors that are either on the supply side (eg. SAS, SPSS, Togaware, Rapid-I) or the demand side (so many to list). 
On your list you also didn't mention Weka but they also execute some PMML models and there are R/Java based solutions and so you could execute PMML->R imports (see fileToXMLNode) in a Java environment (but you could also just execute R).
Finally, if you have a very specific model in mind and you understand what it means mathematically to 'execute it' then it shouldn't be too difficult to build what you need yourself. 
Data Mining Operation using SQL Query (Fuzzy Apriori Algorithm) - How do i code it using SQL? 
So i have this Table :  
Trans_ID    Name    Fuzzy_Value    Total_Item  
100          I1  0.33333333        3  
100          I2  0.33333333        3  
100          I5  0.33333333        3  
200          I2  0.5               2  
200          I5  0.5               2  
300          I2  0.5               2  
300          I3  0.5               2  
400          I1  0.33333333        3  
400          I2  0.33333333        3  
400          I4  0.33333333        3  
500          I1  0.5               2  
500          I3  0.5               2  
600          I2  0.5               2  
600          I3  0.5               2  
700          I1  0.5               2  
700          I3  0.5               2  
800          I1  0.25              4  
800          I2  0.25              4  
800          I3  0.25              4  
800          I5  0.25              4  
900          I1  0.33333333        3  
900          I2  0.33333333        3  
900          I3  0.33333333        3  
1000         I1  0.2               5  
1000         I2  0.2               5  
1000         I4  0.2               5  
1000         I6  0.2               5  
1000         I8  0.2               5  

And 2 Blank Table : 
Table  ITEMSET

"ITEM_SET" "Support" 



Table Confidence

"ANTECEDENT" "CONSEQUENT" 


I need to find FUZZY value for each item that occur in each transaction:  
I1 = Sum of (Fuzzy_Value from item I1 in trans 100 until 1000 which is trans: 100,400,500,700,800,900,1000)/Total Trans  
-> (.33333333+0.33333333+0.5+0.5+0.25+0.33333333+0.2)/10 = 0.244999999


I2 = Sum of (Fuzzy_Value from item I2 in trans 100 - 1000 which is trans:100,200,300,400,600,800,900,1000)/Total Trans  
-> (0.33333333+0.5+0.5+0.33333333+0.5+0.25+0.33333333)/10 = 0.274999999


I3 -> 0.258333333  
I4 -> 0.103333333  
I5 -> 0.058333333    
I6 -> 0.02    
I8 -> 0.02    

For EX: i use minimum Support 10% -> 0.1
I need to remove I5,I6,I8 since it's value < 0.1  => prune step 
then store 
I1=0.244999999, I2=0.274999999, I3=0.258333333,I4=0.103333333  on new table 'ITEMSET' 


2 COMBINATION
NOTE: this is the basic 1st step after this most likely need to use repeat or recursive, since the process will keep go on until no other item combination is possible
then from what's left i need to find K+1 itemset (which is 2 combination itemset) => join step
{I1,I2} =Sum of (Fuzzy_Value from item I1 + I2 in trans 100 - 1000 which is trans:100,400,800,900,1000)/Total Trans 
->(0.666666667+0.666666667+0.5+0.666666667+0.4)/9 = 0.29

*do the same for the rest*
{I1,I3} =(1+1+0.5+0.666666667)/9 = 0.316666667
{I1,I4} =(0.666666667+0.4)/9 = 0.106666667
{I2,I3} =(1+1+0.5+0.666666667)/9 = 0.316666667
{I2,I4} =(1+0.666666667+0.4)/9 =0.206666667
{I3,I4} =0  

Then Do another Prune Step removing less than 0.1 value which is {I3,I4}
Store {I1,I2} = 0.29, {I1,I3} = 0.316666667, {I1,I4} =0.106666667, {I2,I3} = 0.316666667, {I2,I4} = 0.206666667  AT "ITEMSET" TABLE 


3 COMBINATION 
After that Do another JOIN STEP combining itemset that pass pruning
{I1,I2,I3} = Sum of (Fuzzy_Value from item I1 + I2 +I3 in trans 100 - 1000 which is trans:800,900)/Total Trans  
-> 0.75+1 = 0.175  
**Same for the rest**  
{I1,I2,I4} = 1+0.6 = 0.16  
{I2,I3,I4} = 0  

Do another Prune Step removing less than 0.1 value which is {I1,I3,I4}  
Store {I1,I2,I3} = 0.176 AND {I1,I2,I4} = 0,16 AT "ITEMSET" TABLE  


4 COMBINATION
Combine itemset that pass pruning  K+4 (4 combination)
{I1,I2,I3,I4} = 0

**since no transaction containing this item
after process stop since there's no possible combination left 

at this point ITEMSET database have :  
ITEM_SET           Support  
{I1}               0.244999999
{I2}               0.274999999     
{I3}               0.258333333    
{I4}               0.103333333  
{I1,I2}            0.29    
{I1,I3}            0.316666667  
{I1,I4}            0.106666667  
{I2,I3}            0.316666667  
{I2,I4}            0.206666667  
{I1,I2,I3}         0.176  
{I1,I2,I4}         0,16  


how do i code that in sql?
thank you very much 
*note: you can add another table as needed
 You could always fix the formatting instead of apologizing for it. I'd start by replacing those &nbsp;s with regular spaces, and then highlighting the whole table and hitting the 'code' button. The one that looks like two rows of binary. code button, thx i'll fix it now There it should be better, THX again
Step 1:
CREATE TABLE ITEMSET
SELECT Name, SUM(Fuzzy_Value)/COUNT(*) Fuzzy_Value
FROM trans
GROUP BY ID
HAVING ROUND(SUM(Fuzzy_Value), 1) >= 0.1

Note the ROUND() function - it's important, because you have values like .33333 that don't sum in a happy way.
Step 2:
ALTER TABLE ITEMSET ADD INDEX (Name)

SELECT a.Name Name1, b.Name Name2, SUM(Fuzzy_Value)/COUNT(*) Fuzzy_Value
FROM ITEMSET a JOIN ITEMSET b ON (a.Name != b.Name)
GROUP BY a.Name, b.Name
HAVING ROUND(SUM(Fuzzy_Value), 1) >= 0.1

Opps: I just noticed that you asked this half a year ago, so I guess there is no point in continuing. If you still need this answer leave a comment.
 If you started you should have finnish it because there are others that may need it.Is there any reason to prefer functional programming for data mining projects? [closed] 






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                
Closed 4 years ago.




I am researching the possibility of starting a data mining project which will include intensive calculations and transformation on data, and should be relatively easy to scale.
In your experience, is the choice of programming language critical for said project?
For example, if I am already working on a JVM environment, should I prefer Clojure over plain Java? Does the functional environment guarantee easier scalability? Better performance?
Put aside other factors such as familiarity with the language, toolchain, etc. In your experience, is the choice of language a critical one?
 The factors you "put aside" are critical. If you start learning Clojure just for this project, you'll either fail to apply its strength - in which case you could just stick with e.g. Java - or lose so much time that it would have to be really awesome for this task to compensate for the lost time.
There are a few good reasons for choosing functional programming for data mining projects.

Usually data mining projects
involve algorithmics and mathematics
(than other types of systems) which
can be more easily expressed in
functional programming
Data
mining projects would involve
aggregate functions - which are better in functional
programming, say Clojure
Data
mining programs also would be more
suitable to parallelism - definitely
data parallelism and could even be
task parallelism, again a forte of
functional programming
And
functional languages like Clojure
can interface with java anyway for I/O, file read and write
I
think one can learn the tool chain
easily; it is not that different and so that shouldn't be a factor.

I was asking the same question myself and came with a big Yes for Clojure - am still thinking through how to include R in the mix.
 Plus, data mining operations can be seen as a chain of data transformations, something which functional programming languages are very good at. Look at Incanter its like R but in Clojure.
Use the most powerful language you are comfortable with.  
In any case, if you want to get scalability you need to have a map-reduce implementation which allow you to parallellize and collect the results.  

No particular reason. Pick whatever language you feel most comfortable with.
See my answer to a similar question about natural language processing. I think that some of the features people think obscure languages are suited to AI are really counterproductive.
 "I think that some of the features people think obscure languages are suited to AI are really counterproductive." << Can you please elaborate on this, preferably with examples? @missingfaktor: See the answer I linked to.
Often, functional programming solutions are more scalable.
 Even more often, they are not.Iregular plot of k-means clustering, outlier removal 
Hi I'm working on trying to cluster network data from the 1999 darpa data set. Unfortunately I'm not really getting clustered data, not compared to some of the literature, using the same techniques and methods. 
My data comes out like this:

As you can see, it is not very Clustered. This is due to a lot of outliers (noise) in the dataset. I have looked at some outlier removal techniques but nothing I have tried so far really cleans the data. One of the methods I have tried:
%% When an outlier is considered to be more than three standard deviations away from the mean, determine the number of outliers in each column of the count matrix:

    mu = mean(data)
    sigma = std(data)
    [n,p] = size(data);
    % Create a matrix of mean values by replicating the mu vector for n rows
    MeanMat = repmat(mu,n,1);
    % Create a matrix of standard deviation values by replicating the sigma vector for n rows
    SigmaMat = repmat(sigma,n,1);
    % Create a matrix of zeros and ones, where ones indicate the location of outliers
    outliers = abs(data - MeanMat) > 3*SigmaMat;
    % Calculate the number of outliers in each column
    nout = sum(outliers) 
    % To remove an entire row of data containing the outlier
    data(any(outliers,2),:) = [];

In the first run, it removed 48 rows from the 1000 normalized random rows which are selected from the full dataset. 
This is the full script I used on the data:
    %% load data
        %# read the list of features
        fid = fopen('kddcup.names','rt');
        C = textscan(fid, '%s %s', 'Delimiter',':', 'HeaderLines',1);
        fclose(fid);

        %# determine type of features
        C{2} = regexprep(C{2}, '.$','');              %# remove "." at the end
        attribNom = [ismember(C{2},'symbolic');true]; %# nominal features

        %# build format string used to read/parse the actual data
        frmt = cell(1,numel(C{1}));
        frmt( ismember(C{2},'continuous') ) = {'%f'}; %# numeric features: read as number
        frmt( ismember(C{2},'symbolic') ) = {'%s'};   %# nominal features: read as string
        frmt = [frmt{:}];
        frmt = [frmt '%s'];                           %# add the class attribute

        %# read dataset
        fid = fopen('kddcup.data_10_percent_corrected','rt');
        C = textscan(fid, frmt, 'Delimiter',',');
        fclose(fid);

        %# convert nominal attributes to numeric
        ind = find(attribNom);
        G = cell(numel(ind),1);
        for i=1:numel(ind)
            [C{ind(i)},G{i}] = grp2idx( C{ind(i)} );
        end

        %# all numeric dataset
        fulldata = cell2mat(C);

%% dimensionality reduction 
columns = 6
[U,S,V]=svds(fulldata,columns);

%% randomly select dataset
rows = 1000;
columns = 6;

%# pick random rows
indX = randperm( size(fulldata,1) );
indX = indX(1:rows)';

%# pick random columns
indY = indY(1:columns);

%# filter data
data = U(indX,indY);

% apply normalization method to every cell
maxData = max(max(data));
minData = min(min(data));
data = ((data-minData)./(maxData));

% output matching data
dataSample = fulldata(indX, :)

%% When an outlier is considered to be more than three standard deviations away from the mean, use the following syntax to determine the number of outliers in each column of the count matrix:

mu = mean(data)
sigma = std(data)
[n,p] = size(data);
% Create a matrix of mean values by replicating the mu vector for n rows
MeanMat = repmat(mu,n,1);
% Create a matrix of standard deviation values by replicating the sigma vector for n rows
SigmaMat = repmat(sigma,n,1);
% Create a matrix of zeros and ones, where ones indicate the location of outliers
outliers = abs(data - MeanMat) > 2.5*SigmaMat;
% Calculate the number of outliers in each column
nout = sum(outliers) 
% To remove an entire row of data containing the outlier
data(any(outliers,2),:) = [];

%% generate sample data
K = 6;
numObservarations = size(data, 1);
dimensions = 3;

%% cluster
opts = statset('MaxIter', 100, 'Display', 'iter');
[clustIDX, clusters, interClustSum, Dist] = kmeans(data, K, 'options',opts, ...
'distance','sqEuclidean', 'EmptyAction','singleton', 'replicates',3);

%% plot data+clusters
figure, hold on
scatter3(data(:,1),data(:,2),data(:,3), 5, clustIDX, 'filled')
scatter3(clusters(:,1),clusters(:,2),clusters(:,3), 100, (1:K)', 'filled')
hold off, xlabel('x'), ylabel('y'), zlabel('z')
grid on
view([90 0]);

%% plot clusters quality
figure
[silh,h] = silhouette(data, clustIDX);
avrgScore = mean(silh);

This is two distinct clusters from the output:

As you can see the data looks cleaner and more clustered than the original. However I still think a better method can be used. 
For instance observing the overall clustering, I still have a lot of noise (outliers) from the dataset. As can be seen here:

I need the outlier rows put into a seperate dataset for later classification (only removed from the clustering) 
Here is a link to the darpa dataset, please note that the 10% data set has had significant reduction in columns, a majority of columns which have 0 or 1's running through-out have been removed (42 columns to 6 columns):
http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html
EDIT
Columns kept in the dataset are:
src_bytes: continuous.

dst_bytes: continuous.

count: continuous.

srv_count: continuous.  

dst_host_count: continuous.

dst_host_srv_count: continuous.         

RE-EDIT:
Based on discussions with Anony-Mousse and his answer, there may be a way of reducing noise in the clustering using K-Medoids http://en.wikipedia.org/wiki/K-medoids. I'm hoping that there isnt much of a change in the code that I currently have but as of yet I do not know how to implement it to test whether this will significantly reduce the noise. So providing that someone can show me a working example this will be accepted as an answer. 
 it doesn't seem like your data is very clustered.. seems like a big blob and some points around it. what do you expect the clustering algorithm to do? if you want to separate the middle from the outliers, maybe hierarchical clustering will do better. can you put a link to an example about this data? I'm not familiar with it. @Ran hey updated question with a link to the data. @JungleBoogie: what have you tried yourself? Most of the code above is simply reusing previous answers. While this is fine, your are now basically asking us to implement outlier detection for you. Your question as stated doesn't show the effort you may have made in searching for a solution. Unfortunately, posting a big reward does not change that fact. @JungleBoogie: the error you are getting is because you did not update numObservarations after you removed rows from the dataset. FWIW the whole section "Assign data to clusters" is not needed, I simply had it in the original post to show how to compute distances between instances and centroids. That information is already provided by kmeans in the clustIDX argument
First things first: you're asking for a lot here. For future reference: try to break up your problem in smaller chunks, and post several questions. This increases your chances of getting answers (and doesn't cost you 400 reputation!).
Luckily for you, I understand your predicament, and just love this sort of question!
Apart from this dataset's possible issues with k-means, this question is still generic enough to apply also to other datasets (and thus Googlers ending up here looking for a similar thing), so let's go ahead and get this solved.
My suggestion is we edit this answer until you get reasonably satisfactory results. 
Number of clusters
Step 1 of any clustering problem: how many clusters to choose? There are a few methods I know of with which you can select the proper number of clusters. There is a nice wiki page about this, containing all of the methods below (and a few more).
Visual inspection
It might seem silly, but if you have well-separated data, a simple plot can tell you already (approximately) how many clusters you'll need, just by looking.
Pros:

quick
simple
works well on well-separated clusters in relatively small datasets

Cons:

and dirty
requires user interaction
it's easy to miss smaller clusters
data with less-well separated clusters, or very many of them, are hard to do by this method
it is all rather subjective -- the next person might select a different amount than you did.

silhouettes plot
As indicated in  one of your other questions, making a silhouettes plot will help you make a better decision about the proper number of clusters in your data.
Pros:

relatively simple
reduces subjectivity by using statistical measures
intuitive way to represent quality of the choice

Cons:

requires user interaction
In the limit, if you take as many clusters as there are datapoints, a silhouettes plot will tell you that that is the best choice
it is still rather subjective, not based on statistical means
can be computationally expensive

elbow method
As with the silhouettes plot approach, you run kmeans repeatedly, each time with a larger amount of clusters, and you see how much of the total variance in the data is explained by the clusters chosen by this kmeans run. There will be a number of clusters where the amount of explaned variance will suddenly increase a lot less than in any of the previous choices of the number of clusters (the "elbow"). The elbow is then statistically speaking the best choice for the number of clusters.
Pros:

no user interaction required -- the elbow can be selected automatically
statistically more sound than any of the aforementioned methods

Cons:

somewhat complicated
still subjective, since the definition of the "elbow" depends on subjectively chosen parameters
can be computationally expensive

Outliers
Once you have chosen the number of clusters with any of the methods above, it is time to do outlier detection to see if the quality of your clusters improves.
I would start by a two-step-iterative approach, using the elbow method. In pseudo-Matlab:
data = your initial dataset
dataMod = your initial dataset

MAX = the number of clusters chosen by visual inspection

while (forever)

    for N = MAX-5 : MAX+5
        if (N < 1), continue, end
        perform k-means with N clusters on dataMod
        if (variance explained shows a jump)
            break
    end

    if (you are satisfied)
        break
    end

    for i = 1:N
        extract all points from cluster i 
        find the centroid (let k-means do that)
        calculate the standard deviation of distances to the centroid
        mark points further than 3 sigma as possible outliers
    end

    dataMod = data with marked points removed

end

The tough part is obviously determining whether you are satisfied. 
This is the key to the algorithm's effectiveness. The rough structure of 
this part
if (you are satisfied)
    break
end

would be something like this
if (situation has improved)
    data = dataMod

elseif (situation is same or worse)
    dataMod = data
    break            
end

the situation has improved when there are fewer outliers, or the variance 
explaned for ALL choices of N is better than during the previous loop in the while. This is also something to fiddle with. 
Anyway, much more than a first attempt I wouldn't call this. 
If anyone sees incompletenesses, flaws or loopholes here, please 
comment or edit. 
 You probably remove normal data from the data set with this "outlier removal". The normal data is much more spread out; the attack data comes in massive burst (and makes up 80% of the data set) that cluster somewhat and that is much more self-similar.
Note that using this dataset is discouraged:
That dataset has errors: KDD Cup '99 dataset (Network Intrusion) considered harmful
Reconsider using a different algorithm. k-means is not really appropriate for mixed-type data, where many attributes are discrete, and have very different scales. K-means needs to be able to compute sensible means. And for a binary vector "0.5" is not a sensible mean, it should be either 0 or 1.
Plus, k-means doesn't like outliers too much.
When plotting, make sure to scale them equally, or the result will look incorrect. You X-axis has a length of around 0.9, your y axis only 0.2 - no wonder they look squashed.
Overall, maybe the data set just doesn't have k-means-style clusters? You definitely should try a density-based methods (because these can deal with outliers) such as DBSCAN. But judging from the visualizations you added, I'd say it has at most 4-5 clusters, and they are not really interesting. They probably can be captured with a number of thresholds in some dimensions.

Here is a visualization of the data set after z-normalization, visualized in parallel coordinates, with 5000 samples. Bright green is normal.
You can clearly see special properties of the data set. All of the attacks are clearly different in attributes 3 and 4 (count and srv_count) and also most very concentrated in dst_host_count and dst_host_srv_count.
I've ran OPTICS on this data set, too. It found a number of clusters, most of them in the wine-colored attack pattern. But they're not really interesting. If you have 10 different hosts ping-flooding, they will form 10 clusters.

You can see very well that OPTICS managed to cluster a number of these attacks. It missed all the orange stuff (maybe if I had set minpts lower, it is quite spread out) but it even discovered *structure within the wine-colored attack), breaking it into a number of separate events.
To really make sense of this data set, you should start with feature extraction, for example by merging such ping flood connection attempts into an aggregate event.
Also note that this is an unrealistic scenario.

There are well-known patterns involved in attacks, in particular port scans. These are best detected with a specialized port scan detector, not with learning.
The simulated data has a lot of completely pointless "attacks" simulated. For example Smurf attack from the 90s, is >50% of the data set, and Syn flood is another 20%; while normal traffic is <20%!
For these kind of attacks, there are well-known signatures.
Much of modern attacks (SQL injection, for example) flow with usual HTTP traffic, and will not show up anomalous in raw traffic pattern.

Just don't use this data for classification or outlier detection. Just don't.
Quoting the KDNuggets link above:

As a result, we strongly recommend that
(1) all researchers stop using the KDD Cup '99 dataset,
(2) The KDD Cup and UCI websites include a warning on the KDD Cup '99 dataset webpage informing researchers that there are known problems with the dataset, and
(3) peer reviewers for conferences and journals ding papers (or even outright reject them, as is common in the network security community) with results drawn solely from the KDD Cup '99 dataset. 

This is neither real nor realistic data. Go get something else.
 Also please note there is no TTL within the data and no data attributes for attacks that result in the majority having numbers of 253 or 126 regardless. If you also look at his references they do not corroborate his attack on the darpa training set either. Actually I only know statements that say "k-means doesn't work on data that has outliers", and that if you have outliers, you should try using k-medians or k-medoids instead. Outlier is defined as a noisy observation, which does not fit to the assumed model that generated the data. In clustering, outliers are considered as observations that should be removed in order to make clustering more reliable. The ability to detect outliers can be improved using a combined perspective of outlier detection and clustering. Some clustering algorithms, for example DBSCAN and ROCK, handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers. Kmeans outlier detection I just dont nor find it appropriate to tell everyone exactly everything im doing, I only wanted a simple way to remove some noise from Kmeans lol @JungleBoogie: don't be too quick to judge. I just tried it myself and it is working as advertised... And I have no idea what Javascript you are talking about, the code is right there in front of you! You don't like it, I can see three other implementations in the first page of a google search. Better yet implement your own...Cosine similarity when one of vectors is all zeros 
How to express the cosine similarity ( http://en.wikipedia.org/wiki/Cosine_similarity ) 
when one of the vectors is all zeros?
v1 = [1, 1, 1, 1, 1]
v2 = [0, 0, 0, 0, 0]
When we calculate according to the classic formula we get division by zero:
Let d1 = 0 0 0 0 0 0
Let d2 = 1 1 1 1 1 1
Cosine Similarity (d1, d2) =  dot(d1, d2) / ||d1|| ||d2||dot(d1, d2) = (0)*(1) + (0)*(1) + (0)*(1) + (0)*(1) + (0)*(1) + (0)*(1) = 0

||d1|| = sqrt((0)^2 + (0)^2 + (0)^2 + (0)^2 + (0)^2 + (0)^2) = 0

||d2|| = sqrt((1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2 + (1)^2) = 2.44948974278

Cosine Similarity (d1, d2) = 0 / (0) * (2.44948974278)
                           = 0 / 0

I want to use this similarity measure in a clustering application.
And I often will need to compare such vectors.
Also [0, 0, 0, 0, 0] vs. [0, 0, 0, 0, 0]
Do you have any experience?
Since this is a similarity (not a distance) measure should I use special case for
d( [1, 1, 1, 1, 1]; [0, 0, 0, 0, 0] ) = 0
d([0, 0, 0, 0, 0]; [0, 0, 0, 0, 0] ) = 1
what about 
d([1, 1, 1, 0, 0]; [0, 0, 0, 0, 0] ) = ? etc.

If you have 0 vectors, cosine is the wrong similarity function for your application.
Cosine distance is essentially equivalent to squared Euclidean distance on L_2 normalized data. I.e. you normalize every vector to unit length 1, then compute squared Euclidean distance.
The other benefit of Cosine is performance - computing it on very sparse, high-dimensional data is faster than Euclidean distance. It benefits from sparsity to the square, not just linear.
While you obviously can try to hack the similarity to be 0 when exactly one is zero, and maximal when they are identical, it won't really solve the underlying problems.
Don't choose the distance by what you can easily compute.
Instead, choose the distance such that the result has a meaning on your data. If the value is undefined, you don't have a meaning...
Sometimes, it may work to discard constant-0 data as meaningless data anyway (e.g. analyzing Twitter noise, and seeing a Tweet that is all numbers, no words). Sometimes it doesn't.
 What would a more appropriate similarity measure be in this case then? Hamming distance? There is no context given. Euclidean distance could also be "more appropriate".
It is undefined.
Think you have a vector C that is not zero in place your zero vector. Multiply it by epsilon > 0 and let run epsilon to zero. The result will depend on C, so the function is not continuous when one of the vectors is zero.
Analyzing noisy data 
I recently launched a rocket with a barometric altimeter that is accurate to roughly 10 ft (calculated via data acquired during flight).  The recorded data is in time increments of 0.05 sec per sample and a graph of altitude vs. time looks pretty much like it should when zoomed out over the entire flight.
The problem is when I try to calculate other values such as velocity or acceleration from the data, the accuracy of the measurements makes the calculated values pretty much worthless.  What techniques can I use to smooth out the data so that I can calculate (or approximate) reasonable values for the velocity and acceleration?  It is important that major events remain in place in time, most notably the 0 for for the first entry and the highest point during flight (2707).
The altitude data follows and is measured in ft above ground level.  The first time would be 0.00 and each sample is 0.05 seconds after the previous sample.  The spike at the beginning of the flight is due to a technical problem that occurred during liftoff and removing the spike is optimal.
I originally tried using linear interpolation, averaging nearby data points, but it took many iterations to smooth the data enough for integration and the flattening of the curve removed the important apogee and ground level events.  
All help is greatly appreciated.  Please note this is not the complete data set and I am looking for suggestions on better ways to analyze the data, not for someone to reply with a transformed data set.  It would be nice to use an algorithm on board future rockets which can predict current altitude/velocity/acceleration without knowing the full flight data, though that is not required.
00000
00000
00000
00076
00229
00095
00057
00038
00048
00057
00057
00076
00086
00095
00105
00114
00124
00133
00152
00152
00171
00190
00200
00219
00229
00248
00267
00277
00286
00305
00334
00343
00363
00363
00382
00382
00401
00420
00440
00459
00469
00488
00517
00527
00546
00565
00585
00613
00633
00652
00671
00691
00710
00729
00759
00778
00798
00817
00837
00856
00885
00904
00924
00944
00963
00983
01002
01022
01041
01061
01080
01100
01120
01139
01149
01169
01179
01198
01218
01238
01257
01277
01297
01317
01327
01346
01356
01376
01396
01415
01425
01445
01465
01475
01495
01515
01525
01545
01554
01574
01594
01614
01614
01634
01654
01664
01674
01694
01714
01724
01734
01754
01764
01774
01794
01804
01814
01834
01844
01854
01874
01884
01894
01914
01924
01934
01954
01954
01975
01995
01995
02015
02015
02035
02045
02055
02075
02075
02096
02096
02116
02126
02136
02146
02156
02167
02177
02187
02197
02207
02217
02227
02237
02237
02258
02268
02278
02278
02298
02298
02319
02319
02319
02339
02349
02359
02359
02370
02380
02380
02400
02400
01914
02319
02420
02482
02523
02461
02502
02543
02564
02595
02625
02666
02707
02646
02605
02605
02584
02574
02543
02543
02543
02543
02543
02543
02554
02543
02554
02554
02554
02554
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02543
02533
02543
02543
02543
02543
02543
02543
02543
02543
02533
02523
02523
02523
02523
02523
02523
02523
02523
02543
02523
02523
02523
02523
02523
02523
02523
02523
02513
02513
02502
02502
02492
02482
02482
02482
02482
02482
02482
02482
02482
02482
02482
02482
02482
02482
02482
02482
02472
02472
02472
02461
02461
02461
02461
02451
02451
02451
02461
02461
02451
02451
02451
02451
02451
02451
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02441
02431
02441
02431
02441
02431
02420
02431
02420
02420
02420
02420
02420
02420
02420
02420
02420
02420
02420
02420
02410
02420
02410
02410
02410
02410
02400
02400
02410
02400
02400
02400
02400
02400
02400
02400
02400
02400
02400
02400
02400
02390
02390
02390
02380
02380
02380
02380
02380
02380
02380
02380
02380
02380
02380
02380
02380
02370
02370
02380
02370
02359
02359
02359
02359
02359
02359
02359
02359
02359
02359
02359
02359
02359
02359
02349
02349
02349
02349
02349
02339
02339
02339
02339
02339
02339
02339
02339
02339
02339
02339
02339
02339

 please use the metric system. We don't want you to accidentally crash on the moon when your target was the neighbor cornfield ;)
Here is my solution, using a Kalman filter. You will need to tune the parameters (even +- orders of magnitude) if you want to smooth more or less.
#!/usr/bin/env octave

% Kalman filter to smooth measures of altitude and estimate
% speed and acceleration. The continuous time model is more or less as follows:
% derivative of altitude := speed
% derivative of speed := acceleration
% acceleration is a Wiener process

%------------------------------------------------------------
% Discretization of the continuous-time linear system
% 
%   d  |x|   | 0 1 0 | |x|
%  --- |v| = | 0 0 1 | |v|   + "noise"
%   dt |a|   | 0 0 0 | |a|
%
%   y = [1 0 0] |x|     + "measurement noise"
%               |v|
%               |a|
%
st = 0.05;    % Sampling time
A = [1  st st^2/2;
     0  1  st    ;
     0  0  1];
C = [1 0 0];

%------------------------------------------------------------
% Fine-tune these parameters! (in particular qa and R)
% The acceleration follows a "random walk". The greater is the variance qa,
% the more "reactive" the system is expected to be, i.e.
% the more the acceleration is expected to vary
% The greater is R, the more noisy is your measurement instrument
% (less "accuracy" of the barometric altimeter);
% if you increase R, you will smooth the estimate more
qx = 1.0;                      % Variance of model noise for position
qv = 1.0;                      % Variance of model noise for speed
qa = 50.0;                     % Variance of model noise for acceleration
Q  = diag([qx, qv, qa]);
R  = 100.0;                    % Variance of measurement noise
                               % (10^2, if 10ft is the standard deviation)

load data.txt  % Put your measures in this file

est_position     = zeros(length(data), 1);
est_speed        = zeros(length(data), 1);
est_acceleration = zeros(length(data), 1);

%------------------------------------------------------------
% Kalman filter
xhat = [0;0;0];     % Initial estimate
P    = zeros(3,3);  % Initial error variance
for i=1:length(data),
   y = data(i);
   xpred = A*xhat;                                    % Prediction
   Ppred = A*P*A' + Q;                                % Prediction error variance
   Lambdainv = 1/(C*Ppred*C' + R);
   xhat  = xpred + Ppred*C'*Lambdainv*(y - C*xpred);  % Update estimation
   P = Ppred - Ppred*C'*Lambdainv*C*Ppred;            % Update estimation error variance
   est_position(i)     = xhat(1);
   est_speed(i)        = xhat(2);
   est_acceleration(i) = xhat(3);
end

%------------------------------------------------------------
% Plot
figure(1);
hold on;
plot(data, 'k');               % Black: real data
plot(est_position, 'b');       % Blue:  estimated position
plot(est_speed, 'g');          % Green: estimated speed
plot(est_acceleration, 'r');   % Red:   estimated acceleration
pause

 I've started reading about this and it looks very promising.  I like how this can be adapted to take multiple input sources. The code ran fine and plots. Nice job. But I'm not sure the acceleration plot is right - it imitates the velocity too closely, not its derivative.  A flaw in the code, or is this a quirk of Kalman filtering?
You could try running the data through a low-pass filter. This will smooth out high frequency noise. Maybe a simple FIR.
Also, you could pull your major events from the raw data, but use a polynomial fit for velocity and acceleration data.
 I like your comment about the polynomial fit.  Perhaps the flight could be divided in two: before the thrust is finished and after.  After the thrust a parabola would be a natural fit for a polynomial and before that some polynomial with a slightly higher order (3 or 4?).  Extreme values such as the early "229" should become outliers and disappear. This answer is right on track. It just needs a specific name to look up... since acceleration and velocity are derivatives respect to time, you should look into Savitzky-Golay.  It's described in Numerical Recipes and online in many places.  Defined as a low-order polynomial fit at each point, it smooths the data and takes a derivative order as a parameter.  This is better numerically than smoothing and then differentiating in separate steps.  S.G. is especially good at preserving peaks, inflection points etc whereas naive attempts at smoothing typically mush out peaks and other fine detail.
have you tried performing a scrolling window average of your values ? Basically you perform a window of, say 10 values (from 0 to 9), and calculate its average. then you scroll the window one point (from 1 to 10) and recalculate. This will smooth the values while keeping the number of points relatively unchanged. Larger windows give smoother data at the price of loosing more high-frequency information.
You can use the median instead of the average if your data happen to present outlier spikes.
You can also try with Autocorrelation.
 I actually did try this, however, I only tried it over a window size of 3.  Perhaps some bigger window tests would make for better data, +1 The shape of the window matters.  If you take a plain simple average of n points to the left and n to the right, along with the point at the center, you get some noise in the output because the points at the ends are entering/leaving the range as you move to calculate the next output point.  Is better to calculate a weighted average - less weight given to points the further they are from the central point.  See other answers for good ways to do this. @DarenW : that's a very good idea. as you said, this way you reduce the "all-or-nothing" presence of a point...
One way you can approach analyzing you data is to try to match it too some model, generate a function, and then test its fitness to your data set.... This can be rather complicated and is probably unnecessary... but the point is that instead of generating acceleration/velocity data directly from you data you can match it to your model (rather simple for a rocket, some acceleration upwards followed by a slow constant speed descent.)  At least that how i would do it in a physics experiment.
As for generating some sense of velocity and acceleration during flight this should be as simple averaging the velocity from several different results.  Something along the lines of:
EsitimatedV = Vmeasured*(1/n) + (1 - 1/n)*EstimatedV.  Set n based on how quickly you want your velocity to adjust by.  

I know nothing about rockets. I plotted your points and they look lovely.
Based on what I see in that plot let me assume that there is usually a single apogee and that the function that gave rise to your points has no derivative wrt time at that apogee.
Suggestion:

Monitor maximum altitude throughout the flight.
Continuously watch for the apogee by (say, simply) comparing the most recent few points with the current maximum.
Until you reach the maximum, with (0,0) fixed and some arbitrary set of knots calculate a collection of natural splines up to the current altitude. Use the residuals wrt the splines to decide which data to discard. Recalculate the splines.
At the maximum retain the most recently calculate splines. Start calculating a new set of splines for the curve beyond the apogee.

 From my understanding of splines is that they are more useful for generating exact replicas of continuous data than they are interpolating missing or noisy data points.  Am I missing something?  To determine apogee, we take every data point for the last 20 samples and compare it to the last 20 samples before each.  Once all samples show a decline in altitude, we say apogee is the highest recorded value during that interval. You're right. Sloppy thinking. Apologies. Was trying to express: it appears to me that this is not the jagged set of data one often sees in a time series and you might omit the task of identifying a complicated error structure. From the beginning of a flight keep fitting easy smooth curves until you hit the apogee, discarding outliers--which appear obvious from the plot that I made. (Are they obvious to you though?) Then, after you hit the apogee start fitting a distinct set of easy smooth curves for the rest of the flight, again discarding outliers.   In this instance, spline="brain f__t"
ARIMA model and look for autocorrelation in the residual is standard procedure. Volatility model another.
What is the meaning of jitter in visualize tab of weka 
In weka I load an arff file. I can view the relationship between attributes using the visualize tab.
However I can't understand the meaning of the jitter slider. What is its purpose?

You can find the answer in the mailing list archives:
The jitter function in the Visualize panel just adds artificial random
noise to the coordinates of the plotted points in order to spread the
data out a bit (so that you can see points that might have been
obscured by others).
 Load the contact lenses example from Weka, and visualize it. In the first plot, some points are exactly on top of each other. By adding jitter, you can see how many points there are for each combination. This won't work that good for larger data sets, but you can still get an impression. For discrete values, this is quite important.
I don't know weka, but generally jitter is a term for the variation of a periodic signal to some reference interval. I'm guessing the slider allows you to set some range or threshold below which data points are treated as being regular, or to modify the output to introduce some variation. The wikipedia entry can give you some background.
Update: from this pdf, the jitter slider is for this purpose:

“Jitter” option to deal with nominal attributes (and to detect “hidden”data points)

Based on the accompanying slide it looks like it introduces some variation in the visualisation, perhaps to show when two data points overlap.
Update 2: This google books extract (to Data mining By Ian H. Witten, Eibe Frank) seems to confirm my guess: 

[jitter] is a random displacement applied to X and Y values to separate points that lie on top of one another. Without jitter, 1000 instances at the same data point would look just the same as 1 instance


I don't know the products you mention, but jittering generally means randomising the sample positions. Eg, in ray tracing you would normally render a ray though each pixel on the screen. Jittering adds a random offset to each ray to reduce issues caused by regular aliasing.
pandas pivot table rename columns 
How to rename columns with multiple levels after pandas pivot operation?
Here's some code to generate test data:
import pandas as pd
df = pd.DataFrame({
    'c0': ['A','A','B','C'],
    'c01': ['A','A1','B','C'],
    'c02': ['b','b','d','c'],
    'v1': [1, 3,4,5],
    'v2': [1, 3,4,5]})

print(df)

gives a test dataframe: 
   c0 c01 c02  v1  v2
0  A   A   b   1   1
1  A  A1   b   3   3
2  B   B   d   4   4
3  C   C   c   5   5

applying pivot
df2 = pd.pivot_table(df, index=["c0"], columns=["c01","c02"], values=["v1","v2"])
df2 = df2.reset_index()

gives

how to rename the columns by joining levels?
with format 
<c01 value>_<c02 value>_<v1>
for example first column should look like 
"A_b_v1"
The order of joining levels  isn't really important to me.

If you want to coalesce the multi-index into a single string index without caring about the index level order, you can simply map a join function over the columns, and assign the result list back:
df2.columns = list(map("_".join, df2.columns))


And for your question, you can loop through the columns where each element is a tuple, unpack the tuple and join them back in the order you want:
df2 = pd.pivot_table(df, index=["c0"], columns=["c01","c02"], values=["v1","v2"])

# Use the list comprehension to make a list of new column names and assign it back
# to the DataFrame columns attribute.
df2.columns = ["_".join((j,k,i)) for i,j,k in df2.columns]
df2.reset_index()


 thanks ['_'.join(str(s).strip() for s in col if s) for col in df2.columns] worked as a general solution, independent of number of levels @muon I like your general solution! It's perfect.