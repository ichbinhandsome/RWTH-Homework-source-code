Explain the differences and similarities of the Anything2vec approach and Glove: Similarity: Both methods can be used to train the word embedding and provide a vector per word as output. Besides, the word embedding provided        by both anthing2vec and Glove capture semantic relatedness and semantic similarity. Additionally both two methods have the drawback that they can not         provide embedding to out-of-vocalbulary words.        Differences: anthingtovec train the embedding based on prediction and there is a overall loss function. The optimal embedding is found        when the overall prediction loss is minimum. On the country, Glove train the embedding based on the cooccurance-matrix, wi * wj.T = p(i]j)        the product of two word vectors represents the conditional probability. The optimal embedding is found when the square error between the         cocurrence term log(Xij) and word vector product wi * wj.T is minumum. 