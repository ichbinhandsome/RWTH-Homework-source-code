{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webmining - Assignment 3\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. You will familiarize yourself with the apriori algorithm, using stochastic gradient descend for matrix factorization and recommender systems using the surprise package. You can use numpy and all the python standard library for all tasks. You can then further use pandas for the first task and the surprise package for the last task.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 30.06.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Alice',\n",
    "        'last_name': 'Foo',\n",
    "        'student_id': 12345\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Bob',\n",
    "        'last_name': 'Bar',\n",
    "        'student_id': 54321\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Apriori (7 points)\n",
    "\n",
    "In this task you should implement a general version of the apriori algorithm.\n",
    "\n",
    "The main idea of the apriori principle/requirement is, that if a constraint $C$ (e.g. minimal support) does not hold for a pattern, it will never hold for any more specific pattern.\n",
    "You encountered a version of the of the apriori principle defined on the **itemset** (mathematically it is a function defined on the powerset of the itemset set)\n",
    "\n",
    "Example:\n",
    "$$C( \\{\\text{apple}, \\text{banana}\\}) = 0 \\Rightarrow C( \\{\\text{apple}, \\text{banana}, \\text{any_other_item}\\}) = 0$$\n",
    "\n",
    "One can also express this for constraints that are defined on **instance** sets: (mathematically the constrain is a function defined on the powerset of the instance set)\n",
    "$$\n",
    "C(X) = 0 \\Rightarrow C(Y) = 0 \\; \\forall Y \\subseteq X\n",
    "$$\n",
    "Hereby $X$ and $Y$ are instance sets!\n",
    "We will write a general apriori algorithm that works with any constraint that satisfies this relationship. So our constraints are defined on the **instance** (power-)set. See the example below for how the support count is defined as a function of the itemset.\n",
    "\n",
    "\n",
    "\n",
    "For this task a pattern will be a *sorted* tuple of items e.g. `('apple', 'banana', 'yoghurt')`. Items are just strings. Assume a constraint is a function that takes a dataframe and returns either True or False depending on whether the constraint is satisfied for that (subset) dataframe. A very popular of those constraints is the `min_support_count_constraint`\n",
    "```python\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def min_support_count_constraint_evaluator(df, min_support_count):\n",
    "    return len(df) >= min_support_count\n",
    "\n",
    "min_supp_100_constraint = partial(min_support_count_constraint_evaluator, min_support=100)\n",
    "\n",
    "df_90 = pd.DataFrame.from_dict({\"sth\":np.random.rand(90)})# dataframe with 90 rows\n",
    "print(min_supp_100_constraint(df_90)) # False\n",
    "\n",
    "df_110 = pd.DataFrame.from_dict({\"sth\":np.random.rand(110)})# dataframe with 110 rows\n",
    "print(min_supp_100_constraint(df_110)) # True\n",
    "```\n",
    "\n",
    "Assume the database is given as a dataframe each row representing a single transaction. It also has a column `'items'` that contains the items for that transaction as a set.\n",
    "Represent association rules as tuple of two itemsets: e.g.\n",
    "$$\n",
    "(\\text{apple}, \\text{banana}) \\Rightarrow (\\text{yoghurt}, \\text{milk})\n",
    "$$\n",
    "is represented as\n",
    "```python\n",
    "( ('apple', 'banana'), ('milk', 'yoghurt') )\n",
    "```\n",
    "Example database:\n",
    "```python\n",
    "db = pd.DataFrame.from_dict(\n",
    "```\n",
    "## a) Checking a constraint and pattern against a database (1.5)\n",
    "Write a function `check_against_database(constraint, pattern, database)` that checks whether a single pattern holds given a specific pattern and the entire database. It returns either 0 (constraint does not hold) or True (constraint holds). Note that now the constraint has to be formulated as a function on the instance set, rather than on the itemsets.\n",
    "\n",
    "## b) Generate next level candidates (1.5)\n",
    "\n",
    "Write a function `generate_next_level_candidates(patterns)` that takes a list of patterns (assume that the constraint holds for all the patterns that are supplied). All the patterns are of the same length lets call it `l`. It `yield`s all patterns of length `l+1` that are not already excluded through the apriori principle. I.e. that for a pattern of length $l+1$ to be in the result **all** sub-patterns of length `l` are in `patterns`.\n",
    "\n",
    "You will need an initial list of patterns that you then prune further. There are two ways of constructing these initial patterns:\n",
    "\n",
    "*a)* The easy way is to collect all items that are in all the patterns and consider all combinations of those items of length $l+1$. It has runtime $O(I\\, \\text{over}\\, (l+1))$. $x\\, \\text{over}\\, y$ meaning the binomial coeff. $I$ is the number of items.\n",
    "\n",
    "*b)* Usually you can gain significantly speedup in this step by only considering initial candidates of length $l+1$ that arise when matching pairs of patterns where the first $l-1$ patterns are the same. It has naive runtime of $O(\\text{len}(\\text{patterns})^2)$ but average case can go below it with a better implementation.\n",
    "\n",
    "Implementing **one** of the strategies **is enough**. If you want, try both. If you are extra fancy try dynamically switching between the two. Implementing both or implementing dynamic switching does not give more points.\n",
    "Name the functions `initial_a(patterns)`, `initial_b(patterns)` and `initial_dynamic(patterns)` respectively. They take a list of patterns and are iterable, yielding initial patterns of length $l+1$.\n",
    "\n",
    "\n",
    "## c) Bringing general apriori together (1.5)\n",
    "\n",
    "Write a function `generic_apriori(constraint, max_length, database)` that returns all patterns with length of at most (<=) `max_length` for which `constraint` holds on that database. Assume that the `constraint` satisfies the apriori relation. Use the apriori principle to reduce the number of calls to the `constraint`/`check_against_database` functions.\n",
    "\n",
    "## d) Classic apriori (1)\n",
    "\n",
    "Write function `fim_apriori(df, max_length, min_supp)` that performs classic frequent-itemset mining (FIM) apriori. `min_supp` is a float between 0 and 1. It is used as a lower bound (>=) for the support. The function returns all patterns that appear at least `min_supp` times and are of at most `max_length` length. You should use the `generic_apriori` function.\n",
    "\n",
    "\n",
    "## e) Single Rule confidence apriori (1.0)\n",
    "This task is slightly more complicated but have a look at the right hand side of the rules slide 07_36 and compare it with slide 07_28.\n",
    "\n",
    "Write a function `rule_apriori(df, pattern, min_conf)` that returns all the association rules that have at least (>=) `min_conf` confidence using the apriori principle. You should use the generic apriori function.\n",
    "\n",
    "## f) Association rule miner (0.5)\n",
    "\n",
    "Write a function `assoc_miner(df, max_length, min_supp, min_conf)` that uses `fim_apriori` and `rule_apriori` to find all association rules with at least `min_supp` and `min_conf`. It threrefor first mines all `min_supp` frequent itemsets and for each it finds the `min_conf` association rules. Use the apriori principle. The function returns a list of association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # VERY USEFUL FOR THIS TASK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from itertools import chain\n",
    "\n",
    "def initial_a(patterns):\n",
    "    if len(patterns) == 0:\n",
    "        return []\n",
    "    level = len(patterns[0])\n",
    "    items = set(chain.from_iterable(patterns))\n",
    "    for pat in  combinations(items, level + 1):\n",
    "        yield tuple(sorted(pat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(entry, pattern):\n",
    "    \"\"\"Small helper function for check_against_database\n",
    "    Hint: there is a map function for pandas Series objects\"\"\"\n",
    "    return all((pat in entry) for pat in pattern)\n",
    "\n",
    "\n",
    "def check_against_database(constraint, pattern, database):\n",
    "    # index to store if pattern in the row of database\n",
    "    df_index = []\n",
    "    for i in range(len(database)):\n",
    "        items = database.at[i,'items']\n",
    "        if set(pattern).issubset(items):\n",
    "            df_index.append(i)\n",
    "    df = database.iloc[df_index]\n",
    "    return constraint(df)\n",
    "\n",
    "\n",
    "def generate_next_level_candidates(patterns):\n",
    "    if len(patterns) == 0:\n",
    "        return []\n",
    "    level = len(patterns[0])\n",
    "    # define b to get all possible patterns\n",
    "    def initial_b(patterns):\n",
    "        if len(patterns) == 0:\n",
    "            return []\n",
    "        level = len(patterns[0])\n",
    "        for i in range(len(patterns)):\n",
    "            for j in range(i+1, len(patterns)):\n",
    "                pattern_1 = patterns[i]\n",
    "                pattern_2 = patterns[j]\n",
    "                a = [l for l in pattern_1 if l in pattern_2]\n",
    "                if len(a) == level - 1:\n",
    "                    c = [l for l in pattern_1 if l not in pattern_2]\n",
    "                    b = [l for l in pattern_2 if l not in pattern_1]\n",
    "                    yield tuple(sorted(a+b+c))\n",
    "    # remove patterns that break the apriori rule\n",
    "    new_patterns = list(set(initial_b(patterns)))\n",
    "    output_patterns = []\n",
    "    for pattern in new_patterns:\n",
    "        flag = True\n",
    "        for i in range(level+1):\n",
    "            temp = pattern[:i] + pattern[(i+1):]\n",
    "            if temp not in patterns:\n",
    "                flag = False\n",
    "        if flag == True:\n",
    "            output_patterns.append(pattern)\n",
    "    return output_patterns\n",
    " \n",
    "    \n",
    "def generic_apriori(constraint, max_length, database):\n",
    "    patterns_all_level = []\n",
    "    # initialize the item tuple for level 1\n",
    "    items = [tuple([l]) for b in database['items'].values for l in b]\n",
    "    items = list(set(items))\n",
    "    i = 0\n",
    "    while i < max_length:\n",
    "        patterns = [l for l in items if check_against_database(constraint, l, example_db)]\n",
    "        patterns_all_level += patterns\n",
    "        items = generate_next_level_candidates(patterns)\n",
    "        i += 1\n",
    "    return patterns_all_level\n",
    "\n",
    "\n",
    "def fim_apriori(database, max_length, min_supp):\n",
    "    num_tran = len(database)\n",
    "    min_support_count = num_tran * min_supp\n",
    "    if min_support_count%1 > 0:\n",
    "        min_support_count = int(min_support_count//1 + 1)\n",
    "    else:\n",
    "        min_support_count = int(min_support_count)\n",
    "    \n",
    "    min_supp_constraint = partial(min_support_count_constraint_evaluator, min_support_count = min_support_count)\n",
    "    patterns = generic_apriori(min_supp_constraint, max_length, database)\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def rule_apriori(database, pattern, min_conf):\n",
    "    \"\"\"\n",
    "    problem: does the output rule must contain the pattern?\n",
    "    in the example the pattern only have length 2, should test more complex case\n",
    "    \"\"\"\n",
    "    if len(pattern) <= 1:\n",
    "        raise ValueError(\"need pattern of length at least 2\")\n",
    "        \n",
    "    # Get new database where all items contain at least one of the items in pattern\n",
    "    # Also discard any items that do not belong to the pattern\n",
    "    index_new = []\n",
    "    for i in range(len(database)):\n",
    "        row_items = database.at[i,'items']\n",
    "        overlap = [l for l in row_items if l in pattern]\n",
    "        if len(overlap) > 0:\n",
    "            index_new.append(i)\n",
    "    new_database = database.iloc[index_new]    \n",
    "    \n",
    "    # now that you have the new database think how you can apply the generic apriori for assoc_rule mining\n",
    "    # get level 1 tuples\n",
    "    rules = []\n",
    "    items = [l for b in new_database['items'].values for l in b]\n",
    "    items = list(set(items))\n",
    "    \n",
    "    for item in pattern:\n",
    "        item = tuple([item])\n",
    "        item_support = len([1 for l in database['items'].values if set(item).issubset(l)])\n",
    "        #max_length = max([len(l) for l in database['items'].values])\n",
    "        min_support_count = min_conf * item_support\n",
    "        if min_support_count%1 > 0:\n",
    "            min_support_count = int(min_support_count//1 + 1)\n",
    "        else:\n",
    "            min_support_count = int(min_support_count)\n",
    "        max_length = len(pattern)\n",
    "        min_supp_constraint = partial(min_support_count_constraint_evaluator, min_support_count = min_support_count)\n",
    "        patterns = generic_apriori(min_supp_constraint, max_length, new_database)\n",
    "        \n",
    "        patterns = [l for l in patterns if set(item).issubset(l) and len(l)>1 and set(l).issubset(pattern)]\n",
    "        if len(patterns) > 0:\n",
    "            for pattern in patterns:\n",
    "                Y = tuple(x for x in pattern if x not in item)\n",
    "                X = item\n",
    "                rules.append((X,Y))\n",
    "    return list(set(rules))\n",
    "\n",
    "\n",
    "def assoc_miner(df, max_length, min_supp, min_conf):\n",
    "    patterns = fim_apriori(df, max_length, min_supp)\n",
    "    patterns = [x for x in patterns if len(x)>1]\n",
    "    rules = []\n",
    "    print(patterns)\n",
    "    for pattern in patterns:\n",
    "        rule = rule_apriori(df, pattern, min_conf)\n",
    "        rules += rule\n",
    "    return list(set(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "\n",
      "a)\n",
      "True\n",
      "True\n",
      "False\n",
      "\n",
      "b)\n",
      "[('C', 'D'), ('A', 'C'), ('A', 'D')]\n",
      "[('A', 'B', 'C')]\n",
      "[]\n",
      "\n",
      "c)\n",
      "[('apple',), ('citron',)]\n",
      "[('apple',), ('citron',), ('apple', 'citron')]\n",
      "[('apple',), ('banana',), ('milk',), ('citron',), ('banana', 'citron'), ('apple', 'citron'), ('apple', 'banana')]\n",
      "[('apple',), ('banana',), ('milk',), ('citron',), ('banana', 'citron'), ('apple', 'citron'), ('apple', 'banana'), ('apple', 'banana', 'citron')]\n",
      "\n",
      "d)\n",
      "[('apple',), ('citron',), ('apple', 'citron')]\n",
      "[('apple',)]\n",
      "[('apple',), ('citron',)]\n",
      "\n",
      "e)\n",
      "[(('citron',), ('apple',)), (('apple',), ('citron',))]\n",
      "[(('citron',), ('apple',))]\n",
      "\n",
      "f)\n",
      "[('apple', 'banana'), ('apple', 'citron')]\n",
      "[(('banana',), ('apple',)), (('citron',), ('apple',))]\n",
      "[('apple', 'banana'), ('apple', 'citron')]\n",
      "[(('apple',), ('citron',)), (('banana',), ('apple',)), (('citron',), ('apple',))]\n",
      "[('apple', 'banana'), ('apple', 'citron')]\n",
      "[(('apple',), ('banana',)), (('apple',), ('citron',)), (('banana',), ('apple',)), (('citron',), ('apple',))]\n",
      "[('apple', 'citron')]\n",
      "[(('apple',), ('citron',)), (('citron',), ('apple',))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def min_support_count_constraint_evaluator(df, min_support_count):\n",
    "    return len(df) >= min_support_count\n",
    "\n",
    "min_supp_100_constraint = partial(min_support_count_constraint_evaluator, min_support_count=100)\n",
    "\n",
    "df_90 = pd.DataFrame.from_dict({\"sth\":np.random.rand(90)})# dataframe with 90 rows\n",
    "print(min_supp_100_constraint(df_90)) # False\n",
    "\n",
    "df_110 = pd.DataFrame.from_dict({\"sth\":np.random.rand(110)})# dataframe with 110 rows\n",
    "print(min_supp_100_constraint(df_110)) # True\n",
    "print()\n",
    "\n",
    "# Example database\n",
    "items = [('apple', 'banana'),\n",
    "      ('apple', 'banana', 'citron'),\n",
    "      ('apple', 'citron'),\n",
    "      ('apple', 'citron'),\n",
    "      ('apple',),\n",
    "      ('milk',)]\n",
    "min_supp_3_constraint = partial(min_support_count_constraint_evaluator, min_support_count=3)\n",
    "min_supp_1_constraint = partial(min_support_count_constraint_evaluator, min_support_count=1)\n",
    "example_db = pd.DataFrame.from_dict({'items':items})\n",
    "\n",
    "# Examples for task 1 a)\n",
    "print(\"a)\")\n",
    "print(check_against_database(min_supp_3_constraint, ('citron',), example_db)) # True\n",
    "print(check_against_database(min_supp_3_constraint, ('citron', 'apple'), example_db)) # True\n",
    "print(check_against_database(min_supp_3_constraint, ('apple', 'banana'), example_db)) # False\n",
    "print()\n",
    "\n",
    "# Examples for task 1 b)\n",
    "# order of the resulting pattern does not matter\n",
    "print(\"b)\")\n",
    "print(generate_next_level_candidates([('A',), ('C',), ('D',)])) # [('C', 'D'), ('A', 'D'), ('A', 'C')]\n",
    "print(generate_next_level_candidates([('A','B'), ('A','C'), ('C', 'D'), ('B', 'C')])) # [('A', 'B', 'C')]\n",
    "print(generate_next_level_candidates([('A','B'), ('A','C'), ('C', 'D')])) # []\n",
    "print()\n",
    "\n",
    "\n",
    "# Examples for task 1 c)\n",
    "# order of the resulting pattern does not matter\n",
    "print(\"c)\")\n",
    "print(generic_apriori(min_supp_3_constraint, 1, example_db)) # [('apple',), ('citron',)]\n",
    "print(generic_apriori(min_supp_3_constraint, 2, example_db)) # [('apple',), ('citron',), ('apple', 'citron')]\n",
    "print(generic_apriori(min_supp_1_constraint, 2, example_db))\n",
    "# [('apple',), ('milk',), ('banana',), ('citron',), ('apple', 'banana'), ('banana', 'citron'), ('apple', 'citron')]\n",
    "print(generic_apriori(min_supp_1_constraint, 3, example_db))\n",
    "# [('apple',), ('milk',), ('banana',), ('citron',), ('apple', 'banana'), ('banana', 'citron'), ('apple', 'citron'), ('apple', 'banana', 'citron')]\n",
    "print()\n",
    "\n",
    "\n",
    "# Examples for task 1 d)\n",
    "# support for 1 itemsets\n",
    "# apple  : 5/6\n",
    "# banana : 2/6\n",
    "# citron : 3/6\n",
    "# milk   : 1/6\n",
    "# order of the resulting pattern does not matter\n",
    "print(\"d)\")\n",
    "print(fim_apriori(example_db, 2, 0.5)) # [('apple',), ('citron',), ('apple', 'citron')]\n",
    "print(fim_apriori(example_db, 1, 0.7)) # [('apple',)]\n",
    "print(fim_apriori(example_db, 1, 0.4)) # [('apple',), ('citron',)]\n",
    "print()\n",
    "\n",
    "\n",
    "# Examples for task 1 e)\n",
    "# citron => apple | conf = 1\n",
    "# apple => citron | conf = 0.6\n",
    "# order of the resulting rules does not matter\n",
    "print(\"e)\")\n",
    "print(rule_apriori(example_db, ('apple', 'citron'), 0.5)) # [(('citron',), ('apple',)), (('apple',), ('citron',))]\n",
    "print(rule_apriori(example_db, ('apple', 'citron'), 0.7)) # [(('citron',), ('apple',))]\n",
    "print()\n",
    "\n",
    "\n",
    "# Examples for task 1 f)\n",
    "# order of the resulting rules does not matter\n",
    "print(\"f)\")\n",
    "print(assoc_miner(example_db, 3, 0.3, 0.8)) #[(('banana',), ('apple',)), (('citron',), ('apple',))]\n",
    "print(assoc_miner(example_db, 3, 0.3, 0.5)) #[(('banana',), ('apple',)), (('citron',), ('apple',)), (('apple',), ('citron',))]\n",
    "print(assoc_miner(example_db, 3, 0.3, 0.2)) #[(('banana',), ('apple',)), (('apple',), ('banana',)), (('citron',), ('apple',)), (('apple',), ('citron',))]\n",
    "print(assoc_miner(example_db, 3, 0.5, 0.2)) # [(('citron',), ('apple',)), (('apple',), ('citron',))]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Matrix factorization (5 points)\n",
    "\n",
    "In this task you should implement matrix factorization using stochastic gradient descent. The goal is to find matrices $P$ and $Q$ given a matrix $R$ such that $P \\cdot Q\\approx R$\n",
    "\n",
    "Unlike in \"normal\" matrix decomposition the rating matrix $R$ can contain blank entries (given as 0). The factorization should be trained only on non-blank entries. Use squared error as your loss function and stochastic gradient descent (SGD) for optimization.\n",
    "\n",
    "\n",
    "The matrices for K latent factors are of size\n",
    "\n",
    "$$\n",
    "P - (I, K)\\\\\n",
    "Q - (K, J)\\\\\n",
    "R - (I, J)\n",
    "$$\n",
    "The Loss for one entry is $L_{i,j} = (R_{i,j} - \\sum_{k} P_{i,k} \\cdot Q_{k,j})^2$.\n",
    "The average loss for a set of indice pairs is $I$\n",
    "$$\n",
    "L_I = \\frac{1}{|I|}\\sum_{(i,j) \\in I} L_{i,j}\n",
    "$$\n",
    "The partial derivatives of the loss are\n",
    "$$\n",
    "\\frac{dL_{i,j}}{dP_{i,v}} = -2(R_{i, j} - \\sum_{k} P_{i,k} \\cdot Q_{k,j})Q_{v,j} \\\\\n",
    "\\frac{dL_{i,j}}{dQ_{v,j}} = -2(R_{i, j} - \\sum_{k} P_{i,k} \\cdot Q_{k,j})P_{i,v}\n",
    "$$\n",
    "SGD update for observation $i,j$\n",
    "\n",
    "$P_{i,v} = P_{i,v} - \\alpha \\frac{dL_{i,j}}{dP_{i,v}}$\n",
    "\n",
    "$Q_{v,j} = Q_{v,j} - \\alpha \\frac{dL_{i,j}}{dQ_{v,j}}$\n",
    "\n",
    "When performing SGD compute all the partial derivates for one index pair $i,j$ first and then update the values.\n",
    "For testing, consider the utility matrix in the file 'utility_matrix.csv'.\n",
    "\n",
    "## a) Average Loss function (0.5)\n",
    "Write a loss function `average_loss(R, P, Q, ijs=None)` that computes the average reconstruction loss of your matrix factorization. If `ijs` are supplied only for those indices, otherwise for the entire matrix. `ijs` is list of tuples of indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_loss(R, P, Q, ijs=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Factorize (2.0)\n",
    "\n",
    "Implement matrix factorization from scratch by implementing a function `matrix_factorization(R, K, alpha, rng, epochs, ijs_train, ijs_test=None)` with\n",
    "\n",
    "* R: Matrix of given ratings (numpy array)\n",
    "* k: Number of latent factors (int)\n",
    "* alpha: learning rate (float)\n",
    "* rng: random number generator (numpy.random.default_rng)\n",
    "* epochs: the number of epoch to train (integer)\n",
    "* ijs_train: indices to train on (list of tuples of two integers)\n",
    "* ijs_test: indices to test on\n",
    "\n",
    "It returns the P, Q matrix, train-loss-list and test-loss-list. The loss is calculated after each epoch of training and is then appended to the list. One epoch is performing an update for all the `ijs_train`. Leave the test_loss_list empty if ijs_test are not specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R, K, alpha, rng, epochs, ijs_train, ijs_test=None):\n",
    "    #randomly initialize matrices\n",
    "    U,I = R.shape\n",
    "    P = rng.random((U,K))\n",
    "    Q = rng.random((K,I))\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i,j in ijs_train:\n",
    "            # Your code goes here\n",
    "            pass\n",
    "        train_losses.append(average_loss(R, P, Q, ijs_train))\n",
    "        if not (ijs_test is None):\n",
    "            test_losses.append() # insert code here\n",
    "    return P, Q, train_losses, test_losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example:\n",
    "R = np.genfromtxt('utility_matrix.csv', delimiter=',',dtype=np.float)\n",
    "rng=default_rng(1)\n",
    "ijs = [(i,j) for i, j in zip(*np.nonzero(R))]\n",
    "epochs=2\n",
    "P, Q, l_train,l_test = matrix_factorization(R, 4, 0.001, rng, epochs, ijs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# epochs = 1\n",
    "# P\n",
    "array([[0.53030017, 0.96568957, 0.17039495, 0.96478729],\n",
    "       [0.32009701, 0.43594586, 0.83751995, 0.41845338],\n",
    "       [0.56471082, 0.03854926, 0.76744862, 0.54845707],\n",
    "       [0.35681508, 0.80943855, 0.33222907, 0.47213126],\n",
    "       [0.14975727, 0.41927103, 0.22232818, 0.2733064 ],\n",
    "       [0.7632887 , 0.29660883, 0.50014336, 0.99427529],\n",
    "       [0.97469191, 0.74041843, 0.55915683, 0.29096398],\n",
    "       [0.17618862, 0.98158305, 0.53054081, 0.12794982],\n",
    "       [0.62839181, 0.78786586, 0.61887645, 0.92215325],\n",
    "       [0.05183058, 0.54145109, 0.47416302, 0.07551933],\n",
    "       [0.65114207, 0.86260615, 0.60851652, 0.27028487],\n",
    "       [0.8600531 , 0.52663855, 0.53355849, 0.76868257],\n",
    "       [0.15773973, 0.82380165, 0.69349028, 0.79731367],\n",
    "       [0.20945627, 0.82255223, 0.21504987, 0.09967779],\n",
    "       [0.86684443, 0.87304182, 0.89573504, 0.48592394],\n",
    "       [0.29209354, 0.0288247 , 0.67087993, 0.73952057],\n",
    "       [0.8405461 , 0.28475842, 0.22651716, 0.64898595],\n",
    "       [0.81803697, 0.97569678, 0.1701876 , 0.49792483],\n",
    "       [0.90125284, 0.42775465, 0.59711584, 0.03434974],\n",
    "       [0.68053857, 0.93020444, 0.83722127, 0.89347768]])\n",
    "# Q.T\n",
    "array([[0.67905308, 0.71703279, 0.6783386 , 0.28217364],\n",
    "       [0.28671191, 0.2245397 , 0.46818009, 0.11601512],\n",
    "       [0.79664329, 0.43092275, 0.8926013 , 0.28903537],\n",
    "       [0.24819143, 0.041076  , 0.66335624, 0.79366653],\n",
    "       [0.85183337, 0.29365763, 0.83269148, 0.71732725],\n",
    "       [0.10102076, 0.46937421, 0.37185553, 0.15782279],\n",
    "       [0.85353666, 0.1410099 , 0.56891687, 0.40167918],\n",
    "       [0.20774953, 0.67103893, 0.24104121, 0.46453524],\n",
    "       [0.39950197, 0.41217316, 1.01234667, 0.68805916],\n",
    "       [0.36029593, 0.78600697, 0.28899546, 0.50123434]])\n",
    "# l_train\n",
    " [6.120285596840649]\n",
    "# l_test\n",
    " []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# epochs = 2\n",
    "# P\n",
    "array([[0.54898348, 0.98157321, 0.19648697, 0.98124088],\n",
    "       [0.32886539, 0.44907869, 0.84768277, 0.42812938],\n",
    "       [0.58012071, 0.05003125, 0.78152081, 0.55921641],\n",
    "       [0.38441392, 0.83141728, 0.3615508 , 0.49147467],\n",
    "       [0.1662158 , 0.43662063, 0.24186757, 0.2852157 ],\n",
    "       [0.77648636, 0.31330686, 0.51525653, 1.00812756],\n",
    "       [0.98826968, 0.75669342, 0.57721966, 0.30538718],\n",
    "       [0.19191371, 0.99378748, 0.54508121, 0.14019524],\n",
    "       [0.63329215, 0.79907542, 0.62449599, 0.92703749],\n",
    "       [0.0645117 , 0.55504459, 0.48935008, 0.08918713],\n",
    "       [0.66116901, 0.8730181 , 0.62392472, 0.28066017],\n",
    "       [0.88065328, 0.54456397, 0.55651066, 0.78492335],\n",
    "       [0.16776704, 0.82830911, 0.70377071, 0.80760509],\n",
    "       [0.22800606, 0.84385433, 0.23923373, 0.11851386],\n",
    "       [0.87842173, 0.88485997, 0.91440644, 0.49974584],\n",
    "       [0.31099925, 0.0515082 , 0.69662905, 0.75999372],\n",
    "       [0.84561165, 0.2879153 , 0.23761533, 0.65868291],\n",
    "       [0.83158875, 0.98837475, 0.19004603, 0.51386302],\n",
    "       [0.90773606, 0.43283726, 0.604502  , 0.04412471],\n",
    "       [0.6871331 , 0.94118386, 0.84689142, 0.90100045]])\n",
    "# Q.T\n",
    "array([[0.69701391, 0.74199718, 0.70229101, 0.30657812],\n",
    "       [0.32711821, 0.26978508, 0.50493145, 0.15823725],\n",
    "       [0.82398116, 0.46445741, 0.91741003, 0.31943684],\n",
    "       [0.28343279, 0.07483107, 0.69377651, 0.82312758],\n",
    "       [0.87141708, 0.32355552, 0.8543836 , 0.73583079],\n",
    "       [0.13855796, 0.51664766, 0.40165922, 0.18632909],\n",
    "       [0.88099643, 0.17496533, 0.59388225, 0.42629548],\n",
    "       [0.24995286, 0.70774974, 0.28510039, 0.50711432],\n",
    "       [0.42283823, 0.44255497, 1.02792469, 0.71036147],\n",
    "       [0.40242657, 0.84484156, 0.33390436, 0.54527132]])\n",
    "# l_train\n",
    "[6.120285596840649, 5.702120509768838]\n",
    "# l_test\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Evaluate the training (0.5+0.5+1.5)\n",
    "\n",
    "Evaluate the training by visualising the training- and test-loss as a function of epochs for different values of `alpha`. Apply it to the 'utility_matrix.csv'.  Perform the train test split like so (it's roughly 80/20):\n",
    "\n",
    "```python\n",
    "R = np.genfromtxt('utility_matrix.csv', delimiter=',',dtype=np.float)\n",
    "rng = default_rng(13)\n",
    "ijs = [(i,j) for i, j in zip(*np.nonzero(R))]\n",
    "rng.shuffle(ijs)\n",
    "ijs_train=ijs[:122]\n",
    "ijs_test=ijs[122:]\n",
    "```\n",
    "\n",
    "The different values for `alpha` are `[0.05, 0.01, 0.001, 0.0001]`.\n",
    "Use K=5 latent factors. Train for 20 and 500 epochs.\n",
    "Save your plot as `training20.png` and `training500.png`.\n",
    "Save a description + explanation of what is happening as `training.txt`. Write 4+ sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.genfromtxt('utility_matrix.csv', delimiter=',',dtype=np.float)\n",
    "rng = default_rng(13)\n",
    "ijs = [(i,j) for i, j in zip(*np.nonzero(R))]\n",
    "rng.shuffle(ijs)\n",
    "ijs_train=ijs[:122]\n",
    "ijs_test=ijs[122:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: \"Surprise\" package (3 points)\n",
    "The surprise package http://surpriselib.com/ is a python package for recommender systems.\n",
    "\n",
    "For testing you can e.g. use the built-in \"movie lens 100k\" dataset.\n",
    "\n",
    "## a) Apply to data (1.5)\n",
    "Write a function `apply_surprise(data)` that splits the data into 80 percent training and 20 percent testing. It then calculate RMSE and MAE for the three recommender types BaselineOnly, NMF and KNNBasic. It returns a dictionary like so:\n",
    "```python\n",
    "{\n",
    "    'BaselineOnly' : (RMSE, MAE, trained_instance),\n",
    "    'NMF' : (RMSE, MAE, trained_instance),\n",
    "    'KNNBasic' : (RMSE, MAE, trained_instance)\n",
    "}\n",
    "```\n",
    "Where RMSE and MAE are calculated on the test set.\n",
    "\n",
    "Result for the movie lens 100k dataset using `random_state=0` for the train-test split:\n",
    "```python\n",
    "{'BaselineOnly': (0.9550543124044737,\n",
    "  0.7573425770736903,\n",
    "  <surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x1e46dd66e50>),\n",
    " 'NMF': (0.9692337693760231,\n",
    "  0.7636117736623521,\n",
    "  <surprise.prediction_algorithms.matrix_factorization.NMF at 0x1e46dc86790>),\n",
    " 'KNNBasic': (0.9872227056345668,\n",
    "  0.7821631158458218,\n",
    "  <surprise.prediction_algorithms.knns.KNNBasic at 0x1e46e008c70>)}\n",
    "  ```\n",
    "\n",
    "## b) Cross validate with the surprise package (1.5)\n",
    "\n",
    "Write a function `cross_val_surprise(data)` that performs 5 fold cross validation and returns the mean an standard deviation of the RMSE and MAE.\n",
    "\n",
    "```python\n",
    "{\n",
    "    'BaselineOnly' : (RMSE_mean, RMSE_std, MAE, MAE_std),\n",
    "    'NMF' : (RMSE_mean, RMSE_std, MAE, MAE_std),\n",
    "    'KNNBasic' : (RMSE_mean, RMSE_std, MAE, MAE_std)\n",
    "}\n",
    "```\n",
    "\n",
    "Result for the movie lens 100k dataset using `cv=KFold(n_splits=5, random_state=0)`.\n",
    "```python\n",
    "{'BaselineOnly': (0.9439693435434332,\n",
    "  0.005917617695892292,\n",
    "  0.7483729454435822,\n",
    "  0.004810636871680646),\n",
    " 'NMF': (0.9639085178892854,\n",
    "  0.003642645329612351,\n",
    "  0.7574494428884382,\n",
    "  0.0036830260508856336),\n",
    " 'KNNBasic': (0.9797335044073868,\n",
    "  0.00496152108148495,\n",
    "  0.7736279266439918,\n",
    "  0.004948889332343514)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NMF, KNNBasic, BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "def apply_surprise(data):\n",
    "    algos = [BaselineOnly, NMF, KNNBasic]\n",
    "    names = ['BaselineOnly', 'NMF', 'KNNBasic']\n",
    "    result_dict={}\n",
    "    # Code here\n",
    "    # split the data into 0.8:0.2\n",
    "    trainset, testset = train_test_split(data, test_size=0.2,random_state=0)\n",
    "    print(trainset)\n",
    "    for algo_class, name in zip(algos, names):\n",
    "        # Code here\n",
    "        algo = algo_class()\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        RMSE = accuracy.rmse(predictions)\n",
    "        MAE = accuracy.mae(predictions)\n",
    "        instance = trainset\n",
    "        result_dict[name] = (RMSE, MAE, instance)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import KFold\n",
    "def cross_val_surprise(data):\n",
    "    \"\"\"\n",
    "    np.std() 得到的结果是0， 参考答案不是0\n",
    "    \"\"\"\n",
    "    algos = [BaselineOnly, NMF, KNNBasic]\n",
    "    names = ['BaselineOnly', 'NMF', 'KNNBasic']\n",
    "    result_dict={}\n",
    "    for algo_class, name in zip(algos, names):\n",
    "        # Code here\n",
    "        cv_RMSE = []\n",
    "        cv_MAE = []\n",
    "        cv = KFold(n_splits=5, random_state=0)\n",
    "        for trainset, testset in cv.split(data):\n",
    "            algo = algo_class()\n",
    "            algo.fit(trainset)\n",
    "            predictions = algo.test(testset)\n",
    "            RMSE = accuracy.rmse(predictions)\n",
    "            cv_RMSE.append(RMSE)\n",
    "            MAE = accuracy.mae(predictions)\n",
    "            cv_MAE.append(MAE)\n",
    "            result_dict[name] = (np.mean(RMSE), np.std(RMSE,dtype=np.float64), np.mean(MAE), np.std(MAE,dtype=np.float64))\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 0.9447\n",
      "MAE:  0.7491\n",
      "Estimating biases using als...\n",
      "RMSE: 0.9401\n",
      "MAE:  0.7459\n",
      "Estimating biases using als...\n",
      "RMSE: 0.9383\n",
      "MAE:  0.7437\n",
      "Estimating biases using als...\n",
      "RMSE: 0.9417\n",
      "MAE:  0.7458\n",
      "Estimating biases using als...\n",
      "RMSE: 0.9551\n",
      "MAE:  0.7573\n",
      "RMSE: 0.9641\n",
      "MAE:  0.7573\n",
      "RMSE: 0.9624\n",
      "MAE:  0.7554\n",
      "RMSE: 0.9628\n",
      "MAE:  0.7578\n",
      "RMSE: 0.9628\n",
      "MAE:  0.7556\n",
      "RMSE: 0.9747\n",
      "MAE:  0.7696\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9841\n",
      "MAE:  0.7763\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9754\n",
      "MAE:  0.7706\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9765\n",
      "MAE:  0.7696\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9755\n",
      "MAE:  0.7695\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9872\n",
      "MAE:  0.7822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BaselineOnly': (0.9550543124044737, 0.0, 0.7573425770736903, 0.0),\n",
       " 'NMF': (0.9747230281875404, 0.0, 0.7696187245632697, 0.0),\n",
       " 'KNNBasic': (0.9872227056345668, 0.0, 0.7821631158458218, 0.0)}"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_surprise(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
